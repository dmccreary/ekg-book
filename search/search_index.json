{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Enterprise Knowledge Graphs This book is about using scalable graph databases to store large-scale connected information for an entire enterprise. This book is in early draft stage and may go through significant structural changes as we get feedback from our readers. Pull requests are gladly accepted. Feel free to post general comments on the GitHub issue area of the GitHub repository that backs this book. If you want to connect with me, please join my LinkedIn graph: https://www.linkedin.com/in/danmccreary/ Table of Contents","title":"Home"},{"location":"#enterprise-knowledge-graphs","text":"This book is about using scalable graph databases to store large-scale connected information for an entire enterprise. This book is in early draft stage and may go through significant structural changes as we get feedback from our readers. Pull requests are gladly accepted. Feel free to post general comments on the GitHub issue area of the GitHub repository that backs this book. If you want to connect with me, please join my LinkedIn graph: https://www.linkedin.com/in/danmccreary/ Table of Contents","title":"Enterprise Knowledge Graphs"},{"location":"blogs/","text":"Summary of Blogs on Enterprise Knowledge Graphs As part of my job as a Distinguished Engineer I am asked to be a recognized thought leader. I attempt to fulfill part of this role in blogging on topics related to AI, machine learning and knowledge representation. You can find my current blogs on Medium: https://dmccreary.medium.com/ I should note that although I try to keep my original blogs mostly intact, sometimes I do fix errors and add additional references. Here is a summary of some of my blogs related directly to Enterprise Knowledge Graphs in reverse chronological order. Rules for Knowledge Graph Rules Published Dec. 2020 Should you store your business rules in a knowledge graph? Understanding Graph Embeddings Published Nov. 23rd 2020 Intel\u2019s Incredible PIUMA Graph Analytics Hardware [The Knowledge Triangle]","title":"Blogs"},{"location":"blogs/#summary-of-blogs-on-enterprise-knowledge-graphs","text":"As part of my job as a Distinguished Engineer I am asked to be a recognized thought leader. I attempt to fulfill part of this role in blogging on topics related to AI, machine learning and knowledge representation. You can find my current blogs on Medium: https://dmccreary.medium.com/ I should note that although I try to keep my original blogs mostly intact, sometimes I do fix errors and add additional references. Here is a summary of some of my blogs related directly to Enterprise Knowledge Graphs in reverse chronological order.","title":"Summary of Blogs on Enterprise Knowledge Graphs"},{"location":"blogs/#rules-for-knowledge-graph-rules-published-dec-2020","text":"Should you store your business rules in a knowledge graph?","title":"Rules for Knowledge Graph Rules Published Dec. 2020"},{"location":"blogs/#understanding-graph-embeddings","text":"Published Nov. 23rd 2020","title":"Understanding Graph Embeddings"},{"location":"blogs/#intels-incredible-piuma-graph-analytics-hardware","text":"","title":"Intel\u2019s Incredible PIUMA Graph Analytics Hardware"},{"location":"blogs/#the-knowledge-triangle","text":"","title":"[The Knowledge Triangle]"},{"location":"contact/","text":"Contact Please connect with me on LinkedIn: Dan McCreary https://www.linkedin.com/in/danmccreary/ You can find blog on medium: https://dmccreary.medium.com/","title":"Contact"},{"location":"contact/#contact","text":"Please connect with me on LinkedIn: Dan McCreary https://www.linkedin.com/in/danmccreary/ You can find blog on medium: https://dmccreary.medium.com/","title":"Contact"},{"location":"glossary/","text":"Enterprise Knowledge Graph Glossary of Terms ABox Types of assertions or statements in a knowledge graph that conform to specific terminologies (knows as TBox statements). The terms ABox and TBox are used to help determine if a statement is universal or related to a specific Subgraph of an Enterprises Knowledge Graph . Within our Enterprise Knowledge Graph architecture, ABox statements often contain knowledge about specific customers, parts, or concepts and may have specific Access Control rules. TBox statements don't usually have these same access rules. See also: TBox Wikipedia page on ABox Accumulator A type of variable that tracks items as you traverse through a graph. Accumulators can be global or be attached to a specific vertex. Accumulators allow MapReduce style queries where each server node in a cluster does work in its local data and returns consolidated results to the query node. For example, in the query \"count all customers that have returned clothing items\" , each node would return only a single count to the query node. This type of query reduces the amount of communication between nodes in the graph cluster. Alternate Label In a concept subgraph of a EKG, every concept has a single preferred label (per language) and many alternate labels that can also be used to name a concept. Alternate labels can be abbreviations, acronyms and synonyms of the concept. In this glossary, alternate labels have a \"Also known as\" prefix. Also known as: atlLabel See also: Preferred Label See also: SKOS Availability Heuristic The availability heuristic suggests that the likelihood of events is estimated based on how many examples of such events come to mind. For example, when a solution architect is presented with a business problem, their likelihood to recommend an enterprise knowledge graph solution is dependant on how many examples of successful enterprise knowledge graph project come to mind. Surrounding solution architects with many successful stories or case studies of successful enterprise knowledge graph projects may positively impact their probability of recomending an enterprise knowledge graph as an option. See also: Cognitive Bias Wikipedia Case Study A Priori and A Posteriori A priori knowledge is that which is independent of experience. A posteriori knowledge is that which depends on empirical evidence. The rules of mathematics, logic, and business rules are usually classified as a priori read-access is usually shared across all users of an enterprise knowledge graph. Knowledge about a specific observable event, observation, customer transaction, etc. are considered A Posteriori and may not need to be universally accessed. In an enterprise knowledge graph, a priori knowledge tends to be more universal such as TBox assertions that are universally accessible by all subgraphs . https://en.wikipedia.org/wiki/A_priori_and_a_posteriori Automatic Sharding The process of automatically migrating data from one server to another server in a distributed database. Auto-sharding is frequently done as a database cluster grows or shrinks based on new data being added or removed from the cluster. Auto-sharding is one of the key features that differentiate scale-out enterprise-class databases from departmental solutions. Testing sharding at scale under continuous load in the face of possible hardware failure is one of the key challenges facing enterprise data architects. Big Data An ambiguous term that many or may not refer to data sizes beyond the ability of commonly used tools to mange data. If you used a spreadsheet, \"Big Data\" could be any data that does not fit into your spreadsheet. If you use a Cray Supercomputer then your definition of Big Data could differ by ten orders of magnitude. If we ever hear someone refer to \"Big Data\" we strongly suggest they use other terms that have more precise meaning. Wikipedia Critique of Big Data Bitermporal Modeling A specific case of Temporal Modeling modeling designed to handle historical data in two different timelines. One timeline is concerned with when an event occurred in the real world and the other timeline is concerned when the data was recorded or corrected in a computer system. This makes it possible to rewind the information to \"as it actually was\" in combination with \"as it was recorded\" at some point in time. In order to implement this feature within an Enterprise Knowledge Graph, the data model must accommodate updates while preserving historical information. Information cannot be overwritten or discarded even if it is erroneous. The consequence is more data must be retained even through only a small percentage of queries might require historical views of data. Bitemporal models are more complex to query and require additional RAM and disk storage. Wikipedia Bitemporal Modeling Brain Analogies Explaining enterprise knowledge graphs in terms of the human brain. Human brains have roughly 82 billion neurons and 10,000 connections for each neuron. This is known as a graph degree of 10,000. Many enterprise knowledge graphs for the largest companies exceed 80 billion vertices but have only a handful of connections between them. See also: Degree . Business Event A change in the state of a business entity within an operational source system that may be published to a downstream consumer such as an enterprise knowledge graph. Business events are usually transmitted by Change Data Capture software and sent via document messages in formats such as JSON or XML. Wikipedia Business Vocabulary A collection of terms and phrases that have meaning to a specific domain of work. A business vocabulary typically starts out with a flat list of terms in a spreadsheet. The terms are listed with their abbreviations and definitions and how they are used within a specific project or department. As vocabularies grow and mature the individual terms might be grouped together. These groupings become taxonomies and can then be used to automatically classify documents with metadata tags of their preferred labels. Classified documents can have a dramatic increase on the search quality of a search engine. Change Data Capture Software that detects changes in a database and transmits the change information via business events to a remote system. These events are often published on stream processing systems using the publish/subscribe integration pattern. Also known as: CDC Wikipedia page on Change Data Capture Classification and Connection Within the data enrichment pattern there are two distinct phases. The first step is to take raw binary and numeric data streams and classify it according to the concept types in our knowledge graph. Once we have business entities identified we next need to connect our business entities together using the context around them in the data. These steps are called classification and connection. See also: Knowledge Triangle Cognitive Bias A systematic pattern of deviation from norm or rationality in judgment. In this book we study why humans don't adopt enterprise knowledge graph technology and how we can use stories, demonstrations and economic reasoning to overcome these bias. In this book we study several types of cognitive bias including: Anchoring bias Availability bias a.k.a. memory bias, familiarity heuristic Bandwagon effect Confirmation bias a.k.a. Fiter bubble Halo effect Hindsight bias Illusory superiority bias Framing effect Narrative-bias Representativeness heuristic Status_quo_bias Sunk cost a.k.a. Gamblers fallacy Wikipedia on Cognitive Bias Concept An idea, notion or a unit of thought. Concept elements are the fundamental unit of work in semantics and are in integral part of enterprise knowledge graphs. In practice, each concept is usually associated with a vertex in a graph and has one preferred label in each language such as English. Concepts may have many alternate labels. Concepts are grouped in Schemas and may be part of one or more Collections. Concept Reference on W3C SKOS Site Concept Graph A graph that stores the core business concepts of a project, department or enterprise. In the ideal world, an enterprise graph will use a combination of machine learning to connect related concepts together. Cost Sharing The ability of a single graph data model to be shared by many business units and thus the costs can also be shared. Lower charge backs make graph databases more cost-effective than other data models. See also: No Complexity Penalty Data Discovery A process of discovering new patterns in large data sets using a combination of query tools, machine learning and often visualization. Data discovery is contrasted with operational reporting which are regular consistent reports that are well known to users and good for spotting trends in datasets. Also known as: Data Mining Also known as: Knowledge discovery Wikipedia page on Data Mining Data Ingestion A process by which data is moved from one or more sources to a destination where it can be stored and further analyzed. The data might be in different formats and come from various sources, including RDBMS , other types of databases, S3 buckets, CSVs, or from streams. Data Layer Raw low-level binary codes that contains information after analysis. Dashboard A set of views, usually presented on a single page, that display information as a set of key performance indicators and charts. In general, dashboard views can be customized for a role or a specific user. Datamart A data warehouse used by a project or a department. Decomissioning datamarts is a key cost-driver to the adoption of enterprise knowledge graphs. Decision Tree A way of storing business rules in a graph. A decision tree contains a series of branches, each branch containing a conditional expression. If the conditional expression returns TRUE, then a true link is traversed. If the conditional returns FALSE and false branch is traversed. Decision trees and the corresponding rules that are represented as pointer hops in an enterprise knowledge graph have many integration and performance benefits. Denormalization A strategy used in relational database design to increase performance for a large number of read-operations that access multiple tables using computationally expensive JOIN operations. Denormalization is frequently used in online analytical processing systems . Although denomalization does increase performance, it also imposes a single departments requirements to optimize the reports relevant to their viewpoints. Denomalization destroys the shareability of data models and thus leads to duplication of information and the increase of enterprise costs. Decommissioning departmental datamarts is a common way to justify the costs of building enterprise knowledge graphs. Wikipedia page on Denormalization Degree The degree of a vertex is the count of the number of connections between the vertex and other vertices. The average degree of a graph is the average number of connections for a vertex. For non-directional graphs, counting is one per edge. For directional graphs that have reverse edges, each connection counts as two connections. In a directional graph, each vertex has both an in-degree and out-degree. Departmental Graph A graph designed to store information from one or more departments of an enterprise. Departmental graphs may be limited in that they can't be scaled up to hold enterprise data. DIKW Pyramid A visualization of the relationships between data, information, knowledge, and wisdom with data at the base followed by information, knowledge and finally wisdom at the top. Although commonly used in the field of we don't use the DIKW pyramid visualization in this book because the top wisdom layer is confusing in the context of an enterprise knowledge graph. We used the simpler three layer Knowledge Triangle . Wikipedia page on DIKW Pyramid Document Store A type of database that stores data as tree-structured data elements such as JSON or XML. Document stores use path-like query languages such as X-PATH to traverse the tree structure. Languages such as XQuery provide high-quality functional programming languages with strong type checking. X-PATH is a mature W3C standard for expressing path traversal using a rich array of standardized wildcard expressions. DB Engines A web site that harvests web documents that discuss databases and classifies the documents based on a taxonomy of database types. The \"Popularity changes per category report\" is frequently cited in many graph presentations. DB Engines Edge of Chaos The edge of chaos is narrow band between order and disorder in [complex systems] such as enterprise knowledge graphs. In enterprise knowledge graphs we think of order as areas we have modeled and understand well. We think of disorder as external areas that we have not yet modeled or we have determined that they are not worth the effort to model. Enterprise knowledge graphs modeling teams are often working at the Edge of Choas. Embedding A data structure, usually a vector of decimal numbers, associated with an item in a graph, that helps users quickly find similar items. Vertices, Edges, and Paths may all have embeddings. Endogenic Knowledge The knowledge that is modeled within your existing enterprise knowledge graph. Finding out if your current endogenic knowledge can promote adequate recommendations and predictions is a key strategy in enterprise graph evolution. In contrast, Exogenous Knowledge is the knowledge that is not modeled inside your current knowledge graph. Wikipedia page on Endogeneity Emergence When an entity is observed to have properties its parts do not have on their own, properties or behaviors which emerge only when the parts interact in a wider whole. Emergence is a primary reason to build enterprise knowledge graphs. Emergence allows us to find new insights in data that we could not find without connected data. Unfortunately, there are few ways to predict the rate and value of insights that emerge when we connect new knowledge into an enterprise knowledge graph. The value of emergence can be difficult to predict without a team that has experience with similar prior projects. Wikipedia Page on https://en.wikipedia.org/wiki/Emergence Employee Graph A graph representation of all your organization employees and their activities. For each employee, the graph may contain items such as reporting structure, job titles, roles, work history, education, training, certifications, current skills, security access groups, goals, projects, tasks assigned, helpdesk tickets, bugs assigned, bugs fixed, inventions, desktop hardware, software being used, software licenses, emails, meetings, salary and performance reviews. A detailed employee graph can be used to match available staff with new projects and find similar employees for career mentoring. Employee graphs can also be used to predict the impact of employees that leave an organization, what employees work as ambassadors between groups, and what teams will be the most productive. Due to confidentially reasons, sensitive employee data is often stored in a subgraph with specific [access controls]. Also know as: Human Resources Graph Also know as: Human Capital Graph Enterprise Knowledge Graph A scalable graph database system used to store large-scale connected information for an entire enterprise. By scalable we mean that it must be able to run on multiple servers as the graph expands. Without scalability the graph might be considered a project or departmental graph. For many large organizations, enterprise knowledge graphs typically have hundreds of developers doing concurrent loading and query development and the models can be dynamic. For example the Google Knowledge Graph team is thought to contain over 1,500 developers. Entity Resolution The process of finding records in a data set that refer to the same entity across different data sources. Entity Resolution is a core technique in converting Information layer data into a consistent knowledge graph. Wikipeia Record Linkage Exogenous Knowledge Knowledge \u201ccoming from outside\u201d of your Enterprise Knowledge Graph. Finding out what exogenous knowledge you need to make accurate predictions is an emerging area of enterprise knowledge graphs. In economic modeling, exogenous events means an influence that arises from outside the scope of your model and that is, therefore, neither predicted nor explained by the model. In contrast, Endogenic Knowledge is the knowledge that is modeled within your enterprise knowledge graph. Wikipedia page on Exogeny Force Directed Graph A graph layout algorithm that simulates forces on springs that move items Wikipedia Force Directed Graph Drawing Four Vs of Scalable Databases Volume, velocity, variability and veracity are considered the four Vs that define scalable systems. Volume refers to the total amount of data in our knowledge graph. Velocity means that new inserts, updates and deletes might be coming in fast via streaming events and these events must be ACID compliant and still never slow down read access times. Service levels agreements (SLAs) must focus not on total average times, but the averages of the slowest 5% of the transactions. Variability means that data is not uniform and can be easily stuffed into a single fact table of an OLAP cube. Veracity means we need to be able to validate the quality of incoming data in real-time and quickly raise warning flags if corrupt data is being transmitted into the EKG. Glossary A business vocabulary associated with a topic. A glossary often has both general definitions of terms as well as contextual definitions for a specific domain or project. See also: Business Vocabulary GraphQL A query language for APIs and a runtime for fulfilling those queries with your existing data. Ironically, GraphGL has nothing to do with graph databases other than the fact that the queries often run much faster on graphs. The name \"graph\" was used internally at FaceBook since they store their data in a graph structure. One concern about GraphQL at the enterprise-scale is that your graph database should be able to detect GraphQL queries that are using too many resources. This means your enterprise graph databases must understand concepts of resource quotas . Graph Query Language A proposed standard graph query language being developed by the Working Group 3 (Database Languages) of ISO/IEC JTC 1's Subcommittee 32. GQL is designed to work with Labeled Property Graphs . Wikipedia GQL Page Graph Structured Query Language A distributed graph query language developed by TigerGraph. GSQL was designed to be syntactically similar to the SQL language but it also integrated distributed query concepts that share patterns similar to MapReduce queries. Also known as: GSQL Graph Database A way of storing information in terms of vertices and edges. Graph databases consider edge traversal as a primary performance consideration. By storing edges as in-memory pointers graph databases offer roughly a 1,000x performance improvement over relational database management JOIN operations that must be calculated for each query. See also: Index Free Adjacency Graph Isomorphism A graph can exist in different forms having the same number of vertices, edges, and also the same edge connectivity. Such graphs are called isomorphic graphs. Hedgehog vs Fox Modeling Focus on accurate modeling a single domain or subgraph of an enterprise knowledge graph (the Hedgehog) vs general modeling of a wide variety of subgraphs or domains. The term comes from Archilochus who stated \"a fox knows many things, but a hedgehog knows one important thing\". There are pros and cons for taking different approaches. No single strategy will work for all enterprise knowledge graphs at all times. The involvement of subject-matter experts (hedgehogs) at different times in the lifecycle of an enterprise knowledge graph will impact the evolution of enterprise knowledge graphs. Higher Order Knowledge A height-related metaphor that is used to describe more abstract knowledge that is more universal in an enterprise knowledge graph. The concept of \"height\" is related to the layers of the Knowledge Triange . For example, the idea behind \"higher-order thinking\" is that some types of learning requires more cognitive processing than others, but also have more generalized benefits. Within knowledge graphs this may not translate into more CPU time for query traversal, but may depend on having more abstract vertices and edges in an upper or mid-range ontology. Wikipedia page on Higher Order Thinking Index Free Adjacency Accessing related entities in a system without having to consult a centralized index. Using direct in-memory pointers to represent relationships is approximately three orders of magnitude faster than referencing a central index system. See also: The Neighborhood Walk Story Information Layer Data about our key business entities. This includes Things, like People, Places and Events. Inmon Data Warehouse The Inmon Data Warehouse is a collection of database design patterns that promote analytics using relational databases promoted by Bill Inmon. The Inomn approach was first enumerated in his 1992 book \"Building the Data Warehouse\". The Inmon approach is usually contrasted to the more recent 2013 Kimball Data Warehouse that focuses on a simplicity and single fact table with many dimensions. Many EKG projects can be funded by their ability to show they can decommission expensive Inmon-style data warehouses that don't have the flexibility of EKGs. Wikipedia page on Bill Inmon Key-Value Store A type of database that stores items as pairs of keys and values. The keys are strings and the values are binary blobs such as files or images. A simple put/get/delete interface is used to manage the database. Key-value stores are excellent complements to graph databases since their simplicity allows for low-cost-per-byte storage. Kimball Data Warehouse A data warehouse design pattern that uses a single fact table joined with dimensional tables to minimize the impact of JOIN statements in reporting performance. Kimball data warehouses are the ultimate in denormalized database design . Their goal is often simplicity at the expense of capturing complex relationship-intensive models of the world that can be reused across the enterprise. As a result, Kimball datamarts duplicate data in each department and each datamart has their own costs to perform ETL operations. In contrast, EKGs focus on highly [normalized] data models of the world that include many complex relationships. This closer our models get to the real world the more they can be reused across many departments. Decomissioning many departmental datamarts is often a key way to justify EKG projects. Wikipedia Page on Ralph Kimball Knowledge Layer A layer in the knowledge triangle that contains connected information. The knowledge layer is often the top layer in our views. There are some views that include a Wisdom layer on top of the knowledge layer. Knowledge Graph A set of interconnected typed entities and their attributes. Entities can be any business objects, customers, products, parts, documents, employees or concepts. Entities are usually implemented as vertices in a graph database and connected through edges. In some types of graphs, for example LPGs , edges also have attributes. Note that this definition has no dependence on semantics and inference. Our definition is intentionally designed to include many types of interconnected datasets. We think your organizational chart is a type of knowledge graph an may be a subgraph of your enterprise knowledge graph. Knowledge Representation The process of representing information (individual facts) about the world in a form that a computer system can utilize to solve complex tasks. Within the context of the enterprise knowledge graph, we used graph databases as our primary way to store knowledge and we complement graph databases with search engines and key-value stores when they are more efficient. There is no single knowledge reprenstation that is ideal for all problems. Graph database are the preferred way to store knowledge because efficient reasoning can be implemented as fast pointer-hopping operations that can be optimized by specialized hardware. Knowledge representation is often the most complex challenge in the field of Artificial Intelligence. Wikipedia Page on Knowledge Representation Knowledge Triangle A stack of three layers that illustrates how knowledge graphs are constructed from raw data. At the base is the Data Layer that stores raw binary data in numeric forms, above that is Information Layer that finds concepts and business entities within the data layer. At the top the triangle is the Knowledge Layer where business entities are connected to make them easy to query using graph traversal algorithms. See also: The DIKW Pyramid Label A string associated with one or more Concepts . Labels have two main types: Preferred Labels and Alternate Labels . Most formal ontologies and taxonomies only permit a single preferred label for each Concept in a given Language. Labeled Property Graph A graph data model where each Vertex and Edge have a single type and goth Vertices and Edges have attributes. Both TigerGraph and Neo4j use the LPG data model. Also known as: LPG Load-As-Is Pattern A data loading pattern that loads the data into a graph with minimal transformation. Once the data is loaded into the graph the transformation is done in the native language of the graph such as GSQL. This pattern allows many projects to share the underlying data loaders and allows each team to customize the post-loading transformation using the native query language of the graph. The other major data model is the RDF model which is discouraged at Optum due to the challenges with Reification. Reification causes RDF SPARQL queries to be rewritten. Load-as-is pattern See also: RDF The Neighborhood Walk Story A story used to illustrate the difference between direct pointer hopping and using centralized indexes to traverse relationships. The story uses a 30-second walk between two houses vs. an 8.2-hour walk to a central location and back. No Complexity Penalty Unlike relational databases, graph databases quickly traverse many complex relationships. As a result, graph databases are better at modeling the real world - which is full of complexity. We use the phrase \"No Complexity Penalty\" every time we are training people who have come from the relational world that worry that too many relationships will slow down their queries due to slow JOINs. See also: One version of the truth One Version of the Truth The real world has many complex relationships. There are many ways to build simple models that take shortcuts to optimize queries by limiting relationships. This is important in relational database modeling. But the closer we get to modeling the real world, the closer to a single version of the truth we get. Models that fairly represent the complexities of the real world can be reused among many business units and thus the costs of holding the information in memory can be shared. This is why graph databases cost less then relational databases. Online Analytical Processing System An approach to answer multi-dimensional analytical queries quickly by minimizing JOIN operations in relational databases. OLAP \"cubes\" often use a star schema with a central fact table and one JOIN operation per dimension of the cube. The denomalization process used to create star schemas limits enterprise-sharing of these structures. Wikipedia On-the-Wire vs. In-the-Can A way of looking at knowledge representation requirements in two domains. On-The-Wire implies that serialization of a dataset must retain connection information within itself and to other external systems. In-The-Can knowledge representations are optimized for ease of query and sustainability . RDF is optimized for On-The-Wire exchange of knowledge. LPG is optimized for In-The-Can tasks such as ease of query and sustainability. Ontology A graph of Concepts within a specific domain. Ontologies often begin as flat term lists, that become taxonomies that then have more complex relationships than simple broader and narrower concepts. Ontologies are often stored in formats such as SKOS and OWL. Open vs Closed World https://en.wikipedia.org/wiki/Open-world_assumption https://en.wikipedia.org/wiki/Closed-world_assumption Operational Source System A transactional computer that is the source of a data stream. Enterprise Graph Databases often use Change Data Capture software on these systems to create an event stream of change records that so they can be stored in a central enterprise knowledge graph. Change records are new, updated, or deleted business entities. PageRank A graph algorithm that is used to rank the most influential vertices in a directed graph. For example web pages in a graph of linked web pages. PageRank was first used by Google Search to rank web pages in their search engine results. The patent for PageRank (now expired) was purchased by Google from Standford University for Google shares. Those shares sold for over $336 million USD when Google went public. Wikipedia Preferred Label A preferred lexical label associated with a Concept . In the SKOS standard, there should be one and only one preferred label per language per concept. Project Graph A graph that supports a specific project. Project graphs may contain knowledge that is not of interest to the rest of the enterprise. Reference Data Reference data is data used to classify or categorize other data. They typically are stored as a set of valid codes for a specific data element. For example the list of Country Codes is a type of reference data. Reference data is often stored as a short code and a definition of what that code represents. Reification Reification is the process by which an abstract idea about a computer program is turned into an explicit data model or other object created in a programming language. Specifically, in the RDF modeling process it is the process of adding an abstract vertex to a graph when properties are needed in a relationship. Reification causes queries that traverse that node to be rewritten. This means that SPARQL queries are inherently much more difficult to maintain than LPG graph queries. Resource Description Framework An early family of standards developed by the World Wide Web Consortium for exchanging graph data championed by the Semantic Web community starting in 1999. RDF gained some traction around 2010 but failed to gain widespread adoption due to the complexity of the standards and the problems of Reification . Wikipedia Resource Quota The ability to limit the resources consumed by a query such as CPU time, or RAM for individuals or groups. Large enterprise-scale graph databases must carefully monitor and constrain queries that consume too many resources. Many older technologies such as Apache Drill are difficult to implement without the ability to monitor and restrict resources. Role-based Access Control The ability to assign access to a resource to individuals that have a specific role. For Enterprise Knowledge Graphs, there are both high-level subgraph rules and fine-grain rules such as vertex-related role-based access control. Rules Engine A software component that executes rules according to some algorithm. In the Enterprise Knowledge Graph space rules are frequently represented in Decision Tree structures within the graph. [Rules for Knowledge Graph Rules])https://dmccreary.medium.com/rules-for-knowledge-graphs-rules-f22587307a8f Semantics The branch of computer science associated with meaning. It can be best understood by understanding the semantic triangle. The key point of the semantic triangle is that we cannot directly associate a label with a referent without traversing concepts. Semantic Graph A graph where each vertex represents a Concept and the edges of the graph represent the relationships between the Concepts. The primary data model for storing semantic graphs is the SKOS data model where Concepts and Labels are distinct types. Shapes Constraint Language A W3C standard RDF vocabulary for validating RDF graphs against a set of conditions. Unlike document validation standards like XML Schema, SHACL assumes that data quality checks should be able to look for relationships in a graph as well as the local context of a document. These conditions are provided as shapes and other constructs expressed in the form of an RDF graph. LPG graphs do not yet have a version of SHACL. Also known as: SHACL SHACL W3C Simple Knowledge Organizational System A model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabularies. SKOS is also the name of the world-wide-web standard for encoding these systems. Serializations of SKOS are typically done in RDF format although other encodings such as XML and JSON are common. See Also: W3C SKOS Primer See Also: W2C SKOS Referecnce Strategy Graph A graph that is used to help determine what strategies might be optimal for an enterprise or a subgroup as well as how organizations are performing on a specific strategy. To be successful, enterprise and departmental strategies must be encoded in machine-readable forms such as StratML and loaded into an enterprise knowledge graph. Strategy graphs can also be used to determine the alignment of proposed projects for the future in an organization. StratML encoding is sometimes required of US federal organizations so that strategies can be analyzed by published public documents. Strategic Serendipity Building a enterprise strategy around the creation of an environment where it is easier to make unexpected connections between items. Strategic serendipity involves getting a large number of people ready to discover new things in an enterprise knowledge graph. Subgraph A subset of an enterprise knowledge graph that may store specific types of knowledge and may have specific access control rules based on the role of a user. For example, a business glossary, taxonomy, ontology or business rules system that contains no customer-specific information may be in one subgraph and have universal read-access for all users. Customer-specific data that is highly confidential may be stored in a different subgraph, with read access only granted on a need-to-know basis. See also: Role Based Access Control Sustainability The ability for a team of developers to maintain the code that supports an enterprise knowledge graph in the face of changes to the data model. The key measure is to avoid problems related to rewriting graph queries when small changes are made to the graph data model. See also: The Jenga Tower Story Systems Thinking A way of looking at problems in terms of components that interact with each other over time using direct connections, indirection connections and both positive and negative feedback cycles. Systems Thinking forces us to think broadly about how our enterprise knowledge graphs interact with external systems. Systems thinking also helps us see the unintended consequences of our actions. TBox A \"terminological component\" or terminology Concept associated with a set of facts assertions (ABox statements) of a knowledge graph. TBox statements tend to more rules or metarules (rules about rules) that individual fact about customers or other business entities. See also: ABox Wikipedia Tbox page Technology Adoption Life Cycle A sociological model that describes the adoption or acceptance of a new product or innovation, according to the demographic and psychological characteristics of defined adopter groups. Wikipedia Temporal Modeling The process of modeling time in a data model. Modeling time can be complex when the requirements of a system require you to be able to recreate detailed reports as they were at a prior point in time. Temporal modeling includes the concept of versioning and bitemporal modeling Triple Store A purpose-built database for the storage and retrieval of RDF triples through semantic queries. Triple stores are not used in most enterprise graphs due to their lack of sustanability due to problems with Reification . https://en.wikipedia.org/wiki/Triplestore Upper Ontology General high-level Concepts that are common across all domains in a knowledge graph. Concepts such as Organization, Customer, Family Unit, Product, Part, Invoice, Document are often consider part of an upper ontology. Enterprise graphs may contain multiple ontologies and the ease of linking ontologies in highly dependant on sharing upper ontologies. Wikipedia Page on Upper Ontology Web Ontology Language A Semantic Web language designed to represent rich and complex knowledge about things, groups of things, and relations between things. W3C OWL Web Site Window of Opportunity A narrow band of time that an organization might be ready to adopt a new technology such as an enterprise knowledge graph. To arrive at the Window of Opportunity to adapt an enterprise knowledge graph an organization must meet a set of preconditions such as an internal chamption, a shared understanding of what enterprise knowledge graphs are capable of, and the ability to ingest enough information to achieve specific business objectives. Enterprise knowledge graphs then have a limited time to create a pilot project before funding runs out, champions move on, or alternative technologies get mindshare. Very often a specific chrisis can trigger an unexpcted window of opportunity. Knowing how to recognize these windows and take advantage of them is a key skill we attempt to explore in this book. Wikipedia article on Window of Opportunity","title":"Glossary"},{"location":"glossary/#enterprise-knowledge-graph-glossary-of-terms","text":"","title":"Enterprise Knowledge Graph Glossary of Terms"},{"location":"glossary/#abox","text":"Types of assertions or statements in a knowledge graph that conform to specific terminologies (knows as TBox statements). The terms ABox and TBox are used to help determine if a statement is universal or related to a specific Subgraph of an Enterprises Knowledge Graph . Within our Enterprise Knowledge Graph architecture, ABox statements often contain knowledge about specific customers, parts, or concepts and may have specific Access Control rules. TBox statements don't usually have these same access rules. See also: TBox Wikipedia page on ABox","title":"ABox"},{"location":"glossary/#accumulator","text":"A type of variable that tracks items as you traverse through a graph. Accumulators can be global or be attached to a specific vertex. Accumulators allow MapReduce style queries where each server node in a cluster does work in its local data and returns consolidated results to the query node. For example, in the query \"count all customers that have returned clothing items\" , each node would return only a single count to the query node. This type of query reduces the amount of communication between nodes in the graph cluster.","title":"Accumulator"},{"location":"glossary/#alternate-label","text":"In a concept subgraph of a EKG, every concept has a single preferred label (per language) and many alternate labels that can also be used to name a concept. Alternate labels can be abbreviations, acronyms and synonyms of the concept. In this glossary, alternate labels have a \"Also known as\" prefix. Also known as: atlLabel See also: Preferred Label See also: SKOS","title":"Alternate Label"},{"location":"glossary/#availability-heuristic","text":"The availability heuristic suggests that the likelihood of events is estimated based on how many examples of such events come to mind. For example, when a solution architect is presented with a business problem, their likelihood to recommend an enterprise knowledge graph solution is dependant on how many examples of successful enterprise knowledge graph project come to mind. Surrounding solution architects with many successful stories or case studies of successful enterprise knowledge graph projects may positively impact their probability of recomending an enterprise knowledge graph as an option. See also: Cognitive Bias Wikipedia Case Study","title":"Availability Heuristic"},{"location":"glossary/#a-priori-and-a-posteriori","text":"A priori knowledge is that which is independent of experience. A posteriori knowledge is that which depends on empirical evidence. The rules of mathematics, logic, and business rules are usually classified as a priori read-access is usually shared across all users of an enterprise knowledge graph. Knowledge about a specific observable event, observation, customer transaction, etc. are considered A Posteriori and may not need to be universally accessed. In an enterprise knowledge graph, a priori knowledge tends to be more universal such as TBox assertions that are universally accessible by all subgraphs . https://en.wikipedia.org/wiki/A_priori_and_a_posteriori","title":"A Priori and A Posteriori"},{"location":"glossary/#automatic-sharding","text":"The process of automatically migrating data from one server to another server in a distributed database. Auto-sharding is frequently done as a database cluster grows or shrinks based on new data being added or removed from the cluster. Auto-sharding is one of the key features that differentiate scale-out enterprise-class databases from departmental solutions. Testing sharding at scale under continuous load in the face of possible hardware failure is one of the key challenges facing enterprise data architects.","title":"Automatic Sharding"},{"location":"glossary/#big-data","text":"An ambiguous term that many or may not refer to data sizes beyond the ability of commonly used tools to mange data. If you used a spreadsheet, \"Big Data\" could be any data that does not fit into your spreadsheet. If you use a Cray Supercomputer then your definition of Big Data could differ by ten orders of magnitude. If we ever hear someone refer to \"Big Data\" we strongly suggest they use other terms that have more precise meaning. Wikipedia Critique of Big Data","title":"Big Data"},{"location":"glossary/#bitermporal-modeling","text":"A specific case of Temporal Modeling modeling designed to handle historical data in two different timelines. One timeline is concerned with when an event occurred in the real world and the other timeline is concerned when the data was recorded or corrected in a computer system. This makes it possible to rewind the information to \"as it actually was\" in combination with \"as it was recorded\" at some point in time. In order to implement this feature within an Enterprise Knowledge Graph, the data model must accommodate updates while preserving historical information. Information cannot be overwritten or discarded even if it is erroneous. The consequence is more data must be retained even through only a small percentage of queries might require historical views of data. Bitemporal models are more complex to query and require additional RAM and disk storage. Wikipedia Bitemporal Modeling","title":"Bitermporal Modeling"},{"location":"glossary/#brain-analogies","text":"Explaining enterprise knowledge graphs in terms of the human brain. Human brains have roughly 82 billion neurons and 10,000 connections for each neuron. This is known as a graph degree of 10,000. Many enterprise knowledge graphs for the largest companies exceed 80 billion vertices but have only a handful of connections between them. See also: Degree .","title":"Brain Analogies"},{"location":"glossary/#business-event","text":"A change in the state of a business entity within an operational source system that may be published to a downstream consumer such as an enterprise knowledge graph. Business events are usually transmitted by Change Data Capture software and sent via document messages in formats such as JSON or XML. Wikipedia","title":"Business Event"},{"location":"glossary/#business-vocabulary","text":"A collection of terms and phrases that have meaning to a specific domain of work. A business vocabulary typically starts out with a flat list of terms in a spreadsheet. The terms are listed with their abbreviations and definitions and how they are used within a specific project or department. As vocabularies grow and mature the individual terms might be grouped together. These groupings become taxonomies and can then be used to automatically classify documents with metadata tags of their preferred labels. Classified documents can have a dramatic increase on the search quality of a search engine.","title":"Business Vocabulary"},{"location":"glossary/#change-data-capture","text":"Software that detects changes in a database and transmits the change information via business events to a remote system. These events are often published on stream processing systems using the publish/subscribe integration pattern. Also known as: CDC Wikipedia page on Change Data Capture","title":"Change Data Capture"},{"location":"glossary/#classification-and-connection","text":"Within the data enrichment pattern there are two distinct phases. The first step is to take raw binary and numeric data streams and classify it according to the concept types in our knowledge graph. Once we have business entities identified we next need to connect our business entities together using the context around them in the data. These steps are called classification and connection. See also: Knowledge Triangle","title":"Classification and Connection"},{"location":"glossary/#cognitive-bias","text":"A systematic pattern of deviation from norm or rationality in judgment. In this book we study why humans don't adopt enterprise knowledge graph technology and how we can use stories, demonstrations and economic reasoning to overcome these bias. In this book we study several types of cognitive bias including: Anchoring bias Availability bias a.k.a. memory bias, familiarity heuristic Bandwagon effect Confirmation bias a.k.a. Fiter bubble Halo effect Hindsight bias Illusory superiority bias Framing effect Narrative-bias Representativeness heuristic Status_quo_bias Sunk cost a.k.a. Gamblers fallacy Wikipedia on Cognitive Bias","title":"Cognitive Bias"},{"location":"glossary/#concept","text":"An idea, notion or a unit of thought. Concept elements are the fundamental unit of work in semantics and are in integral part of enterprise knowledge graphs. In practice, each concept is usually associated with a vertex in a graph and has one preferred label in each language such as English. Concepts may have many alternate labels. Concepts are grouped in Schemas and may be part of one or more Collections. Concept Reference on W3C SKOS Site","title":"Concept"},{"location":"glossary/#concept-graph","text":"A graph that stores the core business concepts of a project, department or enterprise. In the ideal world, an enterprise graph will use a combination of machine learning to connect related concepts together.","title":"Concept Graph"},{"location":"glossary/#cost-sharing","text":"The ability of a single graph data model to be shared by many business units and thus the costs can also be shared. Lower charge backs make graph databases more cost-effective than other data models. See also: No Complexity Penalty","title":"Cost Sharing"},{"location":"glossary/#data-discovery","text":"A process of discovering new patterns in large data sets using a combination of query tools, machine learning and often visualization. Data discovery is contrasted with operational reporting which are regular consistent reports that are well known to users and good for spotting trends in datasets. Also known as: Data Mining Also known as: Knowledge discovery Wikipedia page on Data Mining","title":"Data Discovery"},{"location":"glossary/#data-ingestion","text":"A process by which data is moved from one or more sources to a destination where it can be stored and further analyzed. The data might be in different formats and come from various sources, including RDBMS , other types of databases, S3 buckets, CSVs, or from streams.","title":"Data Ingestion"},{"location":"glossary/#data-layer","text":"Raw low-level binary codes that contains information after analysis.","title":"Data Layer"},{"location":"glossary/#dashboard","text":"A set of views, usually presented on a single page, that display information as a set of key performance indicators and charts. In general, dashboard views can be customized for a role or a specific user.","title":"Dashboard"},{"location":"glossary/#datamart","text":"A data warehouse used by a project or a department. Decomissioning datamarts is a key cost-driver to the adoption of enterprise knowledge graphs.","title":"Datamart"},{"location":"glossary/#decision-tree","text":"A way of storing business rules in a graph. A decision tree contains a series of branches, each branch containing a conditional expression. If the conditional expression returns TRUE, then a true link is traversed. If the conditional returns FALSE and false branch is traversed. Decision trees and the corresponding rules that are represented as pointer hops in an enterprise knowledge graph have many integration and performance benefits.","title":"Decision Tree"},{"location":"glossary/#denormalization","text":"A strategy used in relational database design to increase performance for a large number of read-operations that access multiple tables using computationally expensive JOIN operations. Denormalization is frequently used in online analytical processing systems . Although denomalization does increase performance, it also imposes a single departments requirements to optimize the reports relevant to their viewpoints. Denomalization destroys the shareability of data models and thus leads to duplication of information and the increase of enterprise costs. Decommissioning departmental datamarts is a common way to justify the costs of building enterprise knowledge graphs. Wikipedia page on Denormalization","title":"Denormalization"},{"location":"glossary/#degree","text":"The degree of a vertex is the count of the number of connections between the vertex and other vertices. The average degree of a graph is the average number of connections for a vertex. For non-directional graphs, counting is one per edge. For directional graphs that have reverse edges, each connection counts as two connections. In a directional graph, each vertex has both an in-degree and out-degree.","title":"Degree"},{"location":"glossary/#departmental-graph","text":"A graph designed to store information from one or more departments of an enterprise. Departmental graphs may be limited in that they can't be scaled up to hold enterprise data.","title":"Departmental Graph"},{"location":"glossary/#dikw-pyramid","text":"A visualization of the relationships between data, information, knowledge, and wisdom with data at the base followed by information, knowledge and finally wisdom at the top. Although commonly used in the field of we don't use the DIKW pyramid visualization in this book because the top wisdom layer is confusing in the context of an enterprise knowledge graph. We used the simpler three layer Knowledge Triangle . Wikipedia page on DIKW Pyramid","title":"DIKW Pyramid"},{"location":"glossary/#document-store","text":"A type of database that stores data as tree-structured data elements such as JSON or XML. Document stores use path-like query languages such as X-PATH to traverse the tree structure. Languages such as XQuery provide high-quality functional programming languages with strong type checking. X-PATH is a mature W3C standard for expressing path traversal using a rich array of standardized wildcard expressions.","title":"Document Store"},{"location":"glossary/#db-engines","text":"A web site that harvests web documents that discuss databases and classifies the documents based on a taxonomy of database types. The \"Popularity changes per category report\" is frequently cited in many graph presentations. DB Engines","title":"DB Engines"},{"location":"glossary/#edge-of-chaos","text":"The edge of chaos is narrow band between order and disorder in [complex systems] such as enterprise knowledge graphs. In enterprise knowledge graphs we think of order as areas we have modeled and understand well. We think of disorder as external areas that we have not yet modeled or we have determined that they are not worth the effort to model. Enterprise knowledge graphs modeling teams are often working at the Edge of Choas.","title":"Edge of Chaos"},{"location":"glossary/#embedding","text":"A data structure, usually a vector of decimal numbers, associated with an item in a graph, that helps users quickly find similar items. Vertices, Edges, and Paths may all have embeddings.","title":"Embedding"},{"location":"glossary/#endogenic-knowledge","text":"The knowledge that is modeled within your existing enterprise knowledge graph. Finding out if your current endogenic knowledge can promote adequate recommendations and predictions is a key strategy in enterprise graph evolution. In contrast, Exogenous Knowledge is the knowledge that is not modeled inside your current knowledge graph. Wikipedia page on Endogeneity","title":"Endogenic Knowledge"},{"location":"glossary/#emergence","text":"When an entity is observed to have properties its parts do not have on their own, properties or behaviors which emerge only when the parts interact in a wider whole. Emergence is a primary reason to build enterprise knowledge graphs. Emergence allows us to find new insights in data that we could not find without connected data. Unfortunately, there are few ways to predict the rate and value of insights that emerge when we connect new knowledge into an enterprise knowledge graph. The value of emergence can be difficult to predict without a team that has experience with similar prior projects. Wikipedia Page on https://en.wikipedia.org/wiki/Emergence","title":"Emergence"},{"location":"glossary/#employee-graph","text":"A graph representation of all your organization employees and their activities. For each employee, the graph may contain items such as reporting structure, job titles, roles, work history, education, training, certifications, current skills, security access groups, goals, projects, tasks assigned, helpdesk tickets, bugs assigned, bugs fixed, inventions, desktop hardware, software being used, software licenses, emails, meetings, salary and performance reviews. A detailed employee graph can be used to match available staff with new projects and find similar employees for career mentoring. Employee graphs can also be used to predict the impact of employees that leave an organization, what employees work as ambassadors between groups, and what teams will be the most productive. Due to confidentially reasons, sensitive employee data is often stored in a subgraph with specific [access controls]. Also know as: Human Resources Graph Also know as: Human Capital Graph","title":"Employee Graph"},{"location":"glossary/#enterprise-knowledge-graph","text":"A scalable graph database system used to store large-scale connected information for an entire enterprise. By scalable we mean that it must be able to run on multiple servers as the graph expands. Without scalability the graph might be considered a project or departmental graph. For many large organizations, enterprise knowledge graphs typically have hundreds of developers doing concurrent loading and query development and the models can be dynamic. For example the Google Knowledge Graph team is thought to contain over 1,500 developers.","title":"Enterprise Knowledge Graph"},{"location":"glossary/#entity-resolution","text":"The process of finding records in a data set that refer to the same entity across different data sources. Entity Resolution is a core technique in converting Information layer data into a consistent knowledge graph. Wikipeia Record Linkage","title":"Entity Resolution"},{"location":"glossary/#exogenous-knowledge","text":"Knowledge \u201ccoming from outside\u201d of your Enterprise Knowledge Graph. Finding out what exogenous knowledge you need to make accurate predictions is an emerging area of enterprise knowledge graphs. In economic modeling, exogenous events means an influence that arises from outside the scope of your model and that is, therefore, neither predicted nor explained by the model. In contrast, Endogenic Knowledge is the knowledge that is modeled within your enterprise knowledge graph. Wikipedia page on Exogeny","title":"Exogenous Knowledge"},{"location":"glossary/#force-directed-graph","text":"A graph layout algorithm that simulates forces on springs that move items Wikipedia Force Directed Graph Drawing","title":"Force Directed Graph"},{"location":"glossary/#four-vs-of-scalable-databases","text":"Volume, velocity, variability and veracity are considered the four Vs that define scalable systems. Volume refers to the total amount of data in our knowledge graph. Velocity means that new inserts, updates and deletes might be coming in fast via streaming events and these events must be ACID compliant and still never slow down read access times. Service levels agreements (SLAs) must focus not on total average times, but the averages of the slowest 5% of the transactions. Variability means that data is not uniform and can be easily stuffed into a single fact table of an OLAP cube. Veracity means we need to be able to validate the quality of incoming data in real-time and quickly raise warning flags if corrupt data is being transmitted into the EKG.","title":"Four Vs of Scalable Databases"},{"location":"glossary/#glossary","text":"A business vocabulary associated with a topic. A glossary often has both general definitions of terms as well as contextual definitions for a specific domain or project. See also: Business Vocabulary","title":"Glossary"},{"location":"glossary/#graphql","text":"A query language for APIs and a runtime for fulfilling those queries with your existing data. Ironically, GraphGL has nothing to do with graph databases other than the fact that the queries often run much faster on graphs. The name \"graph\" was used internally at FaceBook since they store their data in a graph structure. One concern about GraphQL at the enterprise-scale is that your graph database should be able to detect GraphQL queries that are using too many resources. This means your enterprise graph databases must understand concepts of resource quotas .","title":"GraphQL"},{"location":"glossary/#graph-query-language","text":"A proposed standard graph query language being developed by the Working Group 3 (Database Languages) of ISO/IEC JTC 1's Subcommittee 32. GQL is designed to work with Labeled Property Graphs . Wikipedia GQL Page","title":"Graph Query Language"},{"location":"glossary/#graph-structured-query-language","text":"A distributed graph query language developed by TigerGraph. GSQL was designed to be syntactically similar to the SQL language but it also integrated distributed query concepts that share patterns similar to MapReduce queries. Also known as: GSQL","title":"Graph Structured Query Language"},{"location":"glossary/#graph-database","text":"A way of storing information in terms of vertices and edges. Graph databases consider edge traversal as a primary performance consideration. By storing edges as in-memory pointers graph databases offer roughly a 1,000x performance improvement over relational database management JOIN operations that must be calculated for each query. See also: Index Free Adjacency","title":"Graph Database"},{"location":"glossary/#graph-isomorphism","text":"A graph can exist in different forms having the same number of vertices, edges, and also the same edge connectivity. Such graphs are called isomorphic graphs.","title":"Graph Isomorphism"},{"location":"glossary/#hedgehog-vs-fox-modeling","text":"Focus on accurate modeling a single domain or subgraph of an enterprise knowledge graph (the Hedgehog) vs general modeling of a wide variety of subgraphs or domains. The term comes from Archilochus who stated \"a fox knows many things, but a hedgehog knows one important thing\". There are pros and cons for taking different approaches. No single strategy will work for all enterprise knowledge graphs at all times. The involvement of subject-matter experts (hedgehogs) at different times in the lifecycle of an enterprise knowledge graph will impact the evolution of enterprise knowledge graphs.","title":"Hedgehog vs Fox Modeling"},{"location":"glossary/#higher-order-knowledge","text":"A height-related metaphor that is used to describe more abstract knowledge that is more universal in an enterprise knowledge graph. The concept of \"height\" is related to the layers of the Knowledge Triange . For example, the idea behind \"higher-order thinking\" is that some types of learning requires more cognitive processing than others, but also have more generalized benefits. Within knowledge graphs this may not translate into more CPU time for query traversal, but may depend on having more abstract vertices and edges in an upper or mid-range ontology. Wikipedia page on Higher Order Thinking","title":"Higher Order Knowledge"},{"location":"glossary/#index-free-adjacency","text":"Accessing related entities in a system without having to consult a centralized index. Using direct in-memory pointers to represent relationships is approximately three orders of magnitude faster than referencing a central index system. See also: The Neighborhood Walk Story","title":"Index Free Adjacency"},{"location":"glossary/#information-layer","text":"Data about our key business entities. This includes Things, like People, Places and Events.","title":"Information Layer"},{"location":"glossary/#inmon-data-warehouse","text":"The Inmon Data Warehouse is a collection of database design patterns that promote analytics using relational databases promoted by Bill Inmon. The Inomn approach was first enumerated in his 1992 book \"Building the Data Warehouse\". The Inmon approach is usually contrasted to the more recent 2013 Kimball Data Warehouse that focuses on a simplicity and single fact table with many dimensions. Many EKG projects can be funded by their ability to show they can decommission expensive Inmon-style data warehouses that don't have the flexibility of EKGs. Wikipedia page on Bill Inmon","title":"Inmon Data Warehouse"},{"location":"glossary/#key-value-store","text":"A type of database that stores items as pairs of keys and values. The keys are strings and the values are binary blobs such as files or images. A simple put/get/delete interface is used to manage the database. Key-value stores are excellent complements to graph databases since their simplicity allows for low-cost-per-byte storage.","title":"Key-Value Store"},{"location":"glossary/#kimball-data-warehouse","text":"A data warehouse design pattern that uses a single fact table joined with dimensional tables to minimize the impact of JOIN statements in reporting performance. Kimball data warehouses are the ultimate in denormalized database design . Their goal is often simplicity at the expense of capturing complex relationship-intensive models of the world that can be reused across the enterprise. As a result, Kimball datamarts duplicate data in each department and each datamart has their own costs to perform ETL operations. In contrast, EKGs focus on highly [normalized] data models of the world that include many complex relationships. This closer our models get to the real world the more they can be reused across many departments. Decomissioning many departmental datamarts is often a key way to justify EKG projects. Wikipedia Page on Ralph Kimball","title":"Kimball Data Warehouse"},{"location":"glossary/#knowledge-layer","text":"A layer in the knowledge triangle that contains connected information. The knowledge layer is often the top layer in our views. There are some views that include a Wisdom layer on top of the knowledge layer.","title":"Knowledge Layer"},{"location":"glossary/#knowledge-graph","text":"A set of interconnected typed entities and their attributes. Entities can be any business objects, customers, products, parts, documents, employees or concepts. Entities are usually implemented as vertices in a graph database and connected through edges. In some types of graphs, for example LPGs , edges also have attributes. Note that this definition has no dependence on semantics and inference. Our definition is intentionally designed to include many types of interconnected datasets. We think your organizational chart is a type of knowledge graph an may be a subgraph of your enterprise knowledge graph.","title":"Knowledge Graph"},{"location":"glossary/#knowledge-representation","text":"The process of representing information (individual facts) about the world in a form that a computer system can utilize to solve complex tasks. Within the context of the enterprise knowledge graph, we used graph databases as our primary way to store knowledge and we complement graph databases with search engines and key-value stores when they are more efficient. There is no single knowledge reprenstation that is ideal for all problems. Graph database are the preferred way to store knowledge because efficient reasoning can be implemented as fast pointer-hopping operations that can be optimized by specialized hardware. Knowledge representation is often the most complex challenge in the field of Artificial Intelligence. Wikipedia Page on Knowledge Representation","title":"Knowledge Representation"},{"location":"glossary/#knowledge-triangle","text":"A stack of three layers that illustrates how knowledge graphs are constructed from raw data. At the base is the Data Layer that stores raw binary data in numeric forms, above that is Information Layer that finds concepts and business entities within the data layer. At the top the triangle is the Knowledge Layer where business entities are connected to make them easy to query using graph traversal algorithms. See also: The DIKW Pyramid","title":"Knowledge Triangle"},{"location":"glossary/#label","text":"A string associated with one or more Concepts . Labels have two main types: Preferred Labels and Alternate Labels . Most formal ontologies and taxonomies only permit a single preferred label for each Concept in a given Language.","title":"Label"},{"location":"glossary/#labeled-property-graph","text":"A graph data model where each Vertex and Edge have a single type and goth Vertices and Edges have attributes. Both TigerGraph and Neo4j use the LPG data model. Also known as: LPG","title":"Labeled Property Graph"},{"location":"glossary/#load-as-is-pattern","text":"A data loading pattern that loads the data into a graph with minimal transformation. Once the data is loaded into the graph the transformation is done in the native language of the graph such as GSQL. This pattern allows many projects to share the underlying data loaders and allows each team to customize the post-loading transformation using the native query language of the graph. The other major data model is the RDF model which is discouraged at Optum due to the challenges with Reification. Reification causes RDF SPARQL queries to be rewritten. Load-as-is pattern See also: RDF","title":"Load-As-Is Pattern"},{"location":"glossary/#the-neighborhood-walk-story","text":"A story used to illustrate the difference between direct pointer hopping and using centralized indexes to traverse relationships. The story uses a 30-second walk between two houses vs. an 8.2-hour walk to a central location and back.","title":"The Neighborhood Walk Story"},{"location":"glossary/#no-complexity-penalty","text":"Unlike relational databases, graph databases quickly traverse many complex relationships. As a result, graph databases are better at modeling the real world - which is full of complexity. We use the phrase \"No Complexity Penalty\" every time we are training people who have come from the relational world that worry that too many relationships will slow down their queries due to slow JOINs. See also: One version of the truth","title":"No Complexity Penalty"},{"location":"glossary/#one-version-of-the-truth","text":"The real world has many complex relationships. There are many ways to build simple models that take shortcuts to optimize queries by limiting relationships. This is important in relational database modeling. But the closer we get to modeling the real world, the closer to a single version of the truth we get. Models that fairly represent the complexities of the real world can be reused among many business units and thus the costs of holding the information in memory can be shared. This is why graph databases cost less then relational databases.","title":"One Version of the Truth"},{"location":"glossary/#online-analytical-processing-system","text":"An approach to answer multi-dimensional analytical queries quickly by minimizing JOIN operations in relational databases. OLAP \"cubes\" often use a star schema with a central fact table and one JOIN operation per dimension of the cube. The denomalization process used to create star schemas limits enterprise-sharing of these structures. Wikipedia","title":"Online Analytical Processing System"},{"location":"glossary/#on-the-wire-vs-in-the-can","text":"A way of looking at knowledge representation requirements in two domains. On-The-Wire implies that serialization of a dataset must retain connection information within itself and to other external systems. In-The-Can knowledge representations are optimized for ease of query and sustainability . RDF is optimized for On-The-Wire exchange of knowledge. LPG is optimized for In-The-Can tasks such as ease of query and sustainability.","title":"On-the-Wire vs. In-the-Can"},{"location":"glossary/#ontology","text":"A graph of Concepts within a specific domain. Ontologies often begin as flat term lists, that become taxonomies that then have more complex relationships than simple broader and narrower concepts. Ontologies are often stored in formats such as SKOS and OWL.","title":"Ontology"},{"location":"glossary/#open-vs-closed-world","text":"https://en.wikipedia.org/wiki/Open-world_assumption https://en.wikipedia.org/wiki/Closed-world_assumption","title":"Open vs Closed World"},{"location":"glossary/#operational-source-system","text":"A transactional computer that is the source of a data stream. Enterprise Graph Databases often use Change Data Capture software on these systems to create an event stream of change records that so they can be stored in a central enterprise knowledge graph. Change records are new, updated, or deleted business entities.","title":"Operational Source System"},{"location":"glossary/#pagerank","text":"A graph algorithm that is used to rank the most influential vertices in a directed graph. For example web pages in a graph of linked web pages. PageRank was first used by Google Search to rank web pages in their search engine results. The patent for PageRank (now expired) was purchased by Google from Standford University for Google shares. Those shares sold for over $336 million USD when Google went public. Wikipedia","title":"PageRank"},{"location":"glossary/#preferred-label","text":"A preferred lexical label associated with a Concept . In the SKOS standard, there should be one and only one preferred label per language per concept.","title":"Preferred Label"},{"location":"glossary/#project-graph","text":"A graph that supports a specific project. Project graphs may contain knowledge that is not of interest to the rest of the enterprise.","title":"Project Graph"},{"location":"glossary/#reference-data","text":"Reference data is data used to classify or categorize other data. They typically are stored as a set of valid codes for a specific data element. For example the list of Country Codes is a type of reference data. Reference data is often stored as a short code and a definition of what that code represents.","title":"Reference Data"},{"location":"glossary/#reification","text":"Reification is the process by which an abstract idea about a computer program is turned into an explicit data model or other object created in a programming language. Specifically, in the RDF modeling process it is the process of adding an abstract vertex to a graph when properties are needed in a relationship. Reification causes queries that traverse that node to be rewritten. This means that SPARQL queries are inherently much more difficult to maintain than LPG graph queries.","title":"Reification"},{"location":"glossary/#resource-description-framework","text":"An early family of standards developed by the World Wide Web Consortium for exchanging graph data championed by the Semantic Web community starting in 1999. RDF gained some traction around 2010 but failed to gain widespread adoption due to the complexity of the standards and the problems of Reification . Wikipedia","title":"Resource Description Framework"},{"location":"glossary/#resource-quota","text":"The ability to limit the resources consumed by a query such as CPU time, or RAM for individuals or groups. Large enterprise-scale graph databases must carefully monitor and constrain queries that consume too many resources. Many older technologies such as Apache Drill are difficult to implement without the ability to monitor and restrict resources.","title":"Resource Quota"},{"location":"glossary/#role-based-access-control","text":"The ability to assign access to a resource to individuals that have a specific role. For Enterprise Knowledge Graphs, there are both high-level subgraph rules and fine-grain rules such as vertex-related role-based access control.","title":"Role-based Access Control"},{"location":"glossary/#rules-engine","text":"A software component that executes rules according to some algorithm. In the Enterprise Knowledge Graph space rules are frequently represented in Decision Tree structures within the graph. [Rules for Knowledge Graph Rules])https://dmccreary.medium.com/rules-for-knowledge-graphs-rules-f22587307a8f","title":"Rules Engine"},{"location":"glossary/#semantics","text":"The branch of computer science associated with meaning. It can be best understood by understanding the semantic triangle. The key point of the semantic triangle is that we cannot directly associate a label with a referent without traversing concepts.","title":"Semantics"},{"location":"glossary/#semantic-graph","text":"A graph where each vertex represents a Concept and the edges of the graph represent the relationships between the Concepts. The primary data model for storing semantic graphs is the SKOS data model where Concepts and Labels are distinct types.","title":"Semantic Graph"},{"location":"glossary/#shapes-constraint-language","text":"A W3C standard RDF vocabulary for validating RDF graphs against a set of conditions. Unlike document validation standards like XML Schema, SHACL assumes that data quality checks should be able to look for relationships in a graph as well as the local context of a document. These conditions are provided as shapes and other constructs expressed in the form of an RDF graph. LPG graphs do not yet have a version of SHACL. Also known as: SHACL SHACL W3C","title":"Shapes Constraint Language"},{"location":"glossary/#simple-knowledge-organizational-system","text":"A model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabularies. SKOS is also the name of the world-wide-web standard for encoding these systems. Serializations of SKOS are typically done in RDF format although other encodings such as XML and JSON are common. See Also: W3C SKOS Primer See Also: W2C SKOS Referecnce","title":"Simple Knowledge Organizational System"},{"location":"glossary/#strategy-graph","text":"A graph that is used to help determine what strategies might be optimal for an enterprise or a subgroup as well as how organizations are performing on a specific strategy. To be successful, enterprise and departmental strategies must be encoded in machine-readable forms such as StratML and loaded into an enterprise knowledge graph. Strategy graphs can also be used to determine the alignment of proposed projects for the future in an organization. StratML encoding is sometimes required of US federal organizations so that strategies can be analyzed by published public documents.","title":"Strategy Graph"},{"location":"glossary/#strategic-serendipity","text":"Building a enterprise strategy around the creation of an environment where it is easier to make unexpected connections between items. Strategic serendipity involves getting a large number of people ready to discover new things in an enterprise knowledge graph.","title":"Strategic Serendipity"},{"location":"glossary/#subgraph","text":"A subset of an enterprise knowledge graph that may store specific types of knowledge and may have specific access control rules based on the role of a user. For example, a business glossary, taxonomy, ontology or business rules system that contains no customer-specific information may be in one subgraph and have universal read-access for all users. Customer-specific data that is highly confidential may be stored in a different subgraph, with read access only granted on a need-to-know basis. See also: Role Based Access Control","title":"Subgraph"},{"location":"glossary/#sustainability","text":"The ability for a team of developers to maintain the code that supports an enterprise knowledge graph in the face of changes to the data model. The key measure is to avoid problems related to rewriting graph queries when small changes are made to the graph data model. See also: The Jenga Tower Story","title":"Sustainability"},{"location":"glossary/#systems-thinking","text":"A way of looking at problems in terms of components that interact with each other over time using direct connections, indirection connections and both positive and negative feedback cycles. Systems Thinking forces us to think broadly about how our enterprise knowledge graphs interact with external systems. Systems thinking also helps us see the unintended consequences of our actions.","title":"Systems Thinking"},{"location":"glossary/#tbox","text":"A \"terminological component\" or terminology Concept associated with a set of facts assertions (ABox statements) of a knowledge graph. TBox statements tend to more rules or metarules (rules about rules) that individual fact about customers or other business entities. See also: ABox Wikipedia Tbox page","title":"TBox"},{"location":"glossary/#technology-adoption-life-cycle","text":"A sociological model that describes the adoption or acceptance of a new product or innovation, according to the demographic and psychological characteristics of defined adopter groups. Wikipedia","title":"Technology Adoption Life Cycle"},{"location":"glossary/#temporal-modeling","text":"The process of modeling time in a data model. Modeling time can be complex when the requirements of a system require you to be able to recreate detailed reports as they were at a prior point in time. Temporal modeling includes the concept of versioning and bitemporal modeling","title":"Temporal Modeling"},{"location":"glossary/#triple-store","text":"A purpose-built database for the storage and retrieval of RDF triples through semantic queries. Triple stores are not used in most enterprise graphs due to their lack of sustanability due to problems with Reification . https://en.wikipedia.org/wiki/Triplestore","title":"Triple Store"},{"location":"glossary/#upper-ontology","text":"General high-level Concepts that are common across all domains in a knowledge graph. Concepts such as Organization, Customer, Family Unit, Product, Part, Invoice, Document are often consider part of an upper ontology. Enterprise graphs may contain multiple ontologies and the ease of linking ontologies in highly dependant on sharing upper ontologies. Wikipedia Page on Upper Ontology","title":"Upper Ontology"},{"location":"glossary/#web-ontology-language","text":"A Semantic Web language designed to represent rich and complex knowledge about things, groups of things, and relations between things. W3C OWL Web Site","title":"Web Ontology Language"},{"location":"glossary/#window-of-opportunity","text":"A narrow band of time that an organization might be ready to adopt a new technology such as an enterprise knowledge graph. To arrive at the Window of Opportunity to adapt an enterprise knowledge graph an organization must meet a set of preconditions such as an internal chamption, a shared understanding of what enterprise knowledge graphs are capable of, and the ability to ingest enough information to achieve specific business objectives. Enterprise knowledge graphs then have a limited time to create a pilot project before funding runs out, champions move on, or alternative technologies get mindshare. Very often a specific chrisis can trigger an unexpcted window of opportunity. Knowing how to recognize these windows and take advantage of them is a key skill we attempt to explore in this book. Wikipedia article on Window of Opportunity","title":"Window of Opportunity"},{"location":"references/","text":"References for Enterprise Knowledge Graphs Henry Knowledge Management Knowledge Management: A New Concern for Public Administration Henry, Nicholas L. (May\u2013June 1974). Public Administration Review. 34 (3): 189\u2013196. Knowledge is Power The phrase knowledge is power is frequently attributed to Sir Francis Bacon who clearly popularized it and helped trigger the . The first occurrence of it in recorded history might be attributed to Imam Ali (AD 599-661). We use this phrase as a theme throughout the book. Wikipedia page on Scientia potentia est Intel PIUMA on Arxiv PIUMA: Programmable Integrated Unified Memory Architecture","title":"References"},{"location":"references/#references-for-enterprise-knowledge-graphs","text":"","title":"References for Enterprise Knowledge Graphs"},{"location":"references/#henry-knowledge-management","text":"Knowledge Management: A New Concern for Public Administration Henry, Nicholas L. (May\u2013June 1974). Public Administration Review. 34 (3): 189\u2013196.","title":"Henry Knowledge Management"},{"location":"references/#knowledge-is-power","text":"The phrase knowledge is power is frequently attributed to Sir Francis Bacon who clearly popularized it and helped trigger the . The first occurrence of it in recorded history might be attributed to Imam Ali (AD 599-661). We use this phrase as a theme throughout the book. Wikipedia page on Scientia potentia est","title":"Knowledge is Power"},{"location":"references/#intel-piuma-on-arxiv","text":"PIUMA: Programmable Integrated Unified Memory Architecture","title":"Intel PIUMA on Arxiv"},{"location":"table-of-contents/","text":"Enterprise Knowledge Graphs Table of Contents Preface Acknowledgements About this book Part 1: Introduction Chapter 1: What is an Enterprise Knowledge Graph Chapter 2: Why Build an Enterprise Knowledge Graph Part 2: Enterprise Knowledge Graphs Concepts The Knowledge Triangle TBD Scale-Out Graph Databases - TBD Graph Machine Learning - TBD Calculating the Cost and Benefits of Enterprise Knowledge Graphs - TBD Part 3: Enterprise Knowledge Graphs Case Studies Customer 360 - a single view of all your customer touch points Analytics Dashboard - the right insights to the right users Semantic Search - EKG services for helping enterprise search Product Recommendation - product graphs Employee Knowledge Graphs - the rightHR insights Strategy Analytics - Are we aligned? Part 4: Promoting Enterprise Knowledge Graphs Storytelling - using narrative bias to your advantage Technology Adoption - telling the right story to the right audi Windows of Opportunity - the EKG pilot Strategic Serendipity - getting ready for discovery Conclusion Glossary","title":"Table of Contents"},{"location":"table-of-contents/#enterprise-knowledge-graphs-table-of-contents","text":"Preface Acknowledgements About this book","title":"Enterprise Knowledge Graphs Table of Contents"},{"location":"table-of-contents/#part-1-introduction","text":"Chapter 1: What is an Enterprise Knowledge Graph Chapter 2: Why Build an Enterprise Knowledge Graph","title":"Part 1: Introduction"},{"location":"table-of-contents/#part-2-enterprise-knowledge-graphs-concepts","text":"The Knowledge Triangle TBD Scale-Out Graph Databases - TBD Graph Machine Learning - TBD Calculating the Cost and Benefits of Enterprise Knowledge Graphs - TBD","title":"Part 2: Enterprise Knowledge Graphs Concepts"},{"location":"table-of-contents/#part-3-enterprise-knowledge-graphs-case-studies","text":"Customer 360 - a single view of all your customer touch points Analytics Dashboard - the right insights to the right users Semantic Search - EKG services for helping enterprise search Product Recommendation - product graphs Employee Knowledge Graphs - the rightHR insights Strategy Analytics - Are we aligned?","title":"Part 3: Enterprise Knowledge Graphs Case Studies"},{"location":"table-of-contents/#part-4-promoting-enterprise-knowledge-graphs","text":"Storytelling - using narrative bias to your advantage Technology Adoption - telling the right story to the right audi Windows of Opportunity - the EKG pilot Strategic Serendipity - getting ready for discovery Conclusion Glossary","title":"Part 4: Promoting Enterprise Knowledge Graphs"},{"location":"case-studies/analytics-dashboard/","text":"","title":"Analytics Dashboard"},{"location":"case-studies/customer-360/","text":"","title":"Customer 360"},{"location":"case-studies/rules-engine/","text":"","title":"Rules Engine"},{"location":"case-studies/semantic-search/","text":"","title":"Semantic Search"},{"location":"case-studies/strategy-graph/","text":"Strategy Graph Strategic planning is complex Funding the right projects Project dependency graphs Project dashboards Strategic planning can be a complex process. A CEO may get hundreds of annual project proposals all claiming to lower expenses, increase revenue or increase agility. Many of these projects will claim to have high return on investments. Perhaps they claim their new system will allow three old systems to be decommissioned or provide valuable new insight to trends existing systems. In this chapter we will look at how an enterprise knowledge graph can be used to predict the actual savings of new projects, systems or initiatives based on three observations: Strategic projects are not isolated silos. Many depend on other projects being successful. If projects can provide infrastructure to other projects they should be prioritized. Thus a project dependency graph is a key aspect to strategic planning. Projects are often similar to other projects that we may already have experience with. By being able to find similar projects we can build better cost models by using data from prior projects. The individuals that are assigned to projects also have a history on prior projects. Building an accurate individual project histories can also aid in predicting a projects success and its return on investment. Using EKG to Build Project Dashboards After an annual strategy has been announced there are many questions that a CEO may want to know about the status of a project and the individual actions that people are taking. We can use EKGs to build dashboards that gather complex metrics from projects, team, tasks and communications to see how strategies are progressing.","title":"Strategy Graph"},{"location":"case-studies/strategy-graph/#strategy-graph","text":"Strategic planning is complex Funding the right projects Project dependency graphs Project dashboards Strategic planning can be a complex process. A CEO may get hundreds of annual project proposals all claiming to lower expenses, increase revenue or increase agility. Many of these projects will claim to have high return on investments. Perhaps they claim their new system will allow three old systems to be decommissioned or provide valuable new insight to trends existing systems. In this chapter we will look at how an enterprise knowledge graph can be used to predict the actual savings of new projects, systems or initiatives based on three observations: Strategic projects are not isolated silos. Many depend on other projects being successful. If projects can provide infrastructure to other projects they should be prioritized. Thus a project dependency graph is a key aspect to strategic planning. Projects are often similar to other projects that we may already have experience with. By being able to find similar projects we can build better cost models by using data from prior projects. The individuals that are assigned to projects also have a history on prior projects. Building an accurate individual project histories can also aid in predicting a projects success and its return on investment.","title":"Strategy Graph"},{"location":"case-studies/strategy-graph/#using-ekg-to-build-project-dashboards","text":"After an annual strategy has been announced there are many questions that a CEO may want to know about the status of a project and the individual actions that people are taking. We can use EKGs to build dashboards that gather complex metrics from projects, team, tasks and communications to see how strategies are progressing.","title":"Using EKG to Build Project Dashboards"},{"location":"concepts/entity-resolution/","text":"","title":"Entity Resolution"},{"location":"concepts/graph-algorithms/","text":"","title":"Graph Algorithms"},{"location":"concepts/graph-hardware/","text":"Graph Hardware Hardware Architecture Intel\u2019s Incredible PIUMA Graph Analytics Hardware","title":"Graph Hardware"},{"location":"concepts/graph-hardware/#graph-hardware","text":"Hardware Architecture Intel\u2019s Incredible PIUMA Graph Analytics Hardware","title":"Graph Hardware"},{"location":"concepts/graph-machine-learning/","text":"","title":"Graph Machine Learning"},{"location":"concepts/ingestion/","text":"Ingesting Data into an Enterprise Knowledge Graph Data Flow concepts Operational Source System Change Data Capture Business Event Publishing Schema Matching Schema Mapping The Publish-Subscribe Pattern Batch Updates Streaming Updates Data Quality Checks Document Validation In Graph Data Checks Case Study: SHACL Shape Constraint Language Entity Resolution Master Data Management","title":"Ch 6 Ingesting Data"},{"location":"concepts/ingestion/#ingesting-data-into-an-enterprise-knowledge-graph","text":"","title":"Ingesting Data into an Enterprise Knowledge Graph"},{"location":"concepts/ingestion/#data-flow-concepts","text":"","title":"Data Flow concepts"},{"location":"concepts/ingestion/#operational-source-system","text":"","title":"Operational Source System"},{"location":"concepts/ingestion/#change-data-capture","text":"","title":"Change Data Capture"},{"location":"concepts/ingestion/#business-event-publishing","text":"","title":"Business Event Publishing"},{"location":"concepts/ingestion/#schema-matching","text":"","title":"Schema Matching"},{"location":"concepts/ingestion/#schema-mapping","text":"","title":"Schema Mapping"},{"location":"concepts/ingestion/#the-publish-subscribe-pattern","text":"","title":"The Publish-Subscribe Pattern"},{"location":"concepts/ingestion/#batch-updates","text":"","title":"Batch Updates"},{"location":"concepts/ingestion/#streaming-updates","text":"","title":"Streaming Updates"},{"location":"concepts/ingestion/#data-quality-checks","text":"","title":"Data Quality Checks"},{"location":"concepts/ingestion/#document-validation","text":"","title":"Document Validation"},{"location":"concepts/ingestion/#in-graph-data-checks","text":"","title":"In Graph Data Checks"},{"location":"concepts/ingestion/#case-study-shacl","text":"Shape Constraint Language","title":"Case Study: SHACL"},{"location":"concepts/ingestion/#entity-resolution","text":"Master Data Management","title":"Entity Resolution"},{"location":"concepts/ingestion/#_1","text":"","title":""},{"location":"concepts/knowledge-triangle/","text":"","title":"Ch 5 Knowledge Triangle"},{"location":"concepts/load-and-stress-testing/","text":"","title":"Load and Stress Testing"},{"location":"concepts/natural-language-processing/","text":"Natural Language Processing What is Natural Language Processing NLP Use Cases Storing NLP Extracts in and EKG When to Use Search Engines What is Natural Language Processing Two Worlds - Text and Codes NLP Use Cases Extracting Facts from Documents Document Classification Classifying documents based on concepts, not keywords. Semantic Search Using taxonomies and ontologies for keyword expansions Chatbots Generating Graph Queries Adding Documents to an EKG Metadata","title":"Natural Language Processing"},{"location":"concepts/natural-language-processing/#natural-language-processing","text":"What is Natural Language Processing NLP Use Cases Storing NLP Extracts in and EKG When to Use Search Engines","title":"Natural Language Processing"},{"location":"concepts/natural-language-processing/#what-is-natural-language-processing","text":"","title":"What is Natural Language Processing"},{"location":"concepts/natural-language-processing/#two-worlds-text-and-codes","text":"","title":"Two Worlds - Text and Codes"},{"location":"concepts/natural-language-processing/#nlp-use-cases","text":"","title":"NLP Use Cases"},{"location":"concepts/natural-language-processing/#extracting-facts-from-documents","text":"","title":"Extracting Facts from Documents"},{"location":"concepts/natural-language-processing/#document-classification","text":"Classifying documents based on concepts, not keywords.","title":"Document Classification"},{"location":"concepts/natural-language-processing/#semantic-search","text":"Using taxonomies and ontologies for keyword expansions","title":"Semantic Search"},{"location":"concepts/natural-language-processing/#chatbots","text":"","title":"Chatbots"},{"location":"concepts/natural-language-processing/#generating-graph-queries","text":"","title":"Generating Graph Queries"},{"location":"concepts/natural-language-processing/#adding-documents-to-an-ekg","text":"Metadata","title":"Adding Documents to an EKG"},{"location":"concepts/scale-out/","text":"Scale-Out Graph Databases Distributing Query Loads Autosharding Partitioning High Availability Fault Tolerance Rolling Upgrades","title":"Scaling Out"},{"location":"concepts/scale-out/#scale-out-graph-databases","text":"","title":"Scale-Out Graph Databases"},{"location":"concepts/scale-out/#distributing-query-loads","text":"","title":"Distributing Query Loads"},{"location":"concepts/scale-out/#autosharding","text":"","title":"Autosharding"},{"location":"concepts/scale-out/#partitioning","text":"","title":"Partitioning"},{"location":"concepts/scale-out/#high-availability","text":"","title":"High Availability"},{"location":"concepts/scale-out/#fault-tolerance","text":"","title":"Fault Tolerance"},{"location":"concepts/scale-out/#rolling-upgrades","text":"","title":"Rolling Upgrades"},{"location":"intro/about-this-book/","text":"About the Enterprise Knowledge Graphs Book Intended Audience The book is intended for individuals and organizations that are considering or currently implementing Enterprise Knowledge Graphs (EKGs). In this book, we define EKGs as scalable graph databases that span two or more business units. The definitions of these terms will be defined in the first chapter of the book. Part 1: Introduction to EKGs Part 1 of this book introduces you to the high-level concepts in EKGs. We describe the what EKGs are, why companies build them and and how EKGs start and grow. Chapter P1.1: Introduction defines what an EKG is and how it is being used in large organizations. We address the question of scalability and how many graph database systems claim to be scalable but fall down when they reach specific architectural limitations. We also bring up the concept of role-based access control since we have found this feature key to widespread adoption of enterprise knowledge graphs. We also introduce other key terms and give their definitions. Chapter P1.2: Cost and Benefits describes why organizations are building EKGs. We briefly cover the cost-benefits of building EKGs and describe the key use-cases of enterprise-knowledge graphs. and economic models that will help you determine if they might be a good fit for your organization. We provide examples of how integration of data aids both customer experience an empowers data scientists to be more productive. Chapter 3: Lifecycles focuses on how EKGs are built in practice in the real world. We describe how pilot projects are created but also designed to scale to meet enterprise needs. We describe how EKG data models evolve an grow to absorbs new knowledge that provides deeper insights. We also discuss the importance of subgraphs and both corse an fine-grain role-based access control. Finally, we introduce the concept of \"Edge of Chaos\" and how it guides how EKGs grow. Part 2: EKG Concepts Part 2 is an in depth analysis of the core concepts involved in building and running EKGs. [Chapter P2.1: Introduction] introduces the concept of the knowledge triangle and the processes we used to turn raw data into entity-based information and then to connect this information into useful and queryable knowledge. [Chapter 2.2: Modeling] covers enterprise-scale graph data modeling issues and the key issue of sharable data models between multiple business groups. If data models can be shared they avoid duplication and lower analytical costs for the enterprise. [Chapter P2.3: Ingestion] covers the concept of scalable transactions and data ingestion into an EKG. We discuss the issues related to change-data-capture in source systems, schema matching and schema mapping, canonical data models, business events, streaming, publish-subscribe patterns and load and stress testing. [Chapter P2.4: Enrichment] introduces the concept if enrichment of data and connecting information. This includes connecting incomming data to taxonomies and ontologies as well as reference data. Part 3: Case Studies Part 3 is a deep dive into the most important case studies that help EKGs get launched. Getting your first project off the ground successfully is critical for EKG success.","title":"About this book"},{"location":"intro/about-this-book/#about-the-enterprise-knowledge-graphs-book","text":"","title":"About the Enterprise Knowledge Graphs Book"},{"location":"intro/about-this-book/#intended-audience","text":"The book is intended for individuals and organizations that are considering or currently implementing Enterprise Knowledge Graphs (EKGs). In this book, we define EKGs as scalable graph databases that span two or more business units. The definitions of these terms will be defined in the first chapter of the book.","title":"Intended Audience"},{"location":"intro/about-this-book/#part-1-introduction-to-ekgs","text":"Part 1 of this book introduces you to the high-level concepts in EKGs. We describe the what EKGs are, why companies build them and and how EKGs start and grow. Chapter P1.1: Introduction defines what an EKG is and how it is being used in large organizations. We address the question of scalability and how many graph database systems claim to be scalable but fall down when they reach specific architectural limitations. We also bring up the concept of role-based access control since we have found this feature key to widespread adoption of enterprise knowledge graphs. We also introduce other key terms and give their definitions. Chapter P1.2: Cost and Benefits describes why organizations are building EKGs. We briefly cover the cost-benefits of building EKGs and describe the key use-cases of enterprise-knowledge graphs. and economic models that will help you determine if they might be a good fit for your organization. We provide examples of how integration of data aids both customer experience an empowers data scientists to be more productive. Chapter 3: Lifecycles focuses on how EKGs are built in practice in the real world. We describe how pilot projects are created but also designed to scale to meet enterprise needs. We describe how EKG data models evolve an grow to absorbs new knowledge that provides deeper insights. We also discuss the importance of subgraphs and both corse an fine-grain role-based access control. Finally, we introduce the concept of \"Edge of Chaos\" and how it guides how EKGs grow.","title":"Part 1: Introduction to EKGs"},{"location":"intro/about-this-book/#part-2-ekg-concepts","text":"Part 2 is an in depth analysis of the core concepts involved in building and running EKGs. [Chapter P2.1: Introduction] introduces the concept of the knowledge triangle and the processes we used to turn raw data into entity-based information and then to connect this information into useful and queryable knowledge. [Chapter 2.2: Modeling] covers enterprise-scale graph data modeling issues and the key issue of sharable data models between multiple business groups. If data models can be shared they avoid duplication and lower analytical costs for the enterprise. [Chapter P2.3: Ingestion] covers the concept of scalable transactions and data ingestion into an EKG. We discuss the issues related to change-data-capture in source systems, schema matching and schema mapping, canonical data models, business events, streaming, publish-subscribe patterns and load and stress testing. [Chapter P2.4: Enrichment] introduces the concept if enrichment of data and connecting information. This includes connecting incomming data to taxonomies and ontologies as well as reference data.","title":"Part 2: EKG Concepts"},{"location":"intro/about-this-book/#part-3-case-studies","text":"Part 3 is a deep dive into the most important case studies that help EKGs get launched. Getting your first project off the ground successfully is critical for EKG success.","title":"Part 3: Case Studies"},{"location":"intro/acknowledgements/","text":"Enterprise Knowledge Graph Acknowledgements I would like to thank the following people: Arun Batchu Nikhil Deshpande Parker Erickson Hank Head Jonathan Herke Mark Megerian John Santelli Sujith Sasidharan Ed Sverdlin Sudeep Vishnumurthy I want to express my gratitude to everyone within the Optum Advanced Technology Collaborative for helping me refine our storytelling strategies to our business areas. Many other people have guided me in my journey in becoming a better storytelling driven solution architect. They have taught me that is critical for us to understand both how technologies work, and to be able to relate the benefits with stories that our stakeholders will remember.","title":"Acknowledgements"},{"location":"intro/acknowledgements/#enterprise-knowledge-graph-acknowledgements","text":"I would like to thank the following people: Arun Batchu Nikhil Deshpande Parker Erickson Hank Head Jonathan Herke Mark Megerian John Santelli Sujith Sasidharan Ed Sverdlin Sudeep Vishnumurthy I want to express my gratitude to everyone within the Optum Advanced Technology Collaborative for helping me refine our storytelling strategies to our business areas. Many other people have guided me in my journey in becoming a better storytelling driven solution architect. They have taught me that is critical for us to understand both how technologies work, and to be able to relate the benefits with stories that our stakeholders will remember.","title":"Enterprise Knowledge Graph Acknowledgements"},{"location":"intro/ekg-cost-benefit/","text":"Chapter 2: Why Build An Enterprise Knowledge Graph Cost-Benefit Analysis Business Value Measuring EKG costs Measuring EKG tangible benefits Measuring EKG intangible benefits When we build any large resource for an organization we are going to need to ask for money to build it. If you work for a company like Apple and you are respected expert at building enterprise knowledge graphs, you many not need to go to a finance committee and justify your spending requests. Apple hires world experts in specific fields and then trusts their judgement. As long as you have a good track record they will keep giving you funding without asking a lot of questions. But most of us don't work at Apple and we are not world leaders with a long successful track record of building EKGs. We will have to fight for every penny we spend on our EKG pilot projects until the ROI is so clear to everyone that the finance people are begging you to expand the scope of the EKG. Until that happens, we are going to need to learn to speak the language of finance to get our EKGs off the ground. Cost-Benefit Analysis [The Business Value of Computers] Wikipedia on Business Value Measuring Costs Cost of extracting knowledge from operational source systems Change data-capture (CDC) Publishing business events The Easy to Measure Benefits Integrated views of anything Integrated views of your customers Centralized business Rules Recommendation systems Difficult to Measure Benefits The value of insights Tracking early insights Predicting future insights Focus on datasets that generate shared value","title":"Ch 2 Costs & Benefits"},{"location":"intro/ekg-cost-benefit/#chapter-2-why-build-an-enterprise-knowledge-graph","text":"Cost-Benefit Analysis Business Value Measuring EKG costs Measuring EKG tangible benefits Measuring EKG intangible benefits When we build any large resource for an organization we are going to need to ask for money to build it. If you work for a company like Apple and you are respected expert at building enterprise knowledge graphs, you many not need to go to a finance committee and justify your spending requests. Apple hires world experts in specific fields and then trusts their judgement. As long as you have a good track record they will keep giving you funding without asking a lot of questions. But most of us don't work at Apple and we are not world leaders with a long successful track record of building EKGs. We will have to fight for every penny we spend on our EKG pilot projects until the ROI is so clear to everyone that the finance people are begging you to expand the scope of the EKG. Until that happens, we are going to need to learn to speak the language of finance to get our EKGs off the ground.","title":"Chapter 2: Why Build An Enterprise Knowledge Graph"},{"location":"intro/ekg-cost-benefit/#cost-benefit-analysis","text":"[The Business Value of Computers] Wikipedia on Business Value","title":"Cost-Benefit Analysis"},{"location":"intro/ekg-cost-benefit/#measuring-costs","text":"Cost of extracting knowledge from operational source systems Change data-capture (CDC) Publishing business events","title":"Measuring Costs"},{"location":"intro/ekg-cost-benefit/#the-easy-to-measure-benefits","text":"Integrated views of anything Integrated views of your customers Centralized business Rules Recommendation systems","title":"The Easy to Measure Benefits"},{"location":"intro/ekg-cost-benefit/#difficult-to-measure-benefits","text":"The value of insights Tracking early insights Predicting future insights Focus on datasets that generate shared value","title":"Difficult to Measure Benefits"},{"location":"intro/introduction/","text":"Chapter 1: What is an Enterprise Knowledge Graph Knowledge is Power - Imam Ali (AD 599-661) EKG are one type of NoSQL architectural patterns Defining enterprise knowledge graphs The Role of architectural scalability Growth Rates of graph databases technology Scale out graph database hardware Scale out graph query languages EKGs in Context: Six NoSQL Architectural Patterns Enterprise Knowledge Graphs (EKGs) are a type of graph database that are designed to scale to meet the demanding requirements of large organizations. Graph databases are just one type of NoSQL database architectural patten that we use to solve business problems. EKGs are also used in combination with other NoSQL databases since each architecture has its strengths and weaknesses. If you are a [solution architect] your job is to work with business units to find how to correctly match business problems to the right NoSQL database architecture pattern. This book will help you determine if EKGs might be a good fit for one or more of your business areas. Although they have become popular, it is critical that we don't begin to think EKGs are the only solution to business problems. Solution architects must have a strong understanding of all NoSQL architecture patterns to keeping recommendations objective. They must know the pros and cons of each of these architectures. Although it is out of the scope of this book to do a deep dive into each of the six architectural pattern, we present each of them here briefly so you can explore more on your own. The six types of NoSQL database architectural patterns are described in Figure 1.1. The six key architectures are: Relational - where data is stored as fixed format rows in tables and new data is added one row at a time. Relationships between tables are calculated at query time using JOIN statements that require central indexes to be used to traverse the relationships. Ironically, relational databases put relationship traversal as a secondary concern and don't optimize the design for fast traversal of billions of relationships. Because relationships evolved as an \"add on feature\" to COBOL flat files running on mainframes, the focus of a relational database is fast atomic transactions on row-oriented data. Also known as row-stores. [Analytical](../glossary.md#analytical-database - where data is stored in centralized fact tables with simple relationships to dimensional tables. Dimensions each represent a way you classify the facts in the fact table. Analytical databases severely restrict the number of JOIN operations to optimise performance but force everyone to agree on the dimensions used to classify data. Analytical databases tend to be the most difficult to use between departments since the denormalization process can be very specific to a single departments view of the enterprise. ** Key-value Store - a very simple type of data storage system that is deliberately designed to be simple so it can easily scale. Key-value stores have a simple API (store, get and delete) and the salient fact is that you cannot efficiently query the content of a value. It is considered a back box or \"binary blob\" of information. Because if the simple design of key-value stores they are easy to distribute over a large cluster of computers and are cost effective when measured annual cost per terabyte per year. They are an ideal complement to EKGs since EKGs focus on minimizing RAM usage. EKGs frequently store just the key portion of items such as images or document references. ** Column-family Store - these databases are similar to key-value stores but they partition the key into multiple components such as a row and column portion of the key. A spreadsheet is a good example where a key is a cell at to get to the cell you must have the row and column identifiers of the cell. ** Graph Database - a database composed of vertices and edges, both with attributes, where relationship traversal is a primary concern, not an afterthought. The graph database architecture pattern is the foundation for all enterprise knowledge graphs. ** Document Store - a database that is composed of recursive tree-like structures where the atomic unit of storage is branches and leaves of this tree. Document stores proved path-like query languages to reach any points in a tree using simple path expressions that may contain wildcards. Document stores are ideal when you have hierarchial tree-like data such as documents or serialized objects that contain other objects. Both XML and JSON are considered data models to serialize document structures. Document stores are ideal for document search and retrieval since they can use document structure to aid in relevancy ranking. These six database architecture patterns are frequently used with the Architecture Tradeoff Analysis Method (ATAM) to help organizations have a transparent discussion on the pros and cons of each alternative. A high-level overview of the ATAM process is shown in Figure 1.2: Defining Enterprise Knowledge Graphs In this book, we define Enterprise Knowledge Graphs as the following: An Enterprise Knowledge Graph is a scalable graph database that stores information from two or more departments of an organization. You will note that this definition is both somewhat general and very specific. It is general because it includes many graph projects using a wide variety of technologies. It is very specific in that it requires whatever graph database being used to have scalable technology under the hood. We have very specific rules about what we mean by enterprise scalable. Unfortunately, this eliminates most departmental graph projects in use today that don't use true scale-out technologies. Why do we define EKGs with scalability as a prerequisite? Because to truly meet the future needs of a large enterprise we must build our graph pilot projects on an infrastructure that will not fall over and die as it grows beyond the initial pilot phase. I have seen many well intentioned software architects claim to be building enterprise-scalable graph databases only to have the projects fail due to performance problems as they grow beyond their early stages. We also exclude knowledge graphs that are only attempting to solve problems for a specific department or specific business unit. These can still be valuable projects for an enterprise since they can help individuals and departments learn the capabilities of graph databases. But we would still classify them as departmental graph project, not enterprise-scale graph projects. Now let's take a closer look at what we mean by enterprise scalable. Enterprise Scalable Knowledge Graphs Scalability is an inherent characteristic of any database architecture. It implies that as the size and complexity of the databases grows, the architecture must accommodate this growth without rewriting the core applications. Although unexpected performance problems with specific queries might occur as the size of your database grows, they should be fixable by doing simple query optimization. To be a true EKG pilot you should never have to redo your architecture from scratch. You should be able to know that the exact same queries should run on ten thousand vertex graph and a trillion vertex graph. The key aspect of scalable graph databases is the ability to distribute a graph over a large number of independent but closely connected servers in a data center. As demands grows, the number of servers in the cluster must easily scale without users or operations being impacted. As you add new servers data must automatically be redistributed over the new servers in the cluster. Scalability Means the Four Vs Volume, velocity, variability and veracity are considered the four Vs that define scalable systems. Volume refers to the total amount of data in our knowledge graph. For example enterprise-scale graphs may easily contain over 10 billion vertices and 50 billion edges. Velocity means that new inserts, updates and deletes might be coming in fast via streaming events and these events must be ACID compliant and still never slow down read access times. Service levels agreements (SLAs) must focus not on total average times, but the averages of the slowest 5% of the transactions. Variability means that data is not uniform and can be easily stuffed into a single fact table of an OLAP cube. Anyone should be able to add new assertions to the graph without ever needing to rewrite queries. This property is called agility and sometime sustainability Veracity means we need to be able to validate the quality of incoming data in real-time and quickly raise warning flags if corrupt data is being transmitted into our EKG. Data quality may not be important for small departmental graph projects but becomes critical as you merge data from hundreds of different data sources. Note: We avoid using the term \"[Big Data](../glossary.md#big-data\" in this book. It is an ambiguous statement of a problem and adds no insight into a solution. Scalability Means Automatic Sharding and Rebalancing The first criteria is that when the size of the enterprise graph grows to take on a new department or a new project, new hardware can be installed and the database is smart enough to use the new hardware without extensive pain and suffering by the database staff. This is illustrated in Figure 1.2. So to be brutally honest, if your graph database does not automatically rebalance data as new nodes are added or removed from a distributed graph cluster I don't consider it a true enterprise graph solution. It may still solve important business problems, but it does not fit our definition for this book. Scalability Means Scalable Access Control Many graph products claim to have performance scale out capabilities, but their software falls down when we give them a a detailed list of what roles can access what data. This means they might be able to scale their data to 100 nodes, but only a single role of \"admin\" can be defined for the graph. They provide an all-or nothing approach to security. This model works in small applications where access to the application is controlled by security access rules. However it breaks down when I want 100 teams to only be able to run queries on their own team's data. We will provide more examples of vertex-level role-based access control in Chapter 3 Scalability Means High Availability Many graph products are perfect for a small team in a single city that work 9am to 5pm. Upgrades can be done at night and on weekends. But Enterprise class systems don't have users in a single city. Employees and customers are all over the world. That means when we upgrade our database with a new version of software we can't shut it down. We need distributed servers that can take a single node down, upgrade the software and get it back into the cluster without ever dropping a single transaction. Scalability Means Ease of Creating Distributed Queries If you are building enterprise-class knowledge graphs you may need to support 100+ concurrent developer all writing queries on a graph distributed over 100+ nodes in a cluster. These queries need to efficiently distribute their work over each node and bring back just the data that is relevant to the results. For example if you run a count of customers that have purchased a specific product, that query needs to distribute the query to each node and have each node return a simple count to the server where the query originated. Those counts are summed together and returned to the user. This type of query is sometimes called a map-reduce query since only the counts (reduced data) are returned to the origin server. The actual customer data never never needs to move around. Scalability Means Resource Quotas In a small project graph, you often have control and review processes of the code that each developer is working on. In an enterprise setting you can't control everyone's code. You need to assume that user will write run-away queries or just run queries that are not optimized for performance during the development and testing phases of query development. You enterprise graph needs to be able to monitor rouge queries and shut them down when vertex counts, edges counts, CPU counts or memory resources reach reasonable limits. Without these resource quotas a single query can take down an entire enterprise graph database. So we need fine-grain controls in query resources, even when the queries may be running on 100 different nodes in a cluster. Scalability Means Scalable Metadata Management For small project graphs, you many only have a few hundred vertices, edges and attributes. For enterprise-scale graphs you may have thousands of these items and staff that are new to the enterprise knowledge graph need to be able to quickly search for the structures they are interested in. Once they find the items of interest they need to understand their meaning and what the various code within value sets mean (reference data). New users also many need to ask the data steward for each attribute questions such as where did the data come from, what assumptions were made about the data if if the dataset contains any sensitive information that might restrict who can view the data. Scalability Needs to Be Cost Effective and Sustainable I have worked on several enterprise data warehouse projects that use relational database to perform complex queries that required many JOIN operations. Both Inman and Kimball data warehouse methodologies use relational databases and JOIN operations as their main vehicle for deployment. Most of the initial projects work fine, but as databases grow in complexity, the number of JOIN operations increase and the performance of the complex queries slowed down. The number of people who can confidently write 25-level JOIN queries is also much smaller then those that can write 10-level JOINs. Even the relational database that are built around priopirateary vendor technology that do many tricks with custom FPGA hardware to minimize the impact of JOIN operations become slower over time. But in the end, the cost most of these enterprise data warehouse systems exceeded the value of the insight gained from the systems. Dispute spending hundreds of millions of dollars on attempting to build enterprise scale analytics systems, they get decommissioned. The costs exceeded the benefits of the system. Our goal when building EKGs is to keep a sharp focus on both the costs of these systems and continuing to provide undisputable evidence of value to our business stakeholders. We will explore this topic in the chapter on EKG Cost Benefit Analysis .","title":"Ch 1 Introduction"},{"location":"intro/introduction/#chapter-1-what-is-an-enterprise-knowledge-graph","text":"Knowledge is Power - Imam Ali (AD 599-661) EKG are one type of NoSQL architectural patterns Defining enterprise knowledge graphs The Role of architectural scalability Growth Rates of graph databases technology Scale out graph database hardware Scale out graph query languages","title":"Chapter 1: What is an Enterprise Knowledge Graph"},{"location":"intro/introduction/#ekgs-in-context-six-nosql-architectural-patterns","text":"Enterprise Knowledge Graphs (EKGs) are a type of graph database that are designed to scale to meet the demanding requirements of large organizations. Graph databases are just one type of NoSQL database architectural patten that we use to solve business problems. EKGs are also used in combination with other NoSQL databases since each architecture has its strengths and weaknesses. If you are a [solution architect] your job is to work with business units to find how to correctly match business problems to the right NoSQL database architecture pattern. This book will help you determine if EKGs might be a good fit for one or more of your business areas. Although they have become popular, it is critical that we don't begin to think EKGs are the only solution to business problems. Solution architects must have a strong understanding of all NoSQL architecture patterns to keeping recommendations objective. They must know the pros and cons of each of these architectures. Although it is out of the scope of this book to do a deep dive into each of the six architectural pattern, we present each of them here briefly so you can explore more on your own. The six types of NoSQL database architectural patterns are described in Figure 1.1. The six key architectures are: Relational - where data is stored as fixed format rows in tables and new data is added one row at a time. Relationships between tables are calculated at query time using JOIN statements that require central indexes to be used to traverse the relationships. Ironically, relational databases put relationship traversal as a secondary concern and don't optimize the design for fast traversal of billions of relationships. Because relationships evolved as an \"add on feature\" to COBOL flat files running on mainframes, the focus of a relational database is fast atomic transactions on row-oriented data. Also known as row-stores. [Analytical](../glossary.md#analytical-database - where data is stored in centralized fact tables with simple relationships to dimensional tables. Dimensions each represent a way you classify the facts in the fact table. Analytical databases severely restrict the number of JOIN operations to optimise performance but force everyone to agree on the dimensions used to classify data. Analytical databases tend to be the most difficult to use between departments since the denormalization process can be very specific to a single departments view of the enterprise. ** Key-value Store - a very simple type of data storage system that is deliberately designed to be simple so it can easily scale. Key-value stores have a simple API (store, get and delete) and the salient fact is that you cannot efficiently query the content of a value. It is considered a back box or \"binary blob\" of information. Because if the simple design of key-value stores they are easy to distribute over a large cluster of computers and are cost effective when measured annual cost per terabyte per year. They are an ideal complement to EKGs since EKGs focus on minimizing RAM usage. EKGs frequently store just the key portion of items such as images or document references. ** Column-family Store - these databases are similar to key-value stores but they partition the key into multiple components such as a row and column portion of the key. A spreadsheet is a good example where a key is a cell at to get to the cell you must have the row and column identifiers of the cell. ** Graph Database - a database composed of vertices and edges, both with attributes, where relationship traversal is a primary concern, not an afterthought. The graph database architecture pattern is the foundation for all enterprise knowledge graphs. ** Document Store - a database that is composed of recursive tree-like structures where the atomic unit of storage is branches and leaves of this tree. Document stores proved path-like query languages to reach any points in a tree using simple path expressions that may contain wildcards. Document stores are ideal when you have hierarchial tree-like data such as documents or serialized objects that contain other objects. Both XML and JSON are considered data models to serialize document structures. Document stores are ideal for document search and retrieval since they can use document structure to aid in relevancy ranking. These six database architecture patterns are frequently used with the Architecture Tradeoff Analysis Method (ATAM) to help organizations have a transparent discussion on the pros and cons of each alternative. A high-level overview of the ATAM process is shown in Figure 1.2:","title":"EKGs in Context: Six NoSQL Architectural Patterns"},{"location":"intro/introduction/#defining-enterprise-knowledge-graphs","text":"In this book, we define Enterprise Knowledge Graphs as the following: An Enterprise Knowledge Graph is a scalable graph database that stores information from two or more departments of an organization. You will note that this definition is both somewhat general and very specific. It is general because it includes many graph projects using a wide variety of technologies. It is very specific in that it requires whatever graph database being used to have scalable technology under the hood. We have very specific rules about what we mean by enterprise scalable. Unfortunately, this eliminates most departmental graph projects in use today that don't use true scale-out technologies. Why do we define EKGs with scalability as a prerequisite? Because to truly meet the future needs of a large enterprise we must build our graph pilot projects on an infrastructure that will not fall over and die as it grows beyond the initial pilot phase. I have seen many well intentioned software architects claim to be building enterprise-scalable graph databases only to have the projects fail due to performance problems as they grow beyond their early stages. We also exclude knowledge graphs that are only attempting to solve problems for a specific department or specific business unit. These can still be valuable projects for an enterprise since they can help individuals and departments learn the capabilities of graph databases. But we would still classify them as departmental graph project, not enterprise-scale graph projects. Now let's take a closer look at what we mean by enterprise scalable.","title":"Defining Enterprise Knowledge Graphs"},{"location":"intro/introduction/#enterprise-scalable-knowledge-graphs","text":"Scalability is an inherent characteristic of any database architecture. It implies that as the size and complexity of the databases grows, the architecture must accommodate this growth without rewriting the core applications. Although unexpected performance problems with specific queries might occur as the size of your database grows, they should be fixable by doing simple query optimization. To be a true EKG pilot you should never have to redo your architecture from scratch. You should be able to know that the exact same queries should run on ten thousand vertex graph and a trillion vertex graph. The key aspect of scalable graph databases is the ability to distribute a graph over a large number of independent but closely connected servers in a data center. As demands grows, the number of servers in the cluster must easily scale without users or operations being impacted. As you add new servers data must automatically be redistributed over the new servers in the cluster.","title":"Enterprise Scalable Knowledge Graphs"},{"location":"intro/introduction/#scalability-means-the-four-vs","text":"Volume, velocity, variability and veracity are considered the four Vs that define scalable systems. Volume refers to the total amount of data in our knowledge graph. For example enterprise-scale graphs may easily contain over 10 billion vertices and 50 billion edges. Velocity means that new inserts, updates and deletes might be coming in fast via streaming events and these events must be ACID compliant and still never slow down read access times. Service levels agreements (SLAs) must focus not on total average times, but the averages of the slowest 5% of the transactions. Variability means that data is not uniform and can be easily stuffed into a single fact table of an OLAP cube. Anyone should be able to add new assertions to the graph without ever needing to rewrite queries. This property is called agility and sometime sustainability Veracity means we need to be able to validate the quality of incoming data in real-time and quickly raise warning flags if corrupt data is being transmitted into our EKG. Data quality may not be important for small departmental graph projects but becomes critical as you merge data from hundreds of different data sources. Note: We avoid using the term \"[Big Data](../glossary.md#big-data\" in this book. It is an ambiguous statement of a problem and adds no insight into a solution.","title":"Scalability Means the Four Vs"},{"location":"intro/introduction/#scalability-means-automatic-sharding-and-rebalancing","text":"The first criteria is that when the size of the enterprise graph grows to take on a new department or a new project, new hardware can be installed and the database is smart enough to use the new hardware without extensive pain and suffering by the database staff. This is illustrated in Figure 1.2. So to be brutally honest, if your graph database does not automatically rebalance data as new nodes are added or removed from a distributed graph cluster I don't consider it a true enterprise graph solution. It may still solve important business problems, but it does not fit our definition for this book.","title":"Scalability Means Automatic Sharding and Rebalancing"},{"location":"intro/introduction/#scalability-means-scalable-access-control","text":"Many graph products claim to have performance scale out capabilities, but their software falls down when we give them a a detailed list of what roles can access what data. This means they might be able to scale their data to 100 nodes, but only a single role of \"admin\" can be defined for the graph. They provide an all-or nothing approach to security. This model works in small applications where access to the application is controlled by security access rules. However it breaks down when I want 100 teams to only be able to run queries on their own team's data. We will provide more examples of vertex-level role-based access control in Chapter 3","title":"Scalability Means Scalable Access Control"},{"location":"intro/introduction/#scalability-means-high-availability","text":"Many graph products are perfect for a small team in a single city that work 9am to 5pm. Upgrades can be done at night and on weekends. But Enterprise class systems don't have users in a single city. Employees and customers are all over the world. That means when we upgrade our database with a new version of software we can't shut it down. We need distributed servers that can take a single node down, upgrade the software and get it back into the cluster without ever dropping a single transaction.","title":"Scalability Means High Availability"},{"location":"intro/introduction/#scalability-means-ease-of-creating-distributed-queries","text":"If you are building enterprise-class knowledge graphs you may need to support 100+ concurrent developer all writing queries on a graph distributed over 100+ nodes in a cluster. These queries need to efficiently distribute their work over each node and bring back just the data that is relevant to the results. For example if you run a count of customers that have purchased a specific product, that query needs to distribute the query to each node and have each node return a simple count to the server where the query originated. Those counts are summed together and returned to the user. This type of query is sometimes called a map-reduce query since only the counts (reduced data) are returned to the origin server. The actual customer data never never needs to move around.","title":"Scalability Means Ease of Creating Distributed Queries"},{"location":"intro/introduction/#scalability-means-resource-quotas","text":"In a small project graph, you often have control and review processes of the code that each developer is working on. In an enterprise setting you can't control everyone's code. You need to assume that user will write run-away queries or just run queries that are not optimized for performance during the development and testing phases of query development. You enterprise graph needs to be able to monitor rouge queries and shut them down when vertex counts, edges counts, CPU counts or memory resources reach reasonable limits. Without these resource quotas a single query can take down an entire enterprise graph database. So we need fine-grain controls in query resources, even when the queries may be running on 100 different nodes in a cluster.","title":"Scalability Means Resource Quotas"},{"location":"intro/introduction/#scalability-means-scalable-metadata-management","text":"For small project graphs, you many only have a few hundred vertices, edges and attributes. For enterprise-scale graphs you may have thousands of these items and staff that are new to the enterprise knowledge graph need to be able to quickly search for the structures they are interested in. Once they find the items of interest they need to understand their meaning and what the various code within value sets mean (reference data). New users also many need to ask the data steward for each attribute questions such as where did the data come from, what assumptions were made about the data if if the dataset contains any sensitive information that might restrict who can view the data.","title":"Scalability Means Scalable Metadata Management"},{"location":"intro/introduction/#scalability-needs-to-be-cost-effective-and-sustainable","text":"I have worked on several enterprise data warehouse projects that use relational database to perform complex queries that required many JOIN operations. Both Inman and Kimball data warehouse methodologies use relational databases and JOIN operations as their main vehicle for deployment. Most of the initial projects work fine, but as databases grow in complexity, the number of JOIN operations increase and the performance of the complex queries slowed down. The number of people who can confidently write 25-level JOIN queries is also much smaller then those that can write 10-level JOINs. Even the relational database that are built around priopirateary vendor technology that do many tricks with custom FPGA hardware to minimize the impact of JOIN operations become slower over time. But in the end, the cost most of these enterprise data warehouse systems exceeded the value of the insight gained from the systems. Dispute spending hundreds of millions of dollars on attempting to build enterprise scale analytics systems, they get decommissioned. The costs exceeded the benefits of the system. Our goal when building EKGs is to keep a sharp focus on both the costs of these systems and continuing to provide undisputable evidence of value to our business stakeholders. We will explore this topic in the chapter on EKG Cost Benefit Analysis . <!-- Non-published notes: Note this definition does not address the word \"enterprise\": https://help.poolparty.biz/pp/white-papers-release-notes/poolparty-technical-white-paper/an-enterprise-knowledge-graph-life-cycle-a-summary/the-enterprise-knowledge-graph-a-definition ->","title":"Scalability Needs to Be Cost Effective and Sustainable"},{"location":"intro/lifecycle/","text":"Enterprise Knowledge Graph Life cycles How EKGs work in practice Pilot projects Going into production Finding customers Charge backs The value of insight The First Customer The Pilot Going Into Production The Second Customer Finding Customers Charge Backs The Value of Insight","title":"Ch 3 Lifecycle"},{"location":"intro/lifecycle/#enterprise-knowledge-graph-life-cycles","text":"How EKGs work in practice Pilot projects Going into production Finding customers Charge backs The value of insight","title":"Enterprise Knowledge Graph Life cycles"},{"location":"intro/lifecycle/#the-first-customer","text":"","title":"The First Customer"},{"location":"intro/lifecycle/#the-pilot","text":"","title":"The Pilot"},{"location":"intro/lifecycle/#going-into-production","text":"","title":"Going Into Production"},{"location":"intro/lifecycle/#the-second-customer","text":"","title":"The Second Customer"},{"location":"intro/lifecycle/#finding-customers","text":"","title":"Finding Customers"},{"location":"intro/lifecycle/#charge-backs","text":"","title":"Charge Backs"},{"location":"intro/lifecycle/#the-value-of-insight","text":"","title":"The Value of Insight"},{"location":"intro/preface/","text":"Preface This book is my personal attempt to help people understand one of the most important developments in information technology: the rise of the Enterprise Knowledge Graph (EKG). I have spent most of my career helping organizations understand the strategic impact of various emerging technologies. In 2011, working with Tony Shaw at Dataversity, we created one of the first international conferences in matching business problems to the emerging market of NoSQL databases. In 2014, working with my wife, Ann Kelly, we published our book \"Making Sense of NoSQL\" which became one of the highest-rated books on the topic of NoSQL databases. Our NoSQL book was the first to propose a taxonomy of database architectures that are used to guide the solution matching process. Graph databases were one of the six solutions we found that provided a unique set of value propositions. However, at the time, graph databases didn't scale well to meet the demanding requirements of the enterprise. My research into graph databases was restricted to what could be done on small projects that ran on a single computer. In rare cases, with large R&D budgets a Cray Graph Engine. But there were so many limitations I could not see the technology as being widespread. The introduction of TigerGraph in 2017 started to change everything. Now we had a graph database that truly could gracefully scale from one to hundreds of servers to meet the demanding needs of the enterprise. TigerGraph also ran on commodity hardware and had the required security controls that would us to allow sensitive data to be loaded but not visible by everyone. This made the prospect of commercial adoption of Enterprise Knowledge Graphs viable for the first time. If you are a specialist looking for tips about a single aspect of Enterprise Knowledge Graphs, this may not be the book for you. If you are a generalist, you have come to the right place. My goal is to integrate knowledge from many fields: solution architecture, NoSQL, data warehousing, enterprise analytics, data discovery, visualization, database design, database modeling, graph algorithms, distributed systems, high-performance computing, hardware architecture, integrated circuit chip design, rules engines, schema matching, integration, data science, design patterns, machine learning, prediction, semantics, search, natural language processing, business glossaries, taxonomies, ontology management, systems thinking, complex adaptive systems, storytelling, biology, brain science, psychology, cognitive bias, philosophy, empiricism, sales, marketing, strategic planning, forecasting and finally, a bit of science fiction and futurism. What brings all these topics together? The creation and promotion of Enterprise Knowledge Graphs. If you are interested in building an Enterprise Knowledge Graph this book is for you. I know that many of the readers many not be familiar many of these topics and each is filled with complex terms that may mean things differently to you from your worldview. So I have provided an extensive Glossary of Terms with both general definitions as well as how the concepts related to Enterprise Knowledge Graphs. After reading this book, you will see that Enterprise Knowledge Graphs can provide not just a strong return on investments today, but they will become the foundation that companies will build their organization's \"enterprise brains\" in the future. Imagine opening a chatbot that knows everything about your company. All the customers, all the products, all the competitive products, all the production flows, all the employees, all the recent promotions and reorgs, all the training programs, all the documents, all the individual systems, all the reports, all the key concepts, and all the sales trends. The Enterprise Knowledge Graph is enabling all of these things today.","title":"Preface"},{"location":"intro/preface/#preface","text":"This book is my personal attempt to help people understand one of the most important developments in information technology: the rise of the Enterprise Knowledge Graph (EKG). I have spent most of my career helping organizations understand the strategic impact of various emerging technologies. In 2011, working with Tony Shaw at Dataversity, we created one of the first international conferences in matching business problems to the emerging market of NoSQL databases. In 2014, working with my wife, Ann Kelly, we published our book \"Making Sense of NoSQL\" which became one of the highest-rated books on the topic of NoSQL databases. Our NoSQL book was the first to propose a taxonomy of database architectures that are used to guide the solution matching process. Graph databases were one of the six solutions we found that provided a unique set of value propositions. However, at the time, graph databases didn't scale well to meet the demanding requirements of the enterprise. My research into graph databases was restricted to what could be done on small projects that ran on a single computer. In rare cases, with large R&D budgets a Cray Graph Engine. But there were so many limitations I could not see the technology as being widespread. The introduction of TigerGraph in 2017 started to change everything. Now we had a graph database that truly could gracefully scale from one to hundreds of servers to meet the demanding needs of the enterprise. TigerGraph also ran on commodity hardware and had the required security controls that would us to allow sensitive data to be loaded but not visible by everyone. This made the prospect of commercial adoption of Enterprise Knowledge Graphs viable for the first time. If you are a specialist looking for tips about a single aspect of Enterprise Knowledge Graphs, this may not be the book for you. If you are a generalist, you have come to the right place. My goal is to integrate knowledge from many fields: solution architecture, NoSQL, data warehousing, enterprise analytics, data discovery, visualization, database design, database modeling, graph algorithms, distributed systems, high-performance computing, hardware architecture, integrated circuit chip design, rules engines, schema matching, integration, data science, design patterns, machine learning, prediction, semantics, search, natural language processing, business glossaries, taxonomies, ontology management, systems thinking, complex adaptive systems, storytelling, biology, brain science, psychology, cognitive bias, philosophy, empiricism, sales, marketing, strategic planning, forecasting and finally, a bit of science fiction and futurism. What brings all these topics together? The creation and promotion of Enterprise Knowledge Graphs. If you are interested in building an Enterprise Knowledge Graph this book is for you. I know that many of the readers many not be familiar many of these topics and each is filled with complex terms that may mean things differently to you from your worldview. So I have provided an extensive Glossary of Terms with both general definitions as well as how the concepts related to Enterprise Knowledge Graphs. After reading this book, you will see that Enterprise Knowledge Graphs can provide not just a strong return on investments today, but they will become the foundation that companies will build their organization's \"enterprise brains\" in the future. Imagine opening a chatbot that knows everything about your company. All the customers, all the products, all the competitive products, all the production flows, all the employees, all the recent promotions and reorgs, all the training programs, all the documents, all the individual systems, all the reports, all the key concepts, and all the sales trends. The Enterprise Knowledge Graph is enabling all of these things today.","title":"Preface"},{"location":"intro/stories/","text":"Enterprise Knowledge Graph Stories The Google Knowledge Graph On May 16, 2012, Google published the \"Things Not Strings\" blog post. Now the world knew that graphs were no longer an academic interest. Google's graph serviced millions of requests per minute and was available 24X7. Knowlege Graphs became cool. Introducing the Knowledge Graph: things, not strings Glossary to Taxonomy to Ontology to Graph Tracy had a background in library science. She was asked to help a manufacturing company organize their datasets. Stage 1: The Glossary On Tracy's first day on the job, she heard dozens of terms she had never heard before. Many of them were acronyms of internal systems and projects. She started buy writing down the terms that she didn't understand in the first column of a spreadsheet. She put the definitions of the term in the second column. She was building a business glossary. Stage 2: The Taxonomy After a while, Tracy saw some recurring patterns in her terms. Some were computer application names, some were product names and some were \"other\". She started grouping the related terms together and added another column for the category of each term. She had a concept taxonomy. Stage 3: The Ontology After a while, Tracy's categories started to grow and become more complex. Categories had sub-categories and now she started to see relationships between terms. Terms had broader terms and narrower terms. Some teams used different names (or labels) for the same concept. Tracy now had a graph of concepts. She had an ontology. Tracy could no longer maintain the system using a simple spreadsheet. She worked with her peers to create a web front end to a graph database so it was easy for anyone to add and update concepts. Stage 4: Reference Data Her graph database continued to grow. For many concepts she was asked to store a code-set that described the valid values that data element could contain. She was building a reference data set. Here reference data started simply - a list of country codes, a list of state codes and a list of regions that included states. Then they asked her to list all the cities that they sold products. Soon she was tracking all the cities in the world and their long-lat coordinates. But this was OK, because the graph database that Tracy selected scaled well as the data complexity and size grew. State 5: The Project Knowledge Graph The company like the fact that when Tracy was asked to add a new feature it was always done quickly. The data model scaled well and Tracy was not forced to rewrite queries as the data grew. As the request for more data continued Tracy added detailed product information and the customers that used these products. She then got regular updates of customer lists and their customer satisfaction surveys. Now multiple departments wanted access to Tracy's database. She became an enterprise resource. State 6: The Enterprise Knowledge Graph Now that Tracy had more and more customer data and their purchase history, Tracy had to use multiple servers to manage the data. The company also needed to access customer records 24X7 so she put in tools to automatically replicate data and automatically migrated data to new servers as they were added to the cluster. Tracy had built a highly available enterprise knowledge graph. Customer 360 For many years the company had grown by acquiring new companies. Unfortunately, the computer systems used by each company was incompatible so the customer call centers would have to put customers on hold to long into each of the 10 different computers that stored the customer information.","title":"Enterprise Knowledge Graph Stories"},{"location":"intro/stories/#enterprise-knowledge-graph-stories","text":"","title":"Enterprise Knowledge Graph Stories"},{"location":"intro/stories/#the-google-knowledge-graph","text":"On May 16, 2012, Google published the \"Things Not Strings\" blog post. Now the world knew that graphs were no longer an academic interest. Google's graph serviced millions of requests per minute and was available 24X7. Knowlege Graphs became cool. Introducing the Knowledge Graph: things, not strings","title":"The Google Knowledge Graph"},{"location":"intro/stories/#glossary-to-taxonomy-to-ontology-to-graph","text":"Tracy had a background in library science. She was asked to help a manufacturing company organize their datasets.","title":"Glossary to Taxonomy to Ontology to Graph"},{"location":"intro/stories/#stage-1-the-glossary","text":"On Tracy's first day on the job, she heard dozens of terms she had never heard before. Many of them were acronyms of internal systems and projects. She started buy writing down the terms that she didn't understand in the first column of a spreadsheet. She put the definitions of the term in the second column. She was building a business glossary.","title":"Stage 1: The Glossary"},{"location":"intro/stories/#stage-2-the-taxonomy","text":"After a while, Tracy saw some recurring patterns in her terms. Some were computer application names, some were product names and some were \"other\". She started grouping the related terms together and added another column for the category of each term. She had a concept taxonomy.","title":"Stage 2: The Taxonomy"},{"location":"intro/stories/#stage-3-the-ontology","text":"After a while, Tracy's categories started to grow and become more complex. Categories had sub-categories and now she started to see relationships between terms. Terms had broader terms and narrower terms. Some teams used different names (or labels) for the same concept. Tracy now had a graph of concepts. She had an ontology. Tracy could no longer maintain the system using a simple spreadsheet. She worked with her peers to create a web front end to a graph database so it was easy for anyone to add and update concepts.","title":"Stage 3: The Ontology"},{"location":"intro/stories/#stage-4-reference-data","text":"Her graph database continued to grow. For many concepts she was asked to store a code-set that described the valid values that data element could contain. She was building a reference data set. Here reference data started simply - a list of country codes, a list of state codes and a list of regions that included states. Then they asked her to list all the cities that they sold products. Soon she was tracking all the cities in the world and their long-lat coordinates. But this was OK, because the graph database that Tracy selected scaled well as the data complexity and size grew.","title":"Stage 4: Reference Data"},{"location":"intro/stories/#state-5-the-project-knowledge-graph","text":"The company like the fact that when Tracy was asked to add a new feature it was always done quickly. The data model scaled well and Tracy was not forced to rewrite queries as the data grew. As the request for more data continued Tracy added detailed product information and the customers that used these products. She then got regular updates of customer lists and their customer satisfaction surveys. Now multiple departments wanted access to Tracy's database. She became an enterprise resource.","title":"State 5: The Project Knowledge Graph"},{"location":"intro/stories/#state-6-the-enterprise-knowledge-graph","text":"Now that Tracy had more and more customer data and their purchase history, Tracy had to use multiple servers to manage the data. The company also needed to access customer records 24X7 so she put in tools to automatically replicate data and automatically migrated data to new servers as they were added to the cluster. Tracy had built a highly available enterprise knowledge graph.","title":"State 6: The Enterprise Knowledge Graph"},{"location":"intro/stories/#customer-360","text":"For many years the company had grown by acquiring new companies. Unfortunately, the computer systems used by each company was incompatible so the customer call centers would have to put customers on hold to long into each of the 10 different computers that stored the customer information.","title":"Customer 360"},{"location":"intro/trends/","text":"Enterprise Knowledge Graph Database Trends In this chapter: DB Engines Trends Hardware Trends Enterprise Graph Vendor Trends Graph Query Language Standardization Trends Standard Graph Algorithm Trends Graph Machine Learning Trends Third Party Trends Graph Visualization Library Trends Graph Experts Salary Trends DB Engines Trends Hardware Trends Enterprise Graph Vendor Trends Graph Query Language Standardization Trends Standard Graph Algorithm Trends Graph Machine Learning Trends Third Party Trends Graph Visualization Library Trends Graph Experts Salary Trends References https://db-engines.com/en/ranking_trend/graph+dbms https://www.eweek.com/database/why-experts-see-graph-databases-headed-to-mainstream-use https://www.eweek.com/database/first-industrywide-graph-db-conference-set-for-sept.-28-30 https://www.eweek.com/innovation/top-10-technology-analysts-of-2020","title":"Ch 4 Graph Adoption Trends"},{"location":"intro/trends/#enterprise-knowledge-graph-database-trends","text":"In this chapter: DB Engines Trends Hardware Trends Enterprise Graph Vendor Trends Graph Query Language Standardization Trends Standard Graph Algorithm Trends Graph Machine Learning Trends Third Party Trends Graph Visualization Library Trends Graph Experts Salary Trends","title":"Enterprise Knowledge Graph Database Trends"},{"location":"intro/trends/#db-engines-trends","text":"","title":"DB Engines Trends"},{"location":"intro/trends/#hardware-trends","text":"","title":"Hardware Trends"},{"location":"intro/trends/#enterprise-graph-vendor-trends","text":"","title":"Enterprise Graph Vendor Trends"},{"location":"intro/trends/#graph-query-language-standardization-trends","text":"","title":"Graph Query Language Standardization Trends"},{"location":"intro/trends/#standard-graph-algorithm-trends","text":"","title":"Standard Graph Algorithm Trends"},{"location":"intro/trends/#graph-machine-learning-trends","text":"","title":"Graph Machine Learning Trends"},{"location":"intro/trends/#third-party-trends","text":"","title":"Third Party Trends"},{"location":"intro/trends/#graph-visualization-library-trends","text":"","title":"Graph Visualization Library Trends"},{"location":"intro/trends/#graph-experts-salary-trends","text":"","title":"Graph Experts Salary Trends"},{"location":"intro/trends/#references","text":"https://db-engines.com/en/ranking_trend/graph+dbms https://www.eweek.com/database/why-experts-see-graph-databases-headed-to-mainstream-use https://www.eweek.com/database/first-industrywide-graph-db-conference-set-for-sept.-28-30 https://www.eweek.com/innovation/top-10-technology-analysts-of-2020","title":"References"},{"location":"promoting/cognitive-bias/","text":"Cognitive Bias in Enterprise Knowledge Selection In this chapter we will look into the reason that organizations are not adopting enterprise knowledge graphs due to logical errors in judgement. We call these persistent patterns of error Cognitive Bias . When we combine this knowledge with our understanding of the Technology adoption life cycle and Windows of Opportunigy we can start to make create a predictive model of when and organization might be ready to adopt enterprise knowledge graph technology. When I first started to focus on solution architecture consulting, I was frequently hired by organizations that wanted to bring in an objective external consultant to help them evaluate options for a specific business project. Although our book, Making Sense of NoSQL had ample information on how to do this objective analysis, many companies still wanted an experienced outside person to oversee this processes. Although many of these projects went well, I was often disheartened when organizations didn't make the appropriate choices. I reasoned that they were making political choices, not rational choices based on evidence, and tried to wash my hands of their choices and I moved on to the next project. Then in 2013, I attended a conference sponsored by the people that developed and used the ATAM process originally developed at CMU's Software Engineering Institute. This was the CMU Saturn Conference that focused on researchers trying to understand the architecture analysis process. One of the speakers was the incredibly insightful Mary Poppendieck . Mary's presentation was all about how cognitive bias has a strong influence on how organizations select any given technology. I was thrilled to finally have a precise taxonomy of the reasons that politics drove organizational decision making. You can see the slides of my presentation on the application of ATAM to database selection here . You can see that many of the concepts in this book are present in this presentation. Since Mary's talk in 2013 I have carefully documented many of the bias I have seen in organization decision making. Here are some of them: Anchoring bias Availability bias a.k.a. memory bias, familiarity heuristic Bandwagon effect Confirmation bias a.k.a. Fiter bubble Halo effect Hindsight bias Illusory superiority bias Framing effect Narrative-bias Representativeness heuristic Silver bullet Status_quo_bias Sunk cost a.k.a. Gamblers fallacy Anchoring Bias Availability Bias Bandwagon Effect Confirmation Bias Halo Effect Hindsight Bias Illusionary Superiority Bias Framing Effect Narrative Bias Representativeness Heuristic Silver Bullet Sunk Cost","title":"Cognitive Bias"},{"location":"promoting/cognitive-bias/#cognitive-bias-in-enterprise-knowledge-selection","text":"In this chapter we will look into the reason that organizations are not adopting enterprise knowledge graphs due to logical errors in judgement. We call these persistent patterns of error Cognitive Bias . When we combine this knowledge with our understanding of the Technology adoption life cycle and Windows of Opportunigy we can start to make create a predictive model of when and organization might be ready to adopt enterprise knowledge graph technology. When I first started to focus on solution architecture consulting, I was frequently hired by organizations that wanted to bring in an objective external consultant to help them evaluate options for a specific business project. Although our book, Making Sense of NoSQL had ample information on how to do this objective analysis, many companies still wanted an experienced outside person to oversee this processes. Although many of these projects went well, I was often disheartened when organizations didn't make the appropriate choices. I reasoned that they were making political choices, not rational choices based on evidence, and tried to wash my hands of their choices and I moved on to the next project. Then in 2013, I attended a conference sponsored by the people that developed and used the ATAM process originally developed at CMU's Software Engineering Institute. This was the CMU Saturn Conference that focused on researchers trying to understand the architecture analysis process. One of the speakers was the incredibly insightful Mary Poppendieck . Mary's presentation was all about how cognitive bias has a strong influence on how organizations select any given technology. I was thrilled to finally have a precise taxonomy of the reasons that politics drove organizational decision making. You can see the slides of my presentation on the application of ATAM to database selection here . You can see that many of the concepts in this book are present in this presentation. Since Mary's talk in 2013 I have carefully documented many of the bias I have seen in organization decision making. Here are some of them: Anchoring bias Availability bias a.k.a. memory bias, familiarity heuristic Bandwagon effect Confirmation bias a.k.a. Fiter bubble Halo effect Hindsight bias Illusory superiority bias Framing effect Narrative-bias Representativeness heuristic Silver bullet Status_quo_bias Sunk cost a.k.a. Gamblers fallacy","title":"Cognitive Bias in Enterprise Knowledge Selection"},{"location":"promoting/cognitive-bias/#anchoring-bias","text":"","title":"Anchoring Bias"},{"location":"promoting/cognitive-bias/#availability-bias","text":"","title":"Availability Bias"},{"location":"promoting/cognitive-bias/#bandwagon-effect","text":"","title":"Bandwagon Effect"},{"location":"promoting/cognitive-bias/#confirmation-bias","text":"","title":"Confirmation Bias"},{"location":"promoting/cognitive-bias/#halo-effect","text":"","title":"Halo Effect"},{"location":"promoting/cognitive-bias/#hindsight-bias","text":"","title":"Hindsight Bias"},{"location":"promoting/cognitive-bias/#illusionary-superiority-bias","text":"","title":"Illusionary Superiority Bias"},{"location":"promoting/cognitive-bias/#framing-effect","text":"","title":"Framing Effect"},{"location":"promoting/cognitive-bias/#narrative-bias","text":"","title":"Narrative Bias"},{"location":"promoting/cognitive-bias/#representativeness-heuristic","text":"","title":"Representativeness Heuristic"},{"location":"promoting/cognitive-bias/#silver-bullet","text":"","title":"Silver Bullet"},{"location":"promoting/cognitive-bias/#sunk-cost","text":"","title":"Sunk Cost"},{"location":"promoting/conclusion/","text":"Conclusion Never mistake a clear view for a short distance.\u201d \u2014 Paul Saffo We now come to an end of our journey through our tour of our knowledge about enterprise knowledge graphs. We have touched on a wide number of topics and integrated concepts that I suspect you will not find in any single course in a typical universities course catalog. Which is too bad. Curriculum design is often driven by historical artifacts, not what is most useful to students. Artificial barriers between college and university departments promotes specialized hedgehog thinking , not systems thinking. I hope that you will agree, that to successfully implement sustainable enterprise knowledge graph you need to be a generalist and draw knowledge from many disciplines. I hope this book will help you in your journey. I would like to conclude with a few thought experiments about what enterprise knowledge graphs can become in the near term future. Being aware of the Paul Saffo quote above, I am going to hold off on giving you a predicted timeline of when these events will occur. Prediction is hard, and there are many uncertain events that will radically change the the course of enterprise knowledge graphs. Specialized Enterprise Graph Hardware We know that as graph databases increase in popularity, that they will gain the attention of people that are trying to optimize hardware to match the unique workload of enterprise knowledge graphs. Companies like Graphcore and Intel with their PIUMA project have already begun this process. The challenge is that not all graph databases will leverage this new hardware. Doing so requires graph databases to take advantage of specialized hardware to create fast graph traversal. It will take time for both software and hardware to align their systems to work together. Natural Language Query Software like GPT-3 has already shown it can take English language description of queries and convert them to SQL. Look for similar tools that will allow non-technical staff to literally ask complex questions about the content of a knowledge graph. Embeddings Everywhere Graph embedding is just in it's infancy. Right now it takes a huge amount of effort from dedicated data scientists that also understand graph data models to build embeddings. In the future, graph databases will automatically build embedding for you by default. Embedding will be associated with vertices, edges and paths and intelligent query generators will know how to use them in queries. We will finally get to the state where our data science and our query developers will work together in a seamless way. Subgraphs and Micro Reasoners Enterprise knowledge graphs may have standardized subgraphs that can be quickly loaded with a few clicks of an administration tool. Need all the geolocations in the US including every geocoded address, city, state, county and zip code? We have a subgraph for you and all the inference queries over that subgraph.","title":"Conclusion"},{"location":"promoting/conclusion/#conclusion","text":"Never mistake a clear view for a short distance.\u201d \u2014 Paul Saffo We now come to an end of our journey through our tour of our knowledge about enterprise knowledge graphs. We have touched on a wide number of topics and integrated concepts that I suspect you will not find in any single course in a typical universities course catalog. Which is too bad. Curriculum design is often driven by historical artifacts, not what is most useful to students. Artificial barriers between college and university departments promotes specialized hedgehog thinking , not systems thinking. I hope that you will agree, that to successfully implement sustainable enterprise knowledge graph you need to be a generalist and draw knowledge from many disciplines. I hope this book will help you in your journey. I would like to conclude with a few thought experiments about what enterprise knowledge graphs can become in the near term future. Being aware of the Paul Saffo quote above, I am going to hold off on giving you a predicted timeline of when these events will occur. Prediction is hard, and there are many uncertain events that will radically change the the course of enterprise knowledge graphs.","title":"Conclusion"},{"location":"promoting/conclusion/#specialized-enterprise-graph-hardware","text":"We know that as graph databases increase in popularity, that they will gain the attention of people that are trying to optimize hardware to match the unique workload of enterprise knowledge graphs. Companies like Graphcore and Intel with their PIUMA project have already begun this process. The challenge is that not all graph databases will leverage this new hardware. Doing so requires graph databases to take advantage of specialized hardware to create fast graph traversal. It will take time for both software and hardware to align their systems to work together.","title":"Specialized Enterprise Graph Hardware"},{"location":"promoting/conclusion/#natural-language-query","text":"Software like GPT-3 has already shown it can take English language description of queries and convert them to SQL. Look for similar tools that will allow non-technical staff to literally ask complex questions about the content of a knowledge graph.","title":"Natural Language Query"},{"location":"promoting/conclusion/#embeddings-everywhere","text":"Graph embedding is just in it's infancy. Right now it takes a huge amount of effort from dedicated data scientists that also understand graph data models to build embeddings. In the future, graph databases will automatically build embedding for you by default. Embedding will be associated with vertices, edges and paths and intelligent query generators will know how to use them in queries. We will finally get to the state where our data science and our query developers will work together in a seamless way.","title":"Embeddings Everywhere"},{"location":"promoting/conclusion/#subgraphs-and-micro-reasoners","text":"Enterprise knowledge graphs may have standardized subgraphs that can be quickly loaded with a few clicks of an administration tool. Need all the geolocations in the US including every geocoded address, city, state, county and zip code? We have a subgraph for you and all the inference queries over that subgraph.","title":"Subgraphs and Micro Reasoners"},{"location":"promoting/strategic-serendipity/","text":"Strategic Serendipity Luck favors the prepared. - Louis Pasteur When we thing of enterprise strategies, we often think of large groups of executives at multi-day off-site retreats. They will be brainstorming, classifying ideas, looking for synergies, prioritizing, and then putting details into what they hope will be thoughtful planning for the next year and beyond. Their goal is to prioritize concrete tasks that can be acted upon, assigned to teams, measured and then deliver results that can be measured with key performance indicators . Their job is to remove uncertainty and randomness from their plan. But there is ample evidence that innovation and insight are not easy to predict. Many stories about great discoveries such as the discovery of Penicillin by Alexander Fleming amount to random events that literally blew in through an open window. Serendipity is the occurrence of random events that trigger positive outcomes. Strategy and serendipity are oppositional thoughts. Not everyone is good at holding oppositional ideas in their mind to come up with new innovations. It takes practice. We define Strategic Serendipity as the process of thoughtfully creating an environment that promotes insights, serendipity and tangible savings from newly connected information. This means creating an environment around your EKG that encourages discovery. Discovery and the EKG Discovery is different from creating regular operational reports on your EKG. Operational reports show things like counts of vertices of various types with special conditions. The key is operational report on an EKG are run regularly and consistently and often feed the monitoring dashboards that are customized for each user. But other than an occasional new trend line, they don't play a role in the EKG Discovery process. In the context of the EKG, data discovery (also called data mining) is a process of discovering new patterns in large data sets using a combination of query tools, machine learning and often visualization. A Story of Connected Data, Serendipity and Fraud Detection Let me start out with a true story about what I mean by this idea of strategic serendipity environment. When we first building graph databases we started to connect new data sources that had not been connected before. One of our bright young engineers started seeing an unusual pattern of very small healthcare claims in dataset. Being naturally curious, the engineer started to \"connect some dots\". Because the team had not just loaded the flat claims items but also loaded and linked the providers that submitted the claims, the engineer could write simple queries to see that many of the small claims came from just a few providers. Although the individual claims were small, when taken in aggregate, they amounted to over $10M in claims. This pattern just did not make sense to the engineer. So the engineer started to dig deeper. The providers submitted these tiny claims were well outside the normal ranges of what other providers charged for similar procedures. The engineer came up with charge distributions for other providers and then showed the average claims for \"unusual\" providers were much lower than normal. Once they had the unusual providers identified they also saw that these claims were for procedures that their specialty codes didn't indicated they should be doing. The engineer had found that some fraudsters had found a \"loophole\" in our audit rules. Things below a specific threshold just went undetected by our audit rules, regardless of volume. The fraud ring used stolen physician IDs to route claim reimbursements to their own personal bank accounts. What is important about this story is that the engineer could never have found the patterns without connecting claims with physicians with their specialty classifications. The values was not just in the flat claims files. We needed to connect the data together using a graph database. Remember that no one ever sat down with the team and said \"go find fraudulent claims\". We know that fraud in healthcare claims cost US taxpayers what is estimated to be $50 billion dollars a year. But fraudsters have grown every more sophisticated in their tactics. And not all engineers are encouraged to follow their curious instincts. So this is the job of a enterprise strategist. We need to create environments that encourage insight and discovery. Understand that it is impossible to predict exactly what the discoveries will be. We can't predict when and where the discoveries will be. But let's be very clear. If we don't create an integrated set of connected data the insights may never be found.","title":"Strategic Serendipity"},{"location":"promoting/strategic-serendipity/#strategic-serendipity","text":"Luck favors the prepared. - Louis Pasteur When we thing of enterprise strategies, we often think of large groups of executives at multi-day off-site retreats. They will be brainstorming, classifying ideas, looking for synergies, prioritizing, and then putting details into what they hope will be thoughtful planning for the next year and beyond. Their goal is to prioritize concrete tasks that can be acted upon, assigned to teams, measured and then deliver results that can be measured with key performance indicators . Their job is to remove uncertainty and randomness from their plan. But there is ample evidence that innovation and insight are not easy to predict. Many stories about great discoveries such as the discovery of Penicillin by Alexander Fleming amount to random events that literally blew in through an open window. Serendipity is the occurrence of random events that trigger positive outcomes. Strategy and serendipity are oppositional thoughts. Not everyone is good at holding oppositional ideas in their mind to come up with new innovations. It takes practice. We define Strategic Serendipity as the process of thoughtfully creating an environment that promotes insights, serendipity and tangible savings from newly connected information. This means creating an environment around your EKG that encourages discovery.","title":"Strategic Serendipity"},{"location":"promoting/strategic-serendipity/#discovery-and-the-ekg","text":"Discovery is different from creating regular operational reports on your EKG. Operational reports show things like counts of vertices of various types with special conditions. The key is operational report on an EKG are run regularly and consistently and often feed the monitoring dashboards that are customized for each user. But other than an occasional new trend line, they don't play a role in the EKG Discovery process. In the context of the EKG, data discovery (also called data mining) is a process of discovering new patterns in large data sets using a combination of query tools, machine learning and often visualization.","title":"Discovery and the EKG"},{"location":"promoting/strategic-serendipity/#a-story-of-connected-data-serendipity-and-fraud-detection","text":"Let me start out with a true story about what I mean by this idea of strategic serendipity environment. When we first building graph databases we started to connect new data sources that had not been connected before. One of our bright young engineers started seeing an unusual pattern of very small healthcare claims in dataset. Being naturally curious, the engineer started to \"connect some dots\". Because the team had not just loaded the flat claims items but also loaded and linked the providers that submitted the claims, the engineer could write simple queries to see that many of the small claims came from just a few providers. Although the individual claims were small, when taken in aggregate, they amounted to over $10M in claims. This pattern just did not make sense to the engineer. So the engineer started to dig deeper. The providers submitted these tiny claims were well outside the normal ranges of what other providers charged for similar procedures. The engineer came up with charge distributions for other providers and then showed the average claims for \"unusual\" providers were much lower than normal. Once they had the unusual providers identified they also saw that these claims were for procedures that their specialty codes didn't indicated they should be doing. The engineer had found that some fraudsters had found a \"loophole\" in our audit rules. Things below a specific threshold just went undetected by our audit rules, regardless of volume. The fraud ring used stolen physician IDs to route claim reimbursements to their own personal bank accounts. What is important about this story is that the engineer could never have found the patterns without connecting claims with physicians with their specialty classifications. The values was not just in the flat claims files. We needed to connect the data together using a graph database. Remember that no one ever sat down with the team and said \"go find fraudulent claims\". We know that fraud in healthcare claims cost US taxpayers what is estimated to be $50 billion dollars a year. But fraudsters have grown every more sophisticated in their tactics. And not all engineers are encouraged to follow their curious instincts. So this is the job of a enterprise strategist. We need to create environments that encourage insight and discovery. Understand that it is impossible to predict exactly what the discoveries will be. We can't predict when and where the discoveries will be. But let's be very clear. If we don't create an integrated set of connected data the insights may never be found.","title":"A Story of Connected Data, Serendipity and Fraud Detection"},{"location":"promoting/technology-adoption/","text":"Technology Adoption Wikipedia","title":"Technology Adoption"},{"location":"promoting/technology-adoption/#technology-adoption","text":"Wikipedia","title":"Technology Adoption"},{"location":"promoting/windows-of-opportunity/","text":"Windows of Opportunity in Knowledge Graph Adoption Defining the EKG window of opportunity Necessary preconditions The EKG pitch deck The cost-benefit analysis The pilot project Testing scalability Taking advantage of a crisis Most large organization goes through annual planning cycles. Executives ponder how they can cut costs, raise sales and create new systems that might be more responsive to customer needs. But it is rare that executives all sit around and say \"we really need an enterprise knowledge graph\". From what I can tell, it has never happened. Companies that successfully adopt enterprise knowledge graphs go through a series of stages before they embark on a pilot EKG project. Once they do start the pilot project, the clocks usually begin ticking. If the right preconditions are not met, the pilot may fail and the organization will quickly determine that the EKG is not a good fit for their organization. This narrow time slot is called the Window of Opportunity for EKGs. Knowing how to recognize these windows and take advantage of them is a key skill we attempt to explore in this book. Necessary Preconditions Prepping the Pitch Deck When I worked with Steve Jobs at NeXT computer one of my jobs was to prep events for Steve's presentations. I learned important lessons on the value of preparation before you give a presentation. Everything has to do with attention to detail and finding a message that will resonate with your audience. People will respect you if you do a detailed analysis of their situation before you propose a solution. Speak the language of the audience - start with their strategic objectives Doing your homeworks - knowing their pain points Knowing who they respect - what books do their leaders quote from? Using insider knowledge - what pain points will the developers share References - who else in the organization do they trust Managing emotions: fear and greed - fear they will be left out - greed for the bonus of cost reduction and better insights Knowing how much detail is enough The Cost Benefit Analysis","title":"Windows of Opportunity in Knowledge Graph Adoption"},{"location":"promoting/windows-of-opportunity/#windows-of-opportunity-in-knowledge-graph-adoption","text":"Defining the EKG window of opportunity Necessary preconditions The EKG pitch deck The cost-benefit analysis The pilot project Testing scalability Taking advantage of a crisis Most large organization goes through annual planning cycles. Executives ponder how they can cut costs, raise sales and create new systems that might be more responsive to customer needs. But it is rare that executives all sit around and say \"we really need an enterprise knowledge graph\". From what I can tell, it has never happened. Companies that successfully adopt enterprise knowledge graphs go through a series of stages before they embark on a pilot EKG project. Once they do start the pilot project, the clocks usually begin ticking. If the right preconditions are not met, the pilot may fail and the organization will quickly determine that the EKG is not a good fit for their organization. This narrow time slot is called the Window of Opportunity for EKGs. Knowing how to recognize these windows and take advantage of them is a key skill we attempt to explore in this book.","title":"Windows of Opportunity in Knowledge Graph Adoption"},{"location":"promoting/windows-of-opportunity/#necessary-preconditions","text":"","title":"Necessary Preconditions"},{"location":"promoting/windows-of-opportunity/#prepping-the-pitch-deck","text":"When I worked with Steve Jobs at NeXT computer one of my jobs was to prep events for Steve's presentations. I learned important lessons on the value of preparation before you give a presentation. Everything has to do with attention to detail and finding a message that will resonate with your audience. People will respect you if you do a detailed analysis of their situation before you propose a solution. Speak the language of the audience - start with their strategic objectives Doing your homeworks - knowing their pain points Knowing who they respect - what books do their leaders quote from? Using insider knowledge - what pain points will the developers share References - who else in the organization do they trust Managing emotions: fear and greed - fear they will be left out - greed for the bonus of cost reduction and better insights Knowing how much detail is enough","title":"Prepping the Pitch Deck"},{"location":"promoting/windows-of-opportunity/#the-cost-benefit-analysis","text":"","title":"The Cost Benefit Analysis"}]}