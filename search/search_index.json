{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Enterprise Knowledge Graphs This book is about using scalable graph databases to store large-scale connected information for an entire enterprise. This book is in early draft stage and may go through significant structural changes as we get feedback from our readers. Pull requests are gladly accepted. Feel free to post general comments on the GitHub issue area of the GitHub repository that backs this book. If you want to connect with me, please join my LinkedIn graph: https://www.linkedin.com/in/danmccreary/ Table of Contents","title":"Home"},{"location":"#enterprise-knowledge-graphs","text":"This book is about using scalable graph databases to store large-scale connected information for an entire enterprise. This book is in early draft stage and may go through significant structural changes as we get feedback from our readers. Pull requests are gladly accepted. Feel free to post general comments on the GitHub issue area of the GitHub repository that backs this book. If you want to connect with me, please join my LinkedIn graph: https://www.linkedin.com/in/danmccreary/ Table of Contents","title":"Enterprise Knowledge Graphs"},{"location":"blogs/","text":"Summary of Blogs on Enterprise Knowledge Graphs As part of my job as a Distinguished Engineer I am asked to be a recognized thought leader. I attempt to fulfill part of this role in blogging on topics related to AI, machine learning and knowledge representation. You can find my current blogs on Medium: https://dmccreary.medium.com/ I should note that although I try to keep my original blogs mostly intact, sometimes I do fix errors and add additional references. Here is a summary of some of my blogs related directly to Enterprise Knowledge Graphs in reverse chronological order. Rules for Knowledge Graph Rules Published Dec. 2020 Should you store your business rules in a knowledge graph? Understanding Graph Embeddings Published Nov. 23rd 2020 Intel\u2019s Incredible PIUMA Graph Analytics Hardware [The Knowledge Triangle] Knowledge Graphs: The Third Era of Computing I make the case that knowledge graphs will become the third era of computing. [From Data Science to Knowledge Science] (https://dmccreary.medium.com/from-data-science-to-knowledge-science-7f6707727489) In this blog post, I make the argument that accessing a high-quality and highly connected knowledge graph is much more productive than trying have all your data scientists re-interpret raw-data for each project.","title":"Blogs"},{"location":"blogs/#summary-of-blogs-on-enterprise-knowledge-graphs","text":"As part of my job as a Distinguished Engineer I am asked to be a recognized thought leader. I attempt to fulfill part of this role in blogging on topics related to AI, machine learning and knowledge representation. You can find my current blogs on Medium: https://dmccreary.medium.com/ I should note that although I try to keep my original blogs mostly intact, sometimes I do fix errors and add additional references. Here is a summary of some of my blogs related directly to Enterprise Knowledge Graphs in reverse chronological order.","title":"Summary of Blogs on Enterprise Knowledge Graphs"},{"location":"blogs/#rules-for-knowledge-graph-rules-published-dec-2020","text":"Should you store your business rules in a knowledge graph?","title":"Rules for Knowledge Graph Rules Published Dec. 2020"},{"location":"blogs/#understanding-graph-embeddings","text":"Published Nov. 23rd 2020","title":"Understanding Graph Embeddings"},{"location":"blogs/#intels-incredible-piuma-graph-analytics-hardware","text":"","title":"Intel\u2019s Incredible PIUMA Graph Analytics Hardware"},{"location":"blogs/#the-knowledge-triangle","text":"","title":"[The Knowledge Triangle]"},{"location":"blogs/#knowledge-graphs-the-third-era-of-computing","text":"I make the case that knowledge graphs will become the third era of computing.","title":"Knowledge Graphs: The Third Era of Computing"},{"location":"blogs/#from-data-science-to-knowledge-science","text":"(https://dmccreary.medium.com/from-data-science-to-knowledge-science-7f6707727489) In this blog post, I make the argument that accessing a high-quality and highly connected knowledge graph is much more productive than trying have all your data scientists re-interpret raw-data for each project.","title":"[From Data Science to Knowledge Science]"},{"location":"contact/","text":"Contact Please connect with me on LinkedIn: Dan McCreary https://www.linkedin.com/in/danmccreary/ You can find blog on medium: https://dmccreary.medium.com/","title":"Contact"},{"location":"contact/#contact","text":"Please connect with me on LinkedIn: Dan McCreary https://www.linkedin.com/in/danmccreary/ You can find blog on medium: https://dmccreary.medium.com/","title":"Contact"},{"location":"glossary/","text":"Enterprise Knowledge Graph Glossary of Terms ABox Types of assertions or statements in a knowledge graph that conform to specific terminologies (knows as TBox statements). The terms ABox and TBox are used to help determine if a statement is universal or related to a specific Subgraph of an Enterprises Knowledge Graph . Within our Enterprise Knowledge Graph architecture, ABox statements often contain knowledge about specific customers, parts, or concepts and may have specific Access Control rules. TBox statements don't usually have these same access rules. See also: TBox Wikipedia page on ABox Accumulator A type of variable that tracks items as you traverse through a graph. Accumulators can be global or be attached to a specific vertex. Accumulators allow MapReduce style queries where each server node in a cluster does work in its local data and returns consolidated results to the query node. For example, in the query \"count all customers that have returned clothing items\" , each node would return only a single count to the query node. This type of query reduces the amount of communication between nodes in the graph cluster. Alternate Label In a concept subgraph of a EKG, every concept has a single preferred label (per language) and many alternate labels that can also be used to name a concept. Alternate labels can be abbreviations, acronyms and synonyms of the concept. In this glossary, alternate labels have a \"Also known as\" prefix. Also known as: atlLabel See also: Preferred Label See also: SKOS Amazon Simple Storage System A high-availability cloud-based Key-value Store system created by Amazon in 2006. In 2006 Amazon was a pioneer in the creation of cloud-based data services. Because S3 was one of the first ultra-reliable cloud based services it gained early market share and now dominates the data storage industry. The API for S3 is now duplicated by dozens of other vendors and has become a defacto standard for cloud-based key-value stores. S3 was the first service launched as part of Amazon Web Services (AWS). Also known as: S3 Wikipedia page on Amazon S3 Availability Heuristic The availability heuristic suggests that the likelihood of events is estimated based on how many examples of such events come to mind. For example, when a solution architect is presented with a business problem, their likelihood to recommend an enterprise knowledge graph solution is dependant on how many examples of successful enterprise knowledge graph project come to mind. Surrounding solution architects with many successful stories or case studies of successful enterprise knowledge graph projects may positively impact their probability of recomending an enterprise knowledge graph as an option. See also: Cognitive Bias Wikipedia Case Study A Priori and A Posteriori A priori knowledge is that which is independent of experience. A posteriori knowledge is that which depends on empirical evidence. The rules of mathematics, logic, and business rules are usually classified as a priori read-access is usually shared across all users of an enterprise knowledge graph. Knowledge about a specific observable event, observation, customer transaction, etc. are considered A Posteriori and may not need to be universally accessed. In an enterprise knowledge graph, a priori knowledge tends to be more universal such as TBox assertions that are universally accessible by all subgraphs . https://en.wikipedia.org/wiki/A_priori_and_a_posteriori Automatic Sharding The process of automatically migrating data from one server to another server in a distributed database. Auto-sharding is frequently done as a database cluster grows or shrinks based on new data being added or removed from the cluster. Auto-sharding is one of the key features that differentiate scale-out enterprise-class databases from departmental solutions. Testing sharding at scale under continuous load in the face of possible hardware failure is one of the key challenges facing enterprise data architects. Big Data An ambiguous term that many or may not refer to data sizes beyond the ability of commonly used tools to mange data. If you used a spreadsheet, \"Big Data\" could be any data that does not fit into your spreadsheet. If you use a Cray Supercomputer then your definition of Big Data could differ by ten orders of magnitude. If we ever hear someone refer to \"Big Data\" we strongly suggest they use other terms that have more precise meaning. Wikipedia Critique of Big Data Bitermporal Modeling A specific case of Temporal Modeling modeling designed to handle historical data in two different timelines. One timeline is concerned with when an event occurred in the real world and the other timeline is concerned when the data was recorded or corrected in a computer system. This makes it possible to rewind the information to \"as it actually was\" in combination with \"as it was recorded\" at some point in time. In order to implement this feature within an Enterprise Knowledge Graph, the data model must accommodate updates while preserving historical information. Information cannot be overwritten or discarded even if it is erroneous. The consequence is more data must be retained even through only a small percentage of queries might require historical views of data. Bitemporal models are more complex to query and require additional RAM and disk storage. Wikipedia Bitemporal Modeling Brain Analogies Explaining enterprise knowledge graphs in terms of the human brain. Human brains have roughly 82 billion neurons and 10,000 connections for each neuron. This is known as a graph degree of 10,000. Many enterprise knowledge graphs for the largest companies exceed 80 billion vertices but have only a handful of connections between them. See also: Degree . Business Event A change in the state of a business entity within an operational source system that may be published to a downstream consumer such as an enterprise knowledge graph. Business events are usually transmitted by Change Data Capture software and sent via document messages in formats such as JSON or XML. Wikipedia Business Vocabulary A collection of terms and phrases that have meaning to a specific domain of work. A business vocabulary typically starts out with a flat list of terms in a spreadsheet. The terms are listed with their abbreviations and definitions and how they are used within a specific project or department. As vocabularies grow and mature the individual terms might be grouped together. These groupings become taxonomies and can then be used to automatically classify documents with metadata tags of their preferred labels. Classified documents can have a dramatic increase on the search quality of a search engine. Change Data Capture Software that detects changes in a database and transmits the change information via business events to a remote system. These events are often published on stream processing systems using the publish/subscribe integration pattern. Also known as: CDC Wikipedia page on Change Data Capture Classification and Connection Within the data enrichment pattern there are two distinct phases. The first step is to take raw binary and numeric data streams and classify it according to the concept types in our knowledge graph. Once we have business entities identified we next need to connect our business entities together using the context around them in the data. These steps are called classification and connection. See also: Knowledge Triangle Codifiable Knowledge Knowledge that can be captured in machine readable formats such as taxonomies, ontologies, rules or decision trees and reused across an enterprise. Codifiable knowledge is contrasted to Tacit Knowledge that can not easily be converted into machine readable forms and shared with others. The process of converting tacit knowledge to codifiable knowledge is part of the field of Knowledge Engineering and Knowledge Architecture . Within an EKG, our focus is how to model and store codifiable knowledge in terms of graph structures such as taxonomies, ontologies and decision trees. Also known as: Explicit Knowledge Wikipedia page on Explicit Knowledge Cognitive Bias A systematic pattern of deviation from norm or rationality in judgment. In this book we study why humans don't adopt enterprise knowledge graph technology and how we can use stories, demonstrations and economic reasoning to overcome these bias. In this book we study several types of cognitive bias including: Anchoring bias Availability bias a.k.a. memory bias, familiarity heuristic Bandwagon effect Confirmation bias a.k.a. Fiter bubble Halo effect Hindsight bias Illusory superiority bias Framing effect Narrative-bias Representativeness heuristic Status_quo_bias Sunk cost a.k.a. Gamblers fallacy Wikipedia on Cognitive Bias Concept An idea, notion or a unit of thought. Concept elements are the fundamental unit of work in semantics and are in integral part of enterprise knowledge graphs. In practice, each concept is usually associated with a vertex in a graph and has one preferred label in each language such as English. Concepts may have many alternate labels. Concepts are grouped in Schemas and may be part of one or more Collections. Concept Reference on W3C SKOS Site Concept Graph A graph that stores the core business concepts of a project, department or enterprise. In the ideal world, an enterprise graph will use a combination of machine learning to connect related concepts together. Consilience The principle that evidence from independent, unrelated sources can \"converge\" on strong conclusions. Combining results from multiple scientific studies to form stronger conclusions than a single study, often called meta analysis, is an example of consilience. One of the fundamental measures of the value of EKGs is can it promote concilience by linking datasets created from multiple independent data sources. Also known as: Convergence of evidence Also known as: Concordance of evidence Wikipedia page on Consilience Cost Sharing The ability of a single graph data model to be shared by many business units and thus the costs can also be shared. Lower charge backs make graph databases more cost-effective than other data models. See also: No Complexity Penalty Data Discovery A process of discovering new patterns in large data sets using a combination of query tools, machine learning and often visualization. Data discovery is contrasted with operational reporting which are regular consistent reports that are well known to users and good for spotting trends in datasets. Also known as: Data Mining Also known as: Knowledge discovery Wikipedia page on Data Mining Data Ingestion A process by which data is moved from one or more sources to a destination where it can be stored and further analyzed. The data might be in different formats and come from various sources, including RDBMS , other types of databases, S3 buckets, CSVs, or from streams. Data Lake A data storage architectural pattern where data is stored its natural/raw format. Data Lakes are a type of key-value store where the key is a path to a file in a file system. The Data Lake pattern was made popular with the Hadoop project. Because Data Lake are simple they have the advantage that they can be easily implemented by distributed files systems such as Amazon S3 object storage system or the Hadoop Distributed File System (HDFS). Data Lakes are not considered a true database because the usually lack a query language, ACID transactions, indexing, caching, search, semantics, role-based access control. Data Lakes are considered a source of data for EKGs. Data Layer An abstraction layer were low-level raw binary data is stored that contains information after analysis. The Data Layer is the lowest level in the Knowledge Triangle . The format of data in the data layer may be narrative text or raw dumps from a relational database. Dashboard A set of views, usually presented on a single page, that display information as a set of key performance indicators and charts. In general, dashboard views can be customized for a role or a specific user. Datamart A data warehouse used by a project or a department. Decomissioning datamarts is a key cost-driver to the adoption of enterprise knowledge graphs. Decision Tree A way of storing business rules in a graph. A decision tree contains a series of branches, each branch containing a conditional expression. If the conditional expression returns TRUE, then a true link is traversed. If the conditional returns FALSE and false branch is traversed. Decision trees and the corresponding rules that are represented as pointer hops in an enterprise knowledge graph have many integration and performance benefits. Denormalization A strategy used in relational database design to increase performance for a large number of read-operations that access multiple tables using computationally expensive JOIN operations. Denormalization is frequently used in online analytical processing systems . Although denomalization does increase performance, it also imposes a single departments requirements to optimize the reports relevant to their viewpoints. Denomalization destroys the shareability of data models and thus leads to duplication of information and the increase of enterprise costs. Decommissioning departmental datamarts is a common way to justify the costs of building enterprise knowledge graphs. Wikipedia page on Denormalization Degree The degree of a vertex is the count of the number of connections between the vertex and other vertices. The average degree of a graph is the average number of connections for a vertex. For non-directional graphs, counting is one per edge. For directional graphs that have reverse edges, each connection counts as two connections. In a directional graph, each vertex has both an in-degree and out-degree. Departmental Graph A graph designed to store information from one or more departments of an enterprise. Departmental graphs may be limited in that they can't be scaled up to hold enterprise data. DIKW Pyramid A visualization of the relationships between data, information, knowledge, and wisdom with data at the base followed by information, knowledge and finally wisdom at the top. Although commonly used in the field of we don't use the DIKW pyramid visualization in this book because the top wisdom layer is confusing in the context of an enterprise knowledge graph. We used the simpler three layer Knowledge Triangle . Wikipedia page on DIKW Pyramid Document Store A type of database that stores data as tree-structured data elements such as JSON or XML. Document stores use path-like query languages such as X-PATH to traverse the tree structure. Languages such as XQuery provide high-quality functional programming languages with strong type checking. X-PATH is a mature W3C standard for expressing path traversal using a rich array of standardized wildcard expressions. DB Engines A web site that harvests web documents that discuss databases and classifies the documents based on a taxonomy of database types. The \"Popularity changes per category report\" is frequently cited in many graph presentations. DB Engines Edge of Chaos The edge of chaos is narrow band between order and disorder in [complex systems] such as enterprise knowledge graphs. In enterprise knowledge graphs we think of order as areas we have modeled and understand well. We think of disorder as external areas that we have not yet modeled or we have determined that they are not worth the effort to model. Enterprise knowledge graphs modeling teams are often working at the Edge of Choas. Embedding A data structure, usually a vector of decimal numbers, associated with an item in a graph, that helps users quickly find similar items. Vertices, Edges, and Paths may all have embeddings. Endogenic Knowledge The knowledge that is modeled within your existing enterprise knowledge graph. Finding out if your current endogenic knowledge can promote adequate recommendations and predictions is a key strategy in enterprise graph evolution. In contrast, Exogenous Knowledge is the knowledge that is not modeled inside your current knowledge graph. Wikipedia page on Endogeneity Emergence When an entity is observed to have properties its parts do not have on their own, properties or behaviors which emerge only when the parts interact in a wider whole. Emergence is a primary reason to build enterprise knowledge graphs. Emergence allows us to find new insights in data that we could not find without connected data. Unfortunately, there are few ways to predict the rate and value of insights that emerge when we connect new knowledge into an enterprise knowledge graph. The value of emergence can be difficult to predict without a team that has experience with similar prior projects. Wikipedia Page on https://en.wikipedia.org/wiki/Emergence Employee Graph A graph representation of all your organization employees and their activities. For each employee, the graph may contain items such as reporting structure, job titles, roles, work history, education, training, certifications, current skills, security access groups, goals, projects, tasks assigned, helpdesk tickets, bugs assigned, bugs fixed, inventions, desktop hardware, software being used, software licenses, emails, meetings, salary and performance reviews. A detailed employee graph can be used to match available staff with new projects and find similar employees for career mentoring. Employee graphs can also be used to predict the impact of employees that leave an organization, what employees work as ambassadors between groups, and what teams will be the most productive. Due to confidentially reasons, sensitive employee data is often stored in a subgraph with specific [access controls]. Also know as: Human Resources Graph Also know as: Human Capital Graph Enterprise Knowledge Graph A scalable graph database system used to store large-scale connected information for an entire enterprise. By scalable we mean that it must be able to run on multiple servers as the graph expands. Without scalability the graph might be considered a project or departmental graph. For many large organizations, enterprise knowledge graphs typically have hundreds of developers doing concurrent loading and query development and the models can be dynamic. For example the Google Knowledge Graph team is thought to contain over 1,500 developers. Entity Resolution The process of finding records in a data set that refer to the same entity across different data sources. Entity Resolution is a core technique in converting Information layer data into a consistent knowledge graph. Wikipeia Record Linkage Exogenous Knowledge Knowledge \u201ccoming from outside\u201d of your Enterprise Knowledge Graph. Finding out what exogenous knowledge you need to make accurate predictions is an emerging area of enterprise knowledge graphs. In economic modeling, exogenous events means an influence that arises from outside the scope of your model and that is, therefore, neither predicted nor explained by the model. In contrast, Endogenic Knowledge is the knowledge that is modeled within your enterprise knowledge graph. Wikipedia page on Exogeny Force Directed Graph A graph layout algorithm that simulates forces on springs that move items Wikipedia Force Directed Graph Drawing Four Vs of Scalable Databases Volume, velocity, variability and veracity are considered the four Vs that define scalable systems. Volume refers to the total amount of data in our knowledge graph. Velocity means that new inserts, updates and deletes might be coming in fast via streaming events and these events must be ACID compliant and still never slow down read access times. Service levels agreements (SLAs) must focus not on total average times, but the averages of the slowest 5% of the transactions. Variability means that data is not uniform and can be easily stuffed into a single fact table of an OLAP cube. Veracity means we need to be able to validate the quality of incoming data in real-time and quickly raise warning flags if corrupt data is being transmitted into the EKG. Glossary A business vocabulary associated with a topic. A glossary often has both general definitions of terms as well as contextual definitions for a specific domain or project. See also: Business Vocabulary GraphQL A query language for APIs and a runtime for fulfilling those queries with your existing data. Ironically, GraphGL has nothing to do with graph databases other than the fact that the queries often run much faster on graphs. The name \"graph\" was used internally at FaceBook since they store their data in a graph structure. One concern about GraphQL at the enterprise-scale is that your graph database should be able to detect GraphQL queries that are using too many resources. This means your enterprise graph databases must understand concepts of resource quotas . Graph Query Language A proposed standard graph query language being developed by the Working Group 3 (Database Languages) of ISO/IEC JTC 1's Subcommittee 32. GQL is designed to work with Labeled Property Graphs . Wikipedia GQL Page Graph Structured Query Language A distributed graph query language developed by TigerGraph. GSQL was designed to be syntactically similar to the SQL language but it also integrated distributed query concepts that share patterns similar to MapReduce queries. Also known as: GSQL Graph Database A way of storing information in terms of vertices and edges. Graph databases consider edge traversal as a primary performance consideration. By storing edges as in-memory pointers graph databases offer roughly a 1,000x performance improvement over relational database management JOIN operations that must be calculated for each query. See also: Index Free Adjacency Graph Isomorphism A graph can exist in different forms having the same number of vertices, edges, and also the same edge connectivity. Such graphs are called isomorphic graphs. Hadoop Distributed File System A high-availability distributed key-value store that was a principal component of the Hadoop system. HDFS was one of the two components of the original Hadoop system. The other component was the MapReduce system. Also known as: HDFS Hedgehog vs Fox Modeling Focus on accurate modeling a single domain or subgraph of an enterprise knowledge graph (the Hedgehog) vs general modeling of a wide variety of subgraphs or domains. The term comes from Archilochus who stated \"a fox knows many things, but a hedgehog knows one important thing\". There are pros and cons for taking different approaches. No single strategy will work for all enterprise knowledge graphs at all times. The involvement of subject-matter experts (hedgehogs) at different times in the lifecycle of an enterprise knowledge graph will impact the evolution of enterprise knowledge graphs. Higher Order Knowledge A height-related metaphor that is used to describe more abstract knowledge that is more universal in an enterprise knowledge graph. The concept of \"height\" is related to the layers of the Knowledge Triange . For example, the idea behind \"higher-order thinking\" is that some types of learning requires more cognitive processing than others, but also have more generalized benefits. Within knowledge graphs this may not translate into more CPU time for query traversal, but may depend on having more abstract vertices and edges in an upper or mid-range ontology. Wikipedia page on Higher Order Thinking Index Free Adjacency Accessing related entities in a system without having to consult a centralized index. Using direct in-memory pointers to represent relationships is approximately three orders of magnitude faster than referencing a central index system. See also: The Neighborhood Walk Story Information Layer Data about our key business entities. This includes Things, like People, Places and Events. Inmon Data Warehouse The Inmon Data Warehouse is a collection of database design patterns that promote analytics using relational databases promoted by Bill Inmon. The Inomn approach was first enumerated in his 1992 book \"Building the Data Warehouse\". The Inmon approach is usually contrasted to the more recent 2013 Kimball Data Warehouse that focuses on a simplicity and single fact table with many dimensions. Many EKG projects can be funded by their ability to show they can decommission expensive Inmon-style data warehouses that don't have the flexibility of EKGs. Wikipedia page on Bill Inmon Key-Value Store A type of database that stores items as pairs of keys and values. The keys are strings and the values are binary blobs such as files or images. A simple put/get/delete interface is used to manage the database. Key-value stores are excellent complements to graph databases since their simplicity allows for low-cost-per-byte storage. Kimball Data Warehouse A data warehouse design pattern that uses a single fact table joined with dimensional tables to minimize the impact of JOIN statements in reporting performance. Kimball data warehouses are the ultimate in denormalized database design . Their goal is often simplicity at the expense of capturing complex relationship-intensive models of the world that can be reused across the enterprise. As a result, Kimball datamarts duplicate data in each department and each datamart has their own costs to perform ETL operations. In contrast, EKGs focus on highly [normalized] data models of the world that include many complex relationships. This closer our models get to the real world the more they can be reused across many departments. Decomissioning many departmental datamarts is often a key way to justify EKG projects. Wikipedia Page on Ralph Kimball Knowledge Layer A layer in the knowledge triangle that contains connected information. The knowledge layer is often the top layer in our views. There are some views that include a Wisdom layer on top of the knowledge layer. Knowledge Graph A set of interconnected typed entities and their attributes. Entities can be any business objects, customers, products, parts, documents, employees or concepts. Entities are usually implemented as vertices in a graph database and connected through edges. In some types of graphs, for example LPGs , edges also have attributes. Note that this definition has no dependence on semantics and inference. Our definition is intentionally designed to include many types of interconnected datasets. We think your organizational chart is a type of knowledge graph an may be a subgraph of your enterprise knowledge graph. Knowledge Management A multi-disciplinary field that include processes of creating, sharing, using and managing the knowledge and information of an organization. Knowledge Management addresses question such as what is organizational knowledge, what types of knowledge are there ( Tacit vs. Codifiable ), how do we encourage employees to capture and store knowledge is forms that can be reused, how do we motivate employees to link isolated knowledge islands together, how do we make tacit Knowledge more codifiable, searchable and reusable. In this book, we take the approach that EKGs should be part of an organizations overall [Enterprise Knowledge Management] strategy and that individuals with formal training in the field of [Knowledge Engineering] should participate in the creation of EKGs and particpate in their evolution. Wikipedia page on Knowledge Management Knowledge Representation The process of representing information (individual facts) about the world in a form that a computer system can utilize to solve complex tasks. Within the context of the enterprise knowledge graph, we used graph databases as our primary way to store knowledge and we complement graph databases with search engines and key-value stores when they are more efficient. There is no single knowledge reprenstation that is ideal for all problems. Graph database are the preferred way to store knowledge because efficient reasoning can be implemented as fast pointer-hopping operations that can be optimized by specialized hardware. Knowledge representation is often the most complex challenge in the field of Artificial Intelligence. Wikipedia Page on Knowledge Representation Knowledge Triangle A stack of three layers that illustrates how knowledge graphs are constructed from raw data. At the base is the Data Layer that stores raw binary data in numeric forms, above that is Information Layer that finds concepts and business entities within the data layer. At the top the triangle is the Knowledge Layer where business entities are connected to make them easy to query using graph traversal algorithms. See also: The DIKW Pyramid Narrative Level The first stage in converting a rule from natural language, such as English, into an executable rule in a knowledge graph or other system. A narrative rule may contain the following elements: Source of rule (authority organization) Area of focus (domain) Scope Context or setting Recommendations or action to be taken The population of items that are included The population of items that ar excluded Policy considerations (e.g. privacy, access, regulations) Narrative levels may contain words or phrases that link to concepts in a Concept Graph. See Levels of Knowledge Label A string associated with one or more Concepts . Labels have two main types: Preferred Labels and Alternate Labels . Most formal ontologies and taxonomies only permit a single preferred label for each Concept in a given Language. Labeled Property Graph A graph data model where each Vertex and Edge have a single type and goth Vertices and Edges have attributes. Both TigerGraph and Neo4j use the LPG data model. Also known as: LPG Load-As-Is Pattern A data loading pattern that loads the data into a graph with minimal transformation. Once the data is loaded into the graph the transformation is done in the native language of the graph such as GSQL. This pattern allows many projects to share the underlying data loaders and allows each team to customize the post-loading transformation using the native query language of the graph. The other major data model is the RDF model which is discouraged at Optum due to the challenges with Reification. Reification causes RDF SPARQL queries to be rewritten. Load-as-is pattern See also: RDF The Neighborhood Walk Story A story used to illustrate the difference between direct pointer hopping and using centralized indexes to traverse relationships. The story uses a 30-second walk between two houses vs. an 8.2-hour walk to a central location and back. No Complexity Penalty Unlike relational databases, graph databases quickly traverse many complex relationships. As a result, graph databases are better at modeling the real world - which is full of complexity. We use the phrase \"No Complexity Penalty\" every time we are training people who have come from the relational world that worry that too many relationships will slow down their queries due to slow JOINs. See also: One version of the truth One Version of the Truth The real world has many complex relationships. There are many ways to build simple models that take shortcuts to optimize queries by limiting relationships. This is important in relational database modeling. But the closer we get to modeling the real world, the closer to a single version of the truth we get. Models that fairly represent the complexities of the real world can be reused among many business units and thus the costs of holding the information in memory can be shared. This is why graph databases cost less then relational databases. Online Analytical Processing System An approach to answer multi-dimensional analytical queries quickly by minimizing JOIN operations in relational databases. OLAP \"cubes\" often use a star schema with a central fact table and one JOIN operation per dimension of the cube. The denomalization process used to create star schemas limits enterprise-sharing of these structures. Wikipedia On-the-Wire vs. In-the-Can A way of looking at knowledge representation requirements in two domains. On-The-Wire implies that serialization of a dataset must retain connection information within itself and to other external systems. In-The-Can knowledge representations are optimized for ease of query and sustainability . RDF is optimized for On-The-Wire exchange of knowledge. LPG is optimized for In-The-Can tasks such as ease of query and sustainability. Ontology A graph of Concepts within a specific domain. Ontologies often begin as flat term lists, that become taxonomies that then have more complex relationships than simple broader and narrower concepts. Ontologies are often stored in formats such as SKOS and OWL. Open vs Closed World https://en.wikipedia.org/wiki/Open-world_assumption https://en.wikipedia.org/wiki/Closed-world_assumption Operational Source System A transactional computer that is the source of a data stream. Enterprise Graph Databases often use Change Data Capture software on these systems to create an event stream of change records that so they can be stored in a central enterprise knowledge graph. Change records are new, updated, or deleted business entities. PageRank A graph algorithm that is used to rank the most influential vertices in a directed graph. For example web pages in a graph of linked web pages. PageRank was first used by Google Search to rank web pages in their search engine results. The patent for PageRank (now expired) was purchased by Google from Standford University for Google shares. Those shares sold for over $336 million USD when Google went public. Wikipedia Preferred Label A preferred lexical label associated with a Concept . In the SKOS standard, there should be one and only one preferred label per language per concept. Project Graph A graph that supports a specific project. Project graphs may contain knowledge that is not of interest to the rest of the enterprise. Reference Data Reference data is data used to classify or categorize other data. They typically are stored as a set of valid codes for a specific data element. For example the list of Country Codes is a type of reference data. Reference data is often stored as a short code and a definition of what that code represents. Reification Reification is the process by which an abstract idea about a computer program is turned into an explicit data model or other object created in a programming language. Specifically, in the RDF modeling process it is the process of adding an abstract vertex to a graph when properties are needed in a relationship. Reification causes queries that traverse that node to be rewritten. This means that SPARQL queries are inherently much more difficult to maintain than LPG graph queries. Resource Description Framework An early family of standards developed by the World Wide Web Consortium for exchanging graph data championed by the Semantic Web community starting in 1999. RDF gained some traction around 2010 but failed to gain widespread adoption due to the complexity of the standards and the problems of Reification . Wikipedia Resource Quota The ability to limit the resources consumed by a query such as CPU time, or RAM for individuals or groups. Large enterprise-scale graph databases must carefully monitor and constrain queries that consume too many resources. Many older technologies such as Apache Drill are difficult to implement without the ability to monitor and restrict resources. Role-based Access Control The ability to assign access to a resource to individuals that have a specific role. For Enterprise Knowledge Graphs, there are both high-level subgraph rules and fine-grain rules such as vertex-related role-based access control. Rules Engine A software component that executes rules according to some algorithm. In the Enterprise Knowledge Graph space rules are frequently represented in Decision Tree structures within the graph. [Rules for Knowledge Graph Rules])https://dmccreary.medium.com/rules-for-knowledge-graphs-rules-f22587307a8f Semantics The branch of computer science associated with meaning. It can be best understood by understanding the semantic triangle. The key point of the semantic triangle is that we cannot directly associate a label with a referent without traversing concepts. Semantic Graph A graph where each vertex represents a Concept and the edges of the graph represent the relationships between the Concepts. The primary data model for storing semantic graphs is the SKOS data model where Concepts and Labels are distinct types. Shapes Constraint Language A W3C standard RDF vocabulary for validating RDF graphs against a set of conditions. Unlike document validation standards like XML Schema, SHACL assumes that data quality checks should be able to look for relationships in a graph as well as the local context of a document. These conditions are provided as shapes and other constructs expressed in the form of an RDF graph. LPG graphs do not yet have a version of SHACL. Also known as: SHACL SHACL W3C Simple Knowledge Organizational System A model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabularies. SKOS is also the name of the world-wide-web standard for encoding these systems. Serializations of SKOS are typically done in RDF format although other encodings such as XML and JSON are common. See Also: W3C SKOS Primer See Also: W2C SKOS Referecnce Strategy Graph A graph that is used to help determine what strategies might be optimal for an enterprise or a subgroup as well as how organizations are performing on a specific strategy. To be successful, enterprise and departmental strategies must be encoded in machine-readable forms such as StratML and loaded into an enterprise knowledge graph. Strategy graphs can also be used to determine the alignment of proposed projects for the future in an organization. StratML encoding is sometimes required of US federal organizations so that strategies can be analyzed by published public documents. Strategic Serendipity Building a enterprise strategy around the creation of an environment where it is easier to make unexpected connections between items. Strategic serendipity involves getting a large number of people ready to discover new things in an enterprise knowledge graph. Subgraph A subset of an enterprise knowledge graph that may store specific types of knowledge and may have specific access control rules based on the role of a user. For example, a business glossary, taxonomy, ontology or business rules system that contains no customer-specific information may be in one subgraph and have universal read-access for all users. Customer-specific data that is highly confidential may be stored in a different subgraph, with read access only granted on a need-to-know basis. See also: Role Based Access Control Sustainability The ability for a team of developers to maintain the code that supports an enterprise knowledge graph in the face of changes to the data model. The key measure is to avoid problems related to rewriting graph queries when small changes are made to the graph data model. See also: The Jenga Tower Story Systems Thinking A way of looking at problems in terms of components that interact with each other over time using direct connections, indirection connections and both positive and negative feedback cycles. Systems Thinking forces us to think broadly about how our enterprise knowledge graphs interact with external systems. Systems thinking also helps us see the unintended consequences of our actions. Tacit Knowledge A type of knowledge that is difficult to codify in terms of a machine readable artifacts that can be reused across an enterprise. Tacit knowledge is difficult to transfer from one person to another person by means of writing or verbalizing. Understanding what tacit knowledge is developing strategies to convert it to codifiable knowledge is a key areas of research in the fields of Knowledge Management and Knowledge Engineering . See also: Codifiable Knowledge Wikipedia Page on Tacit Knowledge TBox A \"terminological component\" or terminology Concept associated with a set of facts assertions (ABox statements) of a knowledge graph. TBox statements tend to more rules or metarules (rules about rules) that individual fact about customers or other business entities. See also: ABox Wikipedia Tbox page Technology Adoption Life Cycle A sociological model that describes the adoption or acceptance of a new product or innovation, according to the demographic and psychological characteristics of defined adopter groups. Wikipedia Temporal Modeling The process of modeling time in a data model. Modeling time can be complex when the requirements of a system require you to be able to recreate detailed reports as they were at a prior point in time. Temporal modeling includes the concept of versioning and bitemporal modeling Triple Store A purpose-built database for the storage and retrieval of RDF triples through semantic queries. Triple stores are not used in most enterprise graphs due to their lack of sustanability due to problems with Reification . https://en.wikipedia.org/wiki/Triplestore Upper Ontology General high-level Concepts that are common across all domains in a knowledge graph. Concepts such as Organization, Customer, Family Unit, Product, Part, Invoice, Document are often consider part of an upper ontology. Enterprise graphs may contain multiple ontologies and the ease of linking ontologies in highly dependant on sharing upper ontologies. Wikipedia Page on Upper Ontology Web Ontology Language A Semantic Web language designed to represent rich and complex knowledge about things, groups of things, and relations between things. W3C OWL Web Site Window of Opportunity A narrow band of time that an organization might be ready to adopt a new technology such as an enterprise knowledge graph. To arrive at the Window of Opportunity to adapt an enterprise knowledge graph an organization must meet a set of preconditions such as an internal chamption, a shared understanding of what enterprise knowledge graphs are capable of, and the ability to ingest enough information to achieve specific business objectives. Enterprise knowledge graphs then have a limited time to create a pilot project before funding runs out, champions move on, or alternative technologies get mindshare. Very often a specific chrisis can trigger an unexpcted window of opportunity. Knowing how to recognize these windows and take advantage of them is a key skill we attempt to explore in this book. Wikipedia article on Window of Opportunity","title":"Glossary"},{"location":"glossary/#enterprise-knowledge-graph-glossary-of-terms","text":"","title":"Enterprise Knowledge Graph Glossary of Terms"},{"location":"glossary/#abox","text":"Types of assertions or statements in a knowledge graph that conform to specific terminologies (knows as TBox statements). The terms ABox and TBox are used to help determine if a statement is universal or related to a specific Subgraph of an Enterprises Knowledge Graph . Within our Enterprise Knowledge Graph architecture, ABox statements often contain knowledge about specific customers, parts, or concepts and may have specific Access Control rules. TBox statements don't usually have these same access rules. See also: TBox Wikipedia page on ABox","title":"ABox"},{"location":"glossary/#accumulator","text":"A type of variable that tracks items as you traverse through a graph. Accumulators can be global or be attached to a specific vertex. Accumulators allow MapReduce style queries where each server node in a cluster does work in its local data and returns consolidated results to the query node. For example, in the query \"count all customers that have returned clothing items\" , each node would return only a single count to the query node. This type of query reduces the amount of communication between nodes in the graph cluster.","title":"Accumulator"},{"location":"glossary/#alternate-label","text":"In a concept subgraph of a EKG, every concept has a single preferred label (per language) and many alternate labels that can also be used to name a concept. Alternate labels can be abbreviations, acronyms and synonyms of the concept. In this glossary, alternate labels have a \"Also known as\" prefix. Also known as: atlLabel See also: Preferred Label See also: SKOS","title":"Alternate Label"},{"location":"glossary/#amazon-simple-storage-system","text":"A high-availability cloud-based Key-value Store system created by Amazon in 2006. In 2006 Amazon was a pioneer in the creation of cloud-based data services. Because S3 was one of the first ultra-reliable cloud based services it gained early market share and now dominates the data storage industry. The API for S3 is now duplicated by dozens of other vendors and has become a defacto standard for cloud-based key-value stores. S3 was the first service launched as part of Amazon Web Services (AWS). Also known as: S3 Wikipedia page on Amazon S3","title":"Amazon Simple Storage System"},{"location":"glossary/#availability-heuristic","text":"The availability heuristic suggests that the likelihood of events is estimated based on how many examples of such events come to mind. For example, when a solution architect is presented with a business problem, their likelihood to recommend an enterprise knowledge graph solution is dependant on how many examples of successful enterprise knowledge graph project come to mind. Surrounding solution architects with many successful stories or case studies of successful enterprise knowledge graph projects may positively impact their probability of recomending an enterprise knowledge graph as an option. See also: Cognitive Bias Wikipedia Case Study","title":"Availability Heuristic"},{"location":"glossary/#a-priori-and-a-posteriori","text":"A priori knowledge is that which is independent of experience. A posteriori knowledge is that which depends on empirical evidence. The rules of mathematics, logic, and business rules are usually classified as a priori read-access is usually shared across all users of an enterprise knowledge graph. Knowledge about a specific observable event, observation, customer transaction, etc. are considered A Posteriori and may not need to be universally accessed. In an enterprise knowledge graph, a priori knowledge tends to be more universal such as TBox assertions that are universally accessible by all subgraphs . https://en.wikipedia.org/wiki/A_priori_and_a_posteriori","title":"A Priori and A Posteriori"},{"location":"glossary/#automatic-sharding","text":"The process of automatically migrating data from one server to another server in a distributed database. Auto-sharding is frequently done as a database cluster grows or shrinks based on new data being added or removed from the cluster. Auto-sharding is one of the key features that differentiate scale-out enterprise-class databases from departmental solutions. Testing sharding at scale under continuous load in the face of possible hardware failure is one of the key challenges facing enterprise data architects.","title":"Automatic Sharding"},{"location":"glossary/#big-data","text":"An ambiguous term that many or may not refer to data sizes beyond the ability of commonly used tools to mange data. If you used a spreadsheet, \"Big Data\" could be any data that does not fit into your spreadsheet. If you use a Cray Supercomputer then your definition of Big Data could differ by ten orders of magnitude. If we ever hear someone refer to \"Big Data\" we strongly suggest they use other terms that have more precise meaning. Wikipedia Critique of Big Data","title":"Big Data"},{"location":"glossary/#bitermporal-modeling","text":"A specific case of Temporal Modeling modeling designed to handle historical data in two different timelines. One timeline is concerned with when an event occurred in the real world and the other timeline is concerned when the data was recorded or corrected in a computer system. This makes it possible to rewind the information to \"as it actually was\" in combination with \"as it was recorded\" at some point in time. In order to implement this feature within an Enterprise Knowledge Graph, the data model must accommodate updates while preserving historical information. Information cannot be overwritten or discarded even if it is erroneous. The consequence is more data must be retained even through only a small percentage of queries might require historical views of data. Bitemporal models are more complex to query and require additional RAM and disk storage. Wikipedia Bitemporal Modeling","title":"Bitermporal Modeling"},{"location":"glossary/#brain-analogies","text":"Explaining enterprise knowledge graphs in terms of the human brain. Human brains have roughly 82 billion neurons and 10,000 connections for each neuron. This is known as a graph degree of 10,000. Many enterprise knowledge graphs for the largest companies exceed 80 billion vertices but have only a handful of connections between them. See also: Degree .","title":"Brain Analogies"},{"location":"glossary/#business-event","text":"A change in the state of a business entity within an operational source system that may be published to a downstream consumer such as an enterprise knowledge graph. Business events are usually transmitted by Change Data Capture software and sent via document messages in formats such as JSON or XML. Wikipedia","title":"Business Event"},{"location":"glossary/#business-vocabulary","text":"A collection of terms and phrases that have meaning to a specific domain of work. A business vocabulary typically starts out with a flat list of terms in a spreadsheet. The terms are listed with their abbreviations and definitions and how they are used within a specific project or department. As vocabularies grow and mature the individual terms might be grouped together. These groupings become taxonomies and can then be used to automatically classify documents with metadata tags of their preferred labels. Classified documents can have a dramatic increase on the search quality of a search engine.","title":"Business Vocabulary"},{"location":"glossary/#change-data-capture","text":"Software that detects changes in a database and transmits the change information via business events to a remote system. These events are often published on stream processing systems using the publish/subscribe integration pattern. Also known as: CDC Wikipedia page on Change Data Capture","title":"Change Data Capture"},{"location":"glossary/#classification-and-connection","text":"Within the data enrichment pattern there are two distinct phases. The first step is to take raw binary and numeric data streams and classify it according to the concept types in our knowledge graph. Once we have business entities identified we next need to connect our business entities together using the context around them in the data. These steps are called classification and connection. See also: Knowledge Triangle","title":"Classification and Connection"},{"location":"glossary/#codifiable-knowledge","text":"Knowledge that can be captured in machine readable formats such as taxonomies, ontologies, rules or decision trees and reused across an enterprise. Codifiable knowledge is contrasted to Tacit Knowledge that can not easily be converted into machine readable forms and shared with others. The process of converting tacit knowledge to codifiable knowledge is part of the field of Knowledge Engineering and Knowledge Architecture . Within an EKG, our focus is how to model and store codifiable knowledge in terms of graph structures such as taxonomies, ontologies and decision trees. Also known as: Explicit Knowledge Wikipedia page on Explicit Knowledge","title":"Codifiable Knowledge"},{"location":"glossary/#cognitive-bias","text":"A systematic pattern of deviation from norm or rationality in judgment. In this book we study why humans don't adopt enterprise knowledge graph technology and how we can use stories, demonstrations and economic reasoning to overcome these bias. In this book we study several types of cognitive bias including: Anchoring bias Availability bias a.k.a. memory bias, familiarity heuristic Bandwagon effect Confirmation bias a.k.a. Fiter bubble Halo effect Hindsight bias Illusory superiority bias Framing effect Narrative-bias Representativeness heuristic Status_quo_bias Sunk cost a.k.a. Gamblers fallacy Wikipedia on Cognitive Bias","title":"Cognitive Bias"},{"location":"glossary/#concept","text":"An idea, notion or a unit of thought. Concept elements are the fundamental unit of work in semantics and are in integral part of enterprise knowledge graphs. In practice, each concept is usually associated with a vertex in a graph and has one preferred label in each language such as English. Concepts may have many alternate labels. Concepts are grouped in Schemas and may be part of one or more Collections. Concept Reference on W3C SKOS Site","title":"Concept"},{"location":"glossary/#concept-graph","text":"A graph that stores the core business concepts of a project, department or enterprise. In the ideal world, an enterprise graph will use a combination of machine learning to connect related concepts together.","title":"Concept Graph"},{"location":"glossary/#consilience","text":"The principle that evidence from independent, unrelated sources can \"converge\" on strong conclusions. Combining results from multiple scientific studies to form stronger conclusions than a single study, often called meta analysis, is an example of consilience. One of the fundamental measures of the value of EKGs is can it promote concilience by linking datasets created from multiple independent data sources. Also known as: Convergence of evidence Also known as: Concordance of evidence Wikipedia page on Consilience","title":"Consilience"},{"location":"glossary/#cost-sharing","text":"The ability of a single graph data model to be shared by many business units and thus the costs can also be shared. Lower charge backs make graph databases more cost-effective than other data models. See also: No Complexity Penalty","title":"Cost Sharing"},{"location":"glossary/#data-discovery","text":"A process of discovering new patterns in large data sets using a combination of query tools, machine learning and often visualization. Data discovery is contrasted with operational reporting which are regular consistent reports that are well known to users and good for spotting trends in datasets. Also known as: Data Mining Also known as: Knowledge discovery Wikipedia page on Data Mining","title":"Data Discovery"},{"location":"glossary/#data-ingestion","text":"A process by which data is moved from one or more sources to a destination where it can be stored and further analyzed. The data might be in different formats and come from various sources, including RDBMS , other types of databases, S3 buckets, CSVs, or from streams.","title":"Data Ingestion"},{"location":"glossary/#data-lake","text":"A data storage architectural pattern where data is stored its natural/raw format. Data Lakes are a type of key-value store where the key is a path to a file in a file system. The Data Lake pattern was made popular with the Hadoop project. Because Data Lake are simple they have the advantage that they can be easily implemented by distributed files systems such as Amazon S3 object storage system or the Hadoop Distributed File System (HDFS). Data Lakes are not considered a true database because the usually lack a query language, ACID transactions, indexing, caching, search, semantics, role-based access control. Data Lakes are considered a source of data for EKGs.","title":"Data Lake"},{"location":"glossary/#data-layer","text":"An abstraction layer were low-level raw binary data is stored that contains information after analysis. The Data Layer is the lowest level in the Knowledge Triangle . The format of data in the data layer may be narrative text or raw dumps from a relational database.","title":"Data Layer"},{"location":"glossary/#dashboard","text":"A set of views, usually presented on a single page, that display information as a set of key performance indicators and charts. In general, dashboard views can be customized for a role or a specific user.","title":"Dashboard"},{"location":"glossary/#datamart","text":"A data warehouse used by a project or a department. Decomissioning datamarts is a key cost-driver to the adoption of enterprise knowledge graphs.","title":"Datamart"},{"location":"glossary/#decision-tree","text":"A way of storing business rules in a graph. A decision tree contains a series of branches, each branch containing a conditional expression. If the conditional expression returns TRUE, then a true link is traversed. If the conditional returns FALSE and false branch is traversed. Decision trees and the corresponding rules that are represented as pointer hops in an enterprise knowledge graph have many integration and performance benefits.","title":"Decision Tree"},{"location":"glossary/#denormalization","text":"A strategy used in relational database design to increase performance for a large number of read-operations that access multiple tables using computationally expensive JOIN operations. Denormalization is frequently used in online analytical processing systems . Although denomalization does increase performance, it also imposes a single departments requirements to optimize the reports relevant to their viewpoints. Denomalization destroys the shareability of data models and thus leads to duplication of information and the increase of enterprise costs. Decommissioning departmental datamarts is a common way to justify the costs of building enterprise knowledge graphs. Wikipedia page on Denormalization","title":"Denormalization"},{"location":"glossary/#degree","text":"The degree of a vertex is the count of the number of connections between the vertex and other vertices. The average degree of a graph is the average number of connections for a vertex. For non-directional graphs, counting is one per edge. For directional graphs that have reverse edges, each connection counts as two connections. In a directional graph, each vertex has both an in-degree and out-degree.","title":"Degree"},{"location":"glossary/#departmental-graph","text":"A graph designed to store information from one or more departments of an enterprise. Departmental graphs may be limited in that they can't be scaled up to hold enterprise data.","title":"Departmental Graph"},{"location":"glossary/#dikw-pyramid","text":"A visualization of the relationships between data, information, knowledge, and wisdom with data at the base followed by information, knowledge and finally wisdom at the top. Although commonly used in the field of we don't use the DIKW pyramid visualization in this book because the top wisdom layer is confusing in the context of an enterprise knowledge graph. We used the simpler three layer Knowledge Triangle . Wikipedia page on DIKW Pyramid","title":"DIKW Pyramid"},{"location":"glossary/#document-store","text":"A type of database that stores data as tree-structured data elements such as JSON or XML. Document stores use path-like query languages such as X-PATH to traverse the tree structure. Languages such as XQuery provide high-quality functional programming languages with strong type checking. X-PATH is a mature W3C standard for expressing path traversal using a rich array of standardized wildcard expressions.","title":"Document Store"},{"location":"glossary/#db-engines","text":"A web site that harvests web documents that discuss databases and classifies the documents based on a taxonomy of database types. The \"Popularity changes per category report\" is frequently cited in many graph presentations. DB Engines","title":"DB Engines"},{"location":"glossary/#edge-of-chaos","text":"The edge of chaos is narrow band between order and disorder in [complex systems] such as enterprise knowledge graphs. In enterprise knowledge graphs we think of order as areas we have modeled and understand well. We think of disorder as external areas that we have not yet modeled or we have determined that they are not worth the effort to model. Enterprise knowledge graphs modeling teams are often working at the Edge of Choas.","title":"Edge of Chaos"},{"location":"glossary/#embedding","text":"A data structure, usually a vector of decimal numbers, associated with an item in a graph, that helps users quickly find similar items. Vertices, Edges, and Paths may all have embeddings.","title":"Embedding"},{"location":"glossary/#endogenic-knowledge","text":"The knowledge that is modeled within your existing enterprise knowledge graph. Finding out if your current endogenic knowledge can promote adequate recommendations and predictions is a key strategy in enterprise graph evolution. In contrast, Exogenous Knowledge is the knowledge that is not modeled inside your current knowledge graph. Wikipedia page on Endogeneity","title":"Endogenic Knowledge"},{"location":"glossary/#emergence","text":"When an entity is observed to have properties its parts do not have on their own, properties or behaviors which emerge only when the parts interact in a wider whole. Emergence is a primary reason to build enterprise knowledge graphs. Emergence allows us to find new insights in data that we could not find without connected data. Unfortunately, there are few ways to predict the rate and value of insights that emerge when we connect new knowledge into an enterprise knowledge graph. The value of emergence can be difficult to predict without a team that has experience with similar prior projects. Wikipedia Page on https://en.wikipedia.org/wiki/Emergence","title":"Emergence"},{"location":"glossary/#employee-graph","text":"A graph representation of all your organization employees and their activities. For each employee, the graph may contain items such as reporting structure, job titles, roles, work history, education, training, certifications, current skills, security access groups, goals, projects, tasks assigned, helpdesk tickets, bugs assigned, bugs fixed, inventions, desktop hardware, software being used, software licenses, emails, meetings, salary and performance reviews. A detailed employee graph can be used to match available staff with new projects and find similar employees for career mentoring. Employee graphs can also be used to predict the impact of employees that leave an organization, what employees work as ambassadors between groups, and what teams will be the most productive. Due to confidentially reasons, sensitive employee data is often stored in a subgraph with specific [access controls]. Also know as: Human Resources Graph Also know as: Human Capital Graph","title":"Employee Graph"},{"location":"glossary/#enterprise-knowledge-graph","text":"A scalable graph database system used to store large-scale connected information for an entire enterprise. By scalable we mean that it must be able to run on multiple servers as the graph expands. Without scalability the graph might be considered a project or departmental graph. For many large organizations, enterprise knowledge graphs typically have hundreds of developers doing concurrent loading and query development and the models can be dynamic. For example the Google Knowledge Graph team is thought to contain over 1,500 developers.","title":"Enterprise Knowledge Graph"},{"location":"glossary/#entity-resolution","text":"The process of finding records in a data set that refer to the same entity across different data sources. Entity Resolution is a core technique in converting Information layer data into a consistent knowledge graph. Wikipeia Record Linkage","title":"Entity Resolution"},{"location":"glossary/#exogenous-knowledge","text":"Knowledge \u201ccoming from outside\u201d of your Enterprise Knowledge Graph. Finding out what exogenous knowledge you need to make accurate predictions is an emerging area of enterprise knowledge graphs. In economic modeling, exogenous events means an influence that arises from outside the scope of your model and that is, therefore, neither predicted nor explained by the model. In contrast, Endogenic Knowledge is the knowledge that is modeled within your enterprise knowledge graph. Wikipedia page on Exogeny","title":"Exogenous Knowledge"},{"location":"glossary/#force-directed-graph","text":"A graph layout algorithm that simulates forces on springs that move items Wikipedia Force Directed Graph Drawing","title":"Force Directed Graph"},{"location":"glossary/#four-vs-of-scalable-databases","text":"Volume, velocity, variability and veracity are considered the four Vs that define scalable systems. Volume refers to the total amount of data in our knowledge graph. Velocity means that new inserts, updates and deletes might be coming in fast via streaming events and these events must be ACID compliant and still never slow down read access times. Service levels agreements (SLAs) must focus not on total average times, but the averages of the slowest 5% of the transactions. Variability means that data is not uniform and can be easily stuffed into a single fact table of an OLAP cube. Veracity means we need to be able to validate the quality of incoming data in real-time and quickly raise warning flags if corrupt data is being transmitted into the EKG.","title":"Four Vs of Scalable Databases"},{"location":"glossary/#glossary","text":"A business vocabulary associated with a topic. A glossary often has both general definitions of terms as well as contextual definitions for a specific domain or project. See also: Business Vocabulary","title":"Glossary"},{"location":"glossary/#graphql","text":"A query language for APIs and a runtime for fulfilling those queries with your existing data. Ironically, GraphGL has nothing to do with graph databases other than the fact that the queries often run much faster on graphs. The name \"graph\" was used internally at FaceBook since they store their data in a graph structure. One concern about GraphQL at the enterprise-scale is that your graph database should be able to detect GraphQL queries that are using too many resources. This means your enterprise graph databases must understand concepts of resource quotas .","title":"GraphQL"},{"location":"glossary/#graph-query-language","text":"A proposed standard graph query language being developed by the Working Group 3 (Database Languages) of ISO/IEC JTC 1's Subcommittee 32. GQL is designed to work with Labeled Property Graphs . Wikipedia GQL Page","title":"Graph Query Language"},{"location":"glossary/#graph-structured-query-language","text":"A distributed graph query language developed by TigerGraph. GSQL was designed to be syntactically similar to the SQL language but it also integrated distributed query concepts that share patterns similar to MapReduce queries. Also known as: GSQL","title":"Graph Structured Query Language"},{"location":"glossary/#graph-database","text":"A way of storing information in terms of vertices and edges. Graph databases consider edge traversal as a primary performance consideration. By storing edges as in-memory pointers graph databases offer roughly a 1,000x performance improvement over relational database management JOIN operations that must be calculated for each query. See also: Index Free Adjacency","title":"Graph Database"},{"location":"glossary/#graph-isomorphism","text":"A graph can exist in different forms having the same number of vertices, edges, and also the same edge connectivity. Such graphs are called isomorphic graphs.","title":"Graph Isomorphism"},{"location":"glossary/#hadoop-distributed-file-system","text":"A high-availability distributed key-value store that was a principal component of the Hadoop system. HDFS was one of the two components of the original Hadoop system. The other component was the MapReduce system. Also known as: HDFS","title":"Hadoop Distributed File System"},{"location":"glossary/#hedgehog-vs-fox-modeling","text":"Focus on accurate modeling a single domain or subgraph of an enterprise knowledge graph (the Hedgehog) vs general modeling of a wide variety of subgraphs or domains. The term comes from Archilochus who stated \"a fox knows many things, but a hedgehog knows one important thing\". There are pros and cons for taking different approaches. No single strategy will work for all enterprise knowledge graphs at all times. The involvement of subject-matter experts (hedgehogs) at different times in the lifecycle of an enterprise knowledge graph will impact the evolution of enterprise knowledge graphs.","title":"Hedgehog vs Fox Modeling"},{"location":"glossary/#higher-order-knowledge","text":"A height-related metaphor that is used to describe more abstract knowledge that is more universal in an enterprise knowledge graph. The concept of \"height\" is related to the layers of the Knowledge Triange . For example, the idea behind \"higher-order thinking\" is that some types of learning requires more cognitive processing than others, but also have more generalized benefits. Within knowledge graphs this may not translate into more CPU time for query traversal, but may depend on having more abstract vertices and edges in an upper or mid-range ontology. Wikipedia page on Higher Order Thinking","title":"Higher Order Knowledge"},{"location":"glossary/#index-free-adjacency","text":"Accessing related entities in a system without having to consult a centralized index. Using direct in-memory pointers to represent relationships is approximately three orders of magnitude faster than referencing a central index system. See also: The Neighborhood Walk Story","title":"Index Free Adjacency"},{"location":"glossary/#information-layer","text":"Data about our key business entities. This includes Things, like People, Places and Events.","title":"Information Layer"},{"location":"glossary/#inmon-data-warehouse","text":"The Inmon Data Warehouse is a collection of database design patterns that promote analytics using relational databases promoted by Bill Inmon. The Inomn approach was first enumerated in his 1992 book \"Building the Data Warehouse\". The Inmon approach is usually contrasted to the more recent 2013 Kimball Data Warehouse that focuses on a simplicity and single fact table with many dimensions. Many EKG projects can be funded by their ability to show they can decommission expensive Inmon-style data warehouses that don't have the flexibility of EKGs. Wikipedia page on Bill Inmon","title":"Inmon Data Warehouse"},{"location":"glossary/#key-value-store","text":"A type of database that stores items as pairs of keys and values. The keys are strings and the values are binary blobs such as files or images. A simple put/get/delete interface is used to manage the database. Key-value stores are excellent complements to graph databases since their simplicity allows for low-cost-per-byte storage.","title":"Key-Value Store"},{"location":"glossary/#kimball-data-warehouse","text":"A data warehouse design pattern that uses a single fact table joined with dimensional tables to minimize the impact of JOIN statements in reporting performance. Kimball data warehouses are the ultimate in denormalized database design . Their goal is often simplicity at the expense of capturing complex relationship-intensive models of the world that can be reused across the enterprise. As a result, Kimball datamarts duplicate data in each department and each datamart has their own costs to perform ETL operations. In contrast, EKGs focus on highly [normalized] data models of the world that include many complex relationships. This closer our models get to the real world the more they can be reused across many departments. Decomissioning many departmental datamarts is often a key way to justify EKG projects. Wikipedia Page on Ralph Kimball","title":"Kimball Data Warehouse"},{"location":"glossary/#knowledge-layer","text":"A layer in the knowledge triangle that contains connected information. The knowledge layer is often the top layer in our views. There are some views that include a Wisdom layer on top of the knowledge layer.","title":"Knowledge Layer"},{"location":"glossary/#knowledge-graph","text":"A set of interconnected typed entities and their attributes. Entities can be any business objects, customers, products, parts, documents, employees or concepts. Entities are usually implemented as vertices in a graph database and connected through edges. In some types of graphs, for example LPGs , edges also have attributes. Note that this definition has no dependence on semantics and inference. Our definition is intentionally designed to include many types of interconnected datasets. We think your organizational chart is a type of knowledge graph an may be a subgraph of your enterprise knowledge graph.","title":"Knowledge Graph"},{"location":"glossary/#knowledge-management","text":"A multi-disciplinary field that include processes of creating, sharing, using and managing the knowledge and information of an organization. Knowledge Management addresses question such as what is organizational knowledge, what types of knowledge are there ( Tacit vs. Codifiable ), how do we encourage employees to capture and store knowledge is forms that can be reused, how do we motivate employees to link isolated knowledge islands together, how do we make tacit Knowledge more codifiable, searchable and reusable. In this book, we take the approach that EKGs should be part of an organizations overall [Enterprise Knowledge Management] strategy and that individuals with formal training in the field of [Knowledge Engineering] should participate in the creation of EKGs and particpate in their evolution. Wikipedia page on Knowledge Management","title":"Knowledge Management"},{"location":"glossary/#knowledge-representation","text":"The process of representing information (individual facts) about the world in a form that a computer system can utilize to solve complex tasks. Within the context of the enterprise knowledge graph, we used graph databases as our primary way to store knowledge and we complement graph databases with search engines and key-value stores when they are more efficient. There is no single knowledge reprenstation that is ideal for all problems. Graph database are the preferred way to store knowledge because efficient reasoning can be implemented as fast pointer-hopping operations that can be optimized by specialized hardware. Knowledge representation is often the most complex challenge in the field of Artificial Intelligence. Wikipedia Page on Knowledge Representation","title":"Knowledge Representation"},{"location":"glossary/#knowledge-triangle","text":"A stack of three layers that illustrates how knowledge graphs are constructed from raw data. At the base is the Data Layer that stores raw binary data in numeric forms, above that is Information Layer that finds concepts and business entities within the data layer. At the top the triangle is the Knowledge Layer where business entities are connected to make them easy to query using graph traversal algorithms. See also: The DIKW Pyramid","title":"Knowledge Triangle"},{"location":"glossary/#narrative-level","text":"The first stage in converting a rule from natural language, such as English, into an executable rule in a knowledge graph or other system. A narrative rule may contain the following elements: Source of rule (authority organization) Area of focus (domain) Scope Context or setting Recommendations or action to be taken The population of items that are included The population of items that ar excluded Policy considerations (e.g. privacy, access, regulations) Narrative levels may contain words or phrases that link to concepts in a Concept Graph. See Levels of Knowledge","title":"Narrative Level"},{"location":"glossary/#label","text":"A string associated with one or more Concepts . Labels have two main types: Preferred Labels and Alternate Labels . Most formal ontologies and taxonomies only permit a single preferred label for each Concept in a given Language.","title":"Label"},{"location":"glossary/#labeled-property-graph","text":"A graph data model where each Vertex and Edge have a single type and goth Vertices and Edges have attributes. Both TigerGraph and Neo4j use the LPG data model. Also known as: LPG","title":"Labeled Property Graph"},{"location":"glossary/#load-as-is-pattern","text":"A data loading pattern that loads the data into a graph with minimal transformation. Once the data is loaded into the graph the transformation is done in the native language of the graph such as GSQL. This pattern allows many projects to share the underlying data loaders and allows each team to customize the post-loading transformation using the native query language of the graph. The other major data model is the RDF model which is discouraged at Optum due to the challenges with Reification. Reification causes RDF SPARQL queries to be rewritten. Load-as-is pattern See also: RDF","title":"Load-As-Is Pattern"},{"location":"glossary/#the-neighborhood-walk-story","text":"A story used to illustrate the difference between direct pointer hopping and using centralized indexes to traverse relationships. The story uses a 30-second walk between two houses vs. an 8.2-hour walk to a central location and back.","title":"The Neighborhood Walk Story"},{"location":"glossary/#no-complexity-penalty","text":"Unlike relational databases, graph databases quickly traverse many complex relationships. As a result, graph databases are better at modeling the real world - which is full of complexity. We use the phrase \"No Complexity Penalty\" every time we are training people who have come from the relational world that worry that too many relationships will slow down their queries due to slow JOINs. See also: One version of the truth","title":"No Complexity Penalty"},{"location":"glossary/#one-version-of-the-truth","text":"The real world has many complex relationships. There are many ways to build simple models that take shortcuts to optimize queries by limiting relationships. This is important in relational database modeling. But the closer we get to modeling the real world, the closer to a single version of the truth we get. Models that fairly represent the complexities of the real world can be reused among many business units and thus the costs of holding the information in memory can be shared. This is why graph databases cost less then relational databases.","title":"One Version of the Truth"},{"location":"glossary/#online-analytical-processing-system","text":"An approach to answer multi-dimensional analytical queries quickly by minimizing JOIN operations in relational databases. OLAP \"cubes\" often use a star schema with a central fact table and one JOIN operation per dimension of the cube. The denomalization process used to create star schemas limits enterprise-sharing of these structures. Wikipedia","title":"Online Analytical Processing System"},{"location":"glossary/#on-the-wire-vs-in-the-can","text":"A way of looking at knowledge representation requirements in two domains. On-The-Wire implies that serialization of a dataset must retain connection information within itself and to other external systems. In-The-Can knowledge representations are optimized for ease of query and sustainability . RDF is optimized for On-The-Wire exchange of knowledge. LPG is optimized for In-The-Can tasks such as ease of query and sustainability.","title":"On-the-Wire vs. In-the-Can"},{"location":"glossary/#ontology","text":"A graph of Concepts within a specific domain. Ontologies often begin as flat term lists, that become taxonomies that then have more complex relationships than simple broader and narrower concepts. Ontologies are often stored in formats such as SKOS and OWL.","title":"Ontology"},{"location":"glossary/#open-vs-closed-world","text":"https://en.wikipedia.org/wiki/Open-world_assumption https://en.wikipedia.org/wiki/Closed-world_assumption","title":"Open vs Closed World"},{"location":"glossary/#operational-source-system","text":"A transactional computer that is the source of a data stream. Enterprise Graph Databases often use Change Data Capture software on these systems to create an event stream of change records that so they can be stored in a central enterprise knowledge graph. Change records are new, updated, or deleted business entities.","title":"Operational Source System"},{"location":"glossary/#pagerank","text":"A graph algorithm that is used to rank the most influential vertices in a directed graph. For example web pages in a graph of linked web pages. PageRank was first used by Google Search to rank web pages in their search engine results. The patent for PageRank (now expired) was purchased by Google from Standford University for Google shares. Those shares sold for over $336 million USD when Google went public. Wikipedia","title":"PageRank"},{"location":"glossary/#preferred-label","text":"A preferred lexical label associated with a Concept . In the SKOS standard, there should be one and only one preferred label per language per concept.","title":"Preferred Label"},{"location":"glossary/#project-graph","text":"A graph that supports a specific project. Project graphs may contain knowledge that is not of interest to the rest of the enterprise.","title":"Project Graph"},{"location":"glossary/#reference-data","text":"Reference data is data used to classify or categorize other data. They typically are stored as a set of valid codes for a specific data element. For example the list of Country Codes is a type of reference data. Reference data is often stored as a short code and a definition of what that code represents.","title":"Reference Data"},{"location":"glossary/#reification","text":"Reification is the process by which an abstract idea about a computer program is turned into an explicit data model or other object created in a programming language. Specifically, in the RDF modeling process it is the process of adding an abstract vertex to a graph when properties are needed in a relationship. Reification causes queries that traverse that node to be rewritten. This means that SPARQL queries are inherently much more difficult to maintain than LPG graph queries.","title":"Reification"},{"location":"glossary/#resource-description-framework","text":"An early family of standards developed by the World Wide Web Consortium for exchanging graph data championed by the Semantic Web community starting in 1999. RDF gained some traction around 2010 but failed to gain widespread adoption due to the complexity of the standards and the problems of Reification . Wikipedia","title":"Resource Description Framework"},{"location":"glossary/#resource-quota","text":"The ability to limit the resources consumed by a query such as CPU time, or RAM for individuals or groups. Large enterprise-scale graph databases must carefully monitor and constrain queries that consume too many resources. Many older technologies such as Apache Drill are difficult to implement without the ability to monitor and restrict resources.","title":"Resource Quota"},{"location":"glossary/#role-based-access-control","text":"The ability to assign access to a resource to individuals that have a specific role. For Enterprise Knowledge Graphs, there are both high-level subgraph rules and fine-grain rules such as vertex-related role-based access control.","title":"Role-based Access Control"},{"location":"glossary/#rules-engine","text":"A software component that executes rules according to some algorithm. In the Enterprise Knowledge Graph space rules are frequently represented in Decision Tree structures within the graph. [Rules for Knowledge Graph Rules])https://dmccreary.medium.com/rules-for-knowledge-graphs-rules-f22587307a8f","title":"Rules Engine"},{"location":"glossary/#semantics","text":"The branch of computer science associated with meaning. It can be best understood by understanding the semantic triangle. The key point of the semantic triangle is that we cannot directly associate a label with a referent without traversing concepts.","title":"Semantics"},{"location":"glossary/#semantic-graph","text":"A graph where each vertex represents a Concept and the edges of the graph represent the relationships between the Concepts. The primary data model for storing semantic graphs is the SKOS data model where Concepts and Labels are distinct types.","title":"Semantic Graph"},{"location":"glossary/#shapes-constraint-language","text":"A W3C standard RDF vocabulary for validating RDF graphs against a set of conditions. Unlike document validation standards like XML Schema, SHACL assumes that data quality checks should be able to look for relationships in a graph as well as the local context of a document. These conditions are provided as shapes and other constructs expressed in the form of an RDF graph. LPG graphs do not yet have a version of SHACL. Also known as: SHACL SHACL W3C","title":"Shapes Constraint Language"},{"location":"glossary/#simple-knowledge-organizational-system","text":"A model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabularies. SKOS is also the name of the world-wide-web standard for encoding these systems. Serializations of SKOS are typically done in RDF format although other encodings such as XML and JSON are common. See Also: W3C SKOS Primer See Also: W2C SKOS Referecnce","title":"Simple Knowledge Organizational System"},{"location":"glossary/#strategy-graph","text":"A graph that is used to help determine what strategies might be optimal for an enterprise or a subgroup as well as how organizations are performing on a specific strategy. To be successful, enterprise and departmental strategies must be encoded in machine-readable forms such as StratML and loaded into an enterprise knowledge graph. Strategy graphs can also be used to determine the alignment of proposed projects for the future in an organization. StratML encoding is sometimes required of US federal organizations so that strategies can be analyzed by published public documents.","title":"Strategy Graph"},{"location":"glossary/#strategic-serendipity","text":"Building a enterprise strategy around the creation of an environment where it is easier to make unexpected connections between items. Strategic serendipity involves getting a large number of people ready to discover new things in an enterprise knowledge graph.","title":"Strategic Serendipity"},{"location":"glossary/#subgraph","text":"A subset of an enterprise knowledge graph that may store specific types of knowledge and may have specific access control rules based on the role of a user. For example, a business glossary, taxonomy, ontology or business rules system that contains no customer-specific information may be in one subgraph and have universal read-access for all users. Customer-specific data that is highly confidential may be stored in a different subgraph, with read access only granted on a need-to-know basis. See also: Role Based Access Control","title":"Subgraph"},{"location":"glossary/#sustainability","text":"The ability for a team of developers to maintain the code that supports an enterprise knowledge graph in the face of changes to the data model. The key measure is to avoid problems related to rewriting graph queries when small changes are made to the graph data model. See also: The Jenga Tower Story","title":"Sustainability"},{"location":"glossary/#systems-thinking","text":"A way of looking at problems in terms of components that interact with each other over time using direct connections, indirection connections and both positive and negative feedback cycles. Systems Thinking forces us to think broadly about how our enterprise knowledge graphs interact with external systems. Systems thinking also helps us see the unintended consequences of our actions.","title":"Systems Thinking"},{"location":"glossary/#tacit-knowledge","text":"A type of knowledge that is difficult to codify in terms of a machine readable artifacts that can be reused across an enterprise. Tacit knowledge is difficult to transfer from one person to another person by means of writing or verbalizing. Understanding what tacit knowledge is developing strategies to convert it to codifiable knowledge is a key areas of research in the fields of Knowledge Management and Knowledge Engineering . See also: Codifiable Knowledge Wikipedia Page on Tacit Knowledge","title":"Tacit Knowledge"},{"location":"glossary/#tbox","text":"A \"terminological component\" or terminology Concept associated with a set of facts assertions (ABox statements) of a knowledge graph. TBox statements tend to more rules or metarules (rules about rules) that individual fact about customers or other business entities. See also: ABox Wikipedia Tbox page","title":"TBox"},{"location":"glossary/#technology-adoption-life-cycle","text":"A sociological model that describes the adoption or acceptance of a new product or innovation, according to the demographic and psychological characteristics of defined adopter groups. Wikipedia","title":"Technology Adoption Life Cycle"},{"location":"glossary/#temporal-modeling","text":"The process of modeling time in a data model. Modeling time can be complex when the requirements of a system require you to be able to recreate detailed reports as they were at a prior point in time. Temporal modeling includes the concept of versioning and bitemporal modeling","title":"Temporal Modeling"},{"location":"glossary/#triple-store","text":"A purpose-built database for the storage and retrieval of RDF triples through semantic queries. Triple stores are not used in most enterprise graphs due to their lack of sustanability due to problems with Reification . https://en.wikipedia.org/wiki/Triplestore","title":"Triple Store"},{"location":"glossary/#upper-ontology","text":"General high-level Concepts that are common across all domains in a knowledge graph. Concepts such as Organization, Customer, Family Unit, Product, Part, Invoice, Document are often consider part of an upper ontology. Enterprise graphs may contain multiple ontologies and the ease of linking ontologies in highly dependant on sharing upper ontologies. Wikipedia Page on Upper Ontology","title":"Upper Ontology"},{"location":"glossary/#web-ontology-language","text":"A Semantic Web language designed to represent rich and complex knowledge about things, groups of things, and relations between things. W3C OWL Web Site","title":"Web Ontology Language"},{"location":"glossary/#window-of-opportunity","text":"A narrow band of time that an organization might be ready to adopt a new technology such as an enterprise knowledge graph. To arrive at the Window of Opportunity to adapt an enterprise knowledge graph an organization must meet a set of preconditions such as an internal chamption, a shared understanding of what enterprise knowledge graphs are capable of, and the ability to ingest enough information to achieve specific business objectives. Enterprise knowledge graphs then have a limited time to create a pilot project before funding runs out, champions move on, or alternative technologies get mindshare. Very often a specific chrisis can trigger an unexpcted window of opportunity. Knowing how to recognize these windows and take advantage of them is a key skill we attempt to explore in this book. Wikipedia article on Window of Opportunity","title":"Window of Opportunity"},{"location":"references/","text":"References for Enterprise Knowledge Graphs Henry Knowledge Management Knowledge Management: A New Concern for Public Administration Henry, Nicholas L. (May\u2013June 1974). Public Administration Review. 34 (3): 189\u2013196. Knowledge is Power The phrase knowledge is power is frequently attributed to Sir Francis Bacon who clearly popularized it and helped trigger the . The first occurrence of it in recorded history might be attributed to Imam Ali (AD 599-661). We use this phrase as a theme throughout the book. Wikipedia page on Scientia potentia est Intel PIUMA on Arxiv PIUMA: Programmable Integrated Unified Memory Architecture MACC Conference The Rainbow Library A Story of Knowledge Classification SemTech 2007","title":"References"},{"location":"references/#references-for-enterprise-knowledge-graphs","text":"","title":"References for Enterprise Knowledge Graphs"},{"location":"references/#henry-knowledge-management","text":"Knowledge Management: A New Concern for Public Administration Henry, Nicholas L. (May\u2013June 1974). Public Administration Review. 34 (3): 189\u2013196.","title":"Henry Knowledge Management"},{"location":"references/#knowledge-is-power","text":"The phrase knowledge is power is frequently attributed to Sir Francis Bacon who clearly popularized it and helped trigger the . The first occurrence of it in recorded history might be attributed to Imam Ali (AD 599-661). We use this phrase as a theme throughout the book. Wikipedia page on Scientia potentia est","title":"Knowledge is Power"},{"location":"references/#intel-piuma-on-arxiv","text":"PIUMA: Programmable Integrated Unified Memory Architecture","title":"Intel PIUMA on Arxiv"},{"location":"references/#macc-conference","text":"","title":"MACC Conference"},{"location":"references/#the-rainbow-library","text":"A Story of Knowledge Classification","title":"The Rainbow Library"},{"location":"references/#semtech-2007","text":"","title":"SemTech 2007"},{"location":"table-of-contents/","text":"Enterprise Knowledge Graphs Table of Contents Preface Acknowledgements About this book Part 1: Introduction Chapter 1: What is an Enterprise Knowledge Graph Chapter 2: Why Build an Enterprise Knowledge Graph Chapter 3: How we Build EKGS - the EKG Lifecycle Stories EKG Timeline Trends Part 2: Enterprise Knowledge Graphs Concepts The Knowledge Triangle Ingesting Data into Your EKG Scale-Out Graph Databases Graph Machine Learning Graph Modeling Graph Algorithms Load and Stress Testing Graph Hardware Part 3: Enterprise Knowledge Graphs Case Studies Customer 360 - a single view of all your customer touch points Analytics Dashboard - the right insights to the right users Semantic Search - EKG services for helping enterprise search Product Recommendation - product graphs Employee Knowledge Graphs - the rightHR insights Strategy Analytics - Are we aligned? Part 4: Promoting Enterprise Knowledge Graphs Storytelling - using narrative bias to your advantage Technology Adoption - telling the right story to the right audi Windows of Opportunity - the EKG pilot Strategic Serendipity - getting ready for discovery Conclusion Glossary","title":"Table of Contents"},{"location":"table-of-contents/#enterprise-knowledge-graphs-table-of-contents","text":"Preface Acknowledgements About this book","title":"Enterprise Knowledge Graphs Table of Contents"},{"location":"table-of-contents/#part-1-introduction","text":"Chapter 1: What is an Enterprise Knowledge Graph Chapter 2: Why Build an Enterprise Knowledge Graph Chapter 3: How we Build EKGS - the EKG Lifecycle Stories EKG Timeline Trends","title":"Part 1: Introduction"},{"location":"table-of-contents/#part-2-enterprise-knowledge-graphs-concepts","text":"The Knowledge Triangle Ingesting Data into Your EKG Scale-Out Graph Databases Graph Machine Learning Graph Modeling Graph Algorithms Load and Stress Testing Graph Hardware","title":"Part 2: Enterprise Knowledge Graphs Concepts"},{"location":"table-of-contents/#part-3-enterprise-knowledge-graphs-case-studies","text":"Customer 360 - a single view of all your customer touch points Analytics Dashboard - the right insights to the right users Semantic Search - EKG services for helping enterprise search Product Recommendation - product graphs Employee Knowledge Graphs - the rightHR insights Strategy Analytics - Are we aligned?","title":"Part 3: Enterprise Knowledge Graphs Case Studies"},{"location":"table-of-contents/#part-4-promoting-enterprise-knowledge-graphs","text":"Storytelling - using narrative bias to your advantage Technology Adoption - telling the right story to the right audi Windows of Opportunity - the EKG pilot Strategic Serendipity - getting ready for discovery Conclusion Glossary","title":"Part 4: Promoting Enterprise Knowledge Graphs"},{"location":"case-studies/analytics-dashboard/","text":"","title":"Analytics Dashboard"},{"location":"case-studies/customer-360/","text":"","title":"Customer 360"},{"location":"case-studies/rules-engine/","text":"","title":"Rules Engine"},{"location":"case-studies/semantic-search/","text":"","title":"Semantic Search"},{"location":"case-studies/strategy-graph/","text":"Strategy Graph Strategic planning is complex Funding the right projects Project dependency graphs Project dashboards Strategic planning can be a complex process. A CEO may get hundreds of annual project proposals all claiming to lower expenses, increase revenue or increase agility. Many of these projects will claim to have high return on investments. Perhaps they claim their new system will allow three old systems to be decommissioned or provide valuable new insight to trends existing systems. In this chapter we will look at how an enterprise knowledge graph can be used to predict the actual savings of new projects, systems or initiatives based on three observations: Strategic projects are not isolated silos. Many depend on other projects being successful. If projects can provide infrastructure to other projects they should be prioritized. Thus a project dependency graph is a key aspect to strategic planning. Projects are often similar to other projects that we may already have experience with. By being able to find similar projects we can build better cost models by using data from prior projects. The individuals that are assigned to projects also have a history on prior projects. Building an accurate individual project histories can also aid in predicting a projects success and its return on investment. Using EKG to Build Project Dashboards After an annual strategy has been announced there are many questions that a CEO may want to know about the status of a project and the individual actions that people are taking. We can use EKGs to build dashboards that gather complex metrics from projects, team, tasks and communications to see how strategies are progressing.","title":"Strategy Graph"},{"location":"case-studies/strategy-graph/#strategy-graph","text":"Strategic planning is complex Funding the right projects Project dependency graphs Project dashboards Strategic planning can be a complex process. A CEO may get hundreds of annual project proposals all claiming to lower expenses, increase revenue or increase agility. Many of these projects will claim to have high return on investments. Perhaps they claim their new system will allow three old systems to be decommissioned or provide valuable new insight to trends existing systems. In this chapter we will look at how an enterprise knowledge graph can be used to predict the actual savings of new projects, systems or initiatives based on three observations: Strategic projects are not isolated silos. Many depend on other projects being successful. If projects can provide infrastructure to other projects they should be prioritized. Thus a project dependency graph is a key aspect to strategic planning. Projects are often similar to other projects that we may already have experience with. By being able to find similar projects we can build better cost models by using data from prior projects. The individuals that are assigned to projects also have a history on prior projects. Building an accurate individual project histories can also aid in predicting a projects success and its return on investment.","title":"Strategy Graph"},{"location":"case-studies/strategy-graph/#using-ekg-to-build-project-dashboards","text":"After an annual strategy has been announced there are many questions that a CEO may want to know about the status of a project and the individual actions that people are taking. We can use EKGs to build dashboards that gather complex metrics from projects, team, tasks and communications to see how strategies are progressing.","title":"Using EKG to Build Project Dashboards"},{"location":"concepts/entity-resolution/","text":"","title":"Entity Resolution"},{"location":"concepts/graph-algorithms/","text":"","title":"Graph Algorithms"},{"location":"concepts/graph-hardware/","text":"Graph Hardware Hardware Architecture Intel\u2019s Incredible PIUMA Graph Analytics Hardware","title":"Graph Hardware"},{"location":"concepts/graph-hardware/#graph-hardware","text":"Hardware Architecture Intel\u2019s Incredible PIUMA Graph Analytics Hardware","title":"Graph Hardware"},{"location":"concepts/graph-machine-learning/","text":"","title":"Graph Machine Learning"},{"location":"concepts/graph-modeling/","text":"Graph Data Modeling All models are wrong, but some are useful. - George Box What is graph data modeling? How is it different from traditional RDBMS data modeling? Keeping blobs out of the graph Accurately modeling complexity Single models of the world Modeling to minimize RAM Pruning unneeded attributes Vertices, edges and attributes Example: A geospatial model What is a Graph Data Model? Single Models of the Truth All happy families are alike, but every unhappy family is unhappy in its own way. (Leo Tolstoy, Anna Karenina, 1878) I think about this quote when I am data modeling. A happy data model is one that adequately captures the complexity of the real world. If the world has lots of complex relationships, then our model may need to capture these relationships. On the other hand, when we denomalization a logical data model to increase the performance of a query, we make our other family members very unhappy. Pruning Unneeded Attributes","title":"Graph Modeling"},{"location":"concepts/graph-modeling/#graph-data-modeling","text":"All models are wrong, but some are useful. - George Box What is graph data modeling? How is it different from traditional RDBMS data modeling? Keeping blobs out of the graph Accurately modeling complexity Single models of the world Modeling to minimize RAM Pruning unneeded attributes Vertices, edges and attributes Example: A geospatial model","title":"Graph Data Modeling"},{"location":"concepts/graph-modeling/#what-is-a-graph-data-model","text":"","title":"What is a Graph Data Model?"},{"location":"concepts/graph-modeling/#single-models-of-the-truth","text":"All happy families are alike, but every unhappy family is unhappy in its own way. (Leo Tolstoy, Anna Karenina, 1878) I think about this quote when I am data modeling. A happy data model is one that adequately captures the complexity of the real world. If the world has lots of complex relationships, then our model may need to capture these relationships. On the other hand, when we denomalization a logical data model to increase the performance of a query, we make our other family members very unhappy.","title":"Single Models of the Truth"},{"location":"concepts/graph-modeling/#pruning-unneeded-attributes","text":"","title":"Pruning Unneeded Attributes"},{"location":"concepts/ingestion/","text":"Ingesting Data into an Enterprise Knowledge Graph Data Flow concepts Operational Source System Change Data Capture Business Event Publishing Schema Matching Schema Mapping The Publish-Subscribe Pattern Batch Updates Streaming Updates Data Quality Checks Document Validation In Graph Data Checks Case Study: SHACL Shape Constraint Language Entity Resolution Master Data Management","title":"Ch 6 Ingesting Data"},{"location":"concepts/ingestion/#ingesting-data-into-an-enterprise-knowledge-graph","text":"","title":"Ingesting Data into an Enterprise Knowledge Graph"},{"location":"concepts/ingestion/#data-flow-concepts","text":"","title":"Data Flow concepts"},{"location":"concepts/ingestion/#operational-source-system","text":"","title":"Operational Source System"},{"location":"concepts/ingestion/#change-data-capture","text":"","title":"Change Data Capture"},{"location":"concepts/ingestion/#business-event-publishing","text":"","title":"Business Event Publishing"},{"location":"concepts/ingestion/#schema-matching","text":"","title":"Schema Matching"},{"location":"concepts/ingestion/#schema-mapping","text":"","title":"Schema Mapping"},{"location":"concepts/ingestion/#the-publish-subscribe-pattern","text":"","title":"The Publish-Subscribe Pattern"},{"location":"concepts/ingestion/#batch-updates","text":"","title":"Batch Updates"},{"location":"concepts/ingestion/#streaming-updates","text":"","title":"Streaming Updates"},{"location":"concepts/ingestion/#data-quality-checks","text":"","title":"Data Quality Checks"},{"location":"concepts/ingestion/#document-validation","text":"","title":"Document Validation"},{"location":"concepts/ingestion/#in-graph-data-checks","text":"","title":"In Graph Data Checks"},{"location":"concepts/ingestion/#case-study-shacl","text":"Shape Constraint Language","title":"Case Study: SHACL"},{"location":"concepts/ingestion/#entity-resolution","text":"Master Data Management","title":"Entity Resolution"},{"location":"concepts/ingestion/#_1","text":"","title":""},{"location":"concepts/knowledge-triangle/","text":"The Knowledge Triangle Knowledge does not emerge from data. - Judea Pearl Information Layer Knowledge Layer Other Models The DIKW Pyramid Although we use the term \u201cknowledge\u201d broadly in normal conversation, it has a specific meaning in the AI and graph database community. Even within computer science, it has many different meanings based on the context of a discussion. This article gives a suggested definition of the term \u201cknowledge\u201d and uses the Knowledge Triangle metaphor to explain our definition. We will then show some variations of the Knowledge Triangle and see how the word knowledge is used in information management and learning management systems. I have found that having a clear image of the knowledge triangle in your mind is essential to understanding the processes around modern database architectures. Here is our definition of Knowledge in the context of AI and graph databases: Knowledge is connected-information that is query ready. This definition is a much shorter than the Wikipedia Knowledge page which is: \u2026a familiarity, awareness, or understanding of someone or something, such as facts, information, descriptions, or skills, which is acquired through experience or education by perceiving, discovering, or learning. The Wikipedia definition is longer, more general and applicable to many domains like philosophy, learning, and cognitive science. Our definition is shorter and only intended for the context of computing. Our definition is also dependant on how we define \u201cinformation\u201d, \u201cconnected\u201d, and \u201cquery ready\u201d. To understand these terms, let\u2019s reference the Knowledge Triangle figure above. The Data Layer In the knowledge triangle diagram, let\u2019s start at the bottom Data Layer. The data layer contains unprocessed raw information in the forms of binary codes, numeric codes, dates, strings, and full-text descriptions that we find in documents. The data layer can also include images (just as a jpeg file), speech (in the form of a sound file), and video data. You can imagine raw data as a stream of ones and zeros. It is a raw dump of data from your hard drive. Some types of raw data, such as an image \u2014 can be directly understood by a person just by viewing it. Usually, raw data is not typically useful without additional processing. We call this processing of raw data enrichment. Enrichment Enrichment takes raw data and extracts the things we care about and converts data into Information. This Information consists of items we call business entities: people, places, events, and concepts. The Information Layer Information is the second layer of the Knowledge Triangle. Information is more useful than raw data, but Information itself consists of islands of disconnected items. When we start to link information together, it becomes part of the Knowledge layer. The Knowledge Layer Knowledge is the top layer of the Knowledge Triangle. The knowledge layer puts information into context with the rest of the information in our system. It is this context that gives information structure. Structure gives us hints about how relevant information is for a given task. Structure Informs Relevancy How does structure inform relevance? Let\u2019s take a search example. Let\u2019s say we have a book on the topic of NoSQL. The word \u201cNoSQL\u201d should appear in the title of that book. There also might be other books on related topics, but they only mention NoSQL in a footnote of the book. If the counts of the term NoSQL are the same in both books then the book on NoSQL might be buried far down in the search results. A search engine that uses structural search knows that titles are essential in findability. Structural search engines boost hits of a keyword within a title of a document by a factor of 10 or 100. Many search engines used for intranet search (notability the default Microsoft Sharepoint) ignore the structure of a document when doing document retrieval, so they have a reputation for their inability to find documents. The structured search example above is an excellent example of where query readiness is enhanced in the knowledge layer. The fact that a keyword appears somewhere in a document reflects very little structure. The fact that a keyword appears in a title has much more value. The fact that the keyword appeared in a chapter title gives us some knowledge that that entire chapter is about that keyword. Enrichment and Machine Learning Today most enrichment is done by using simple rule-based systems. The most basic rules are called data ingestion rules where data transformation maps are created and executed when new data is loaded into our system. A typical map rule says take data from the fourth column of the CSV file and assign this to the field PersonFamilyName. These rules are manually created and maintained. About 70% of the cost of building enterprise knowledge graphs are related to building and maintaining these mapping processes. These mapping steps are often the most tedious parts of building AI systems since they require attention to detail and validation. Source systems frequently change, and there the meaning of codes may drift over time. Continuous testing and data quality staff are essential for these processes to be robust. The phrase garbage-in, garbage-out (GIGO) applies. What is revolutionary about the mapping process is we are just starting to see machine learning play a role in this process. These processes are often called automated schema mapping or algorithm assisted mapping. To be automated, these processes involve keeping a careful log of prior mappings as a training set. New maps can then be predicted with up to 95% accuracy for new data sources. These algorithms leverage lexical names, field definitions, data profiles, and semantic links for predicting matching. Automatic schema mapping is an active field of research for many organizations building knowledge graphs. Automated mapping will lower the cost of building enterprise knowledge graphs dramatically. Graph algorithms such as cosine similarity can be ideal for finding the right matches. Structure and Abstraction We should also note that many things in the real world also reflect the Knowledge Triangle architecture of raw data at the bottom and connected concepts at the top. One of my favorite examples is the multi-level architecture of the neural networks in animal brains, as depicted below. Brains have multiple layers of neural networks. Data arrives at the bottom layers and travels upward with each layer representing more abstract concepts. The human neocortex has up to six layers of processing. This figure is derived from Jeff Hawkin\u2019s book On Intelligence. Just like the Knowledge Triangle, raw data arrives at the bottom layer and travels upwards. But unlike our three-layer model, brains have up to six layers of complex non-linear data transformations. At the top of the stack, concepts such as recognition of an object in a file, the detection of a specific object in an image or the detection a person\u2019s face are turned to the \u201con\u201d state. There are also feedback layers downward so that if the output of one layer has quality problems, new signals are sent back down to gain more insights at what objects are recognized. Many people like to use brain metaphors when they explain knowledge graphs. Although some of these metaphors are useful, I urge you to use them cautiously. Brains typically have 10,000 connections per-vertex, and each connection does complex signal processing. So the architectures are very different in practice. The last term we need to define is query readiness. Query Readiness Of the many ways we can store data, which forms are the most useful for general analysis? Which forms need the minimum of processing before we can look for insights? What are the queries, searches, and algorithms we can use plug it to quickly find meaning in the data? The larger the number of these things you can use without modification, the more query ready your data is. What the industry is finding is that the number of algorithms available to graph developers today is large and growing. The rise of distributed native labeled property graphs is making these algorithms available even for tens of billions of vertices. In summary, graphs are wining the algorithms race. The performance and scale-out abilities of modern graph database are pushing them to the forefront of innovation. Variations on the Knowledge Triangle There are also many variations on the basic knowledge triangle metaphor that are useful in some situations. One of the most common is to add a Wisdom layer on top of the Knowledge layer. This four-layer triangle is known as the DIKW pyramid and is used frequently in information architecture discussions. I tend to downplay the role of wisdom in my early knowledge graph courses since the wisdom layer seems to be associated with touchy-feely topics or stories about visiting the guru on the mountain top for advice. That being said, there are some useful things to consider about the term wisdom. For example, when you go to an experienced person for advice, you share with them your problem and the context of that problem. You expect them to use their knowledge to give you advice about your problem. You are expecting them to transfer a bit of their knowledge to you. We imagine the wisdom layer as a feedback layer to the structure of the knowledge layer. Wisdom can inform us how we can structure our knowledge in a way that it can be transferred from one context to another and still be valuable. Stated in another way, can we take a small sub-graph out of an enterprise knowledge graph and port it to another graph and still be useful? For example, let\u2019s suppose we have a sub-graph that stores information about geography. It might have postal codes, cities, counties, states, regions, countries, islands and continents in the graphs. Can we lift the geospatial subgraph out of one graph and drop into another graph? In neural networks and deep learning, taking a few layers of one neural network and dropping it into another network is called transfer learning. Transfer learning is frequently used in image and language models where training times are extensive. How you reconnect these networks into a new setting is a non-trivial problem. These are the questions about the knowledge layer that you should be asking when you design your enterprise knowledge graph. If calling reuse issues the \u201cWisdom\u201d layer helps you in your discussions, we encourage you to adopt this layer. Data Science and Knowledge Science In some of my prior articles, I discussed the trends of moving from data science to knowledge science. We can also use the Knowledge Triangle metaphor to explain this process. This process is fundamentally about allowing staff direct access to a connected graph of your enterprise knowledge, thus saving them all the hassles of making meaning out of your raw data in the data lake. Data science staff can get faster time to insights using direct access to a knowledge graph. To wrap up the post, I also want to suggest one other version of the knowledge triangle that has been mapped to an actual set of tools in a production knowledge graph. Instead of the abstract concept of raw data, we replace it with a diagram of a Data Lake or an object store such as Amazon S3. At the Information layer, we list the concepts we are looking for in the Data Lake, the definitions of these concepts, and the rules to validate each of these atomic data elements to make them valid. We also allow users to associate each business entity with a URI so they can be linked together in the next higher step. At the Knowledge Graph layer, we talk about making connections between the entities found in the information layer and the tools we use to connect data and find missing relationships automatically. These processes include entity resolution, master data management, deduplication and shape validation. Image for post From Data Lakes to transfer learning. The Knowledge Triangle in practice. This diagram also mentions that there is often a feedback layer that automatically sends alerts to the data enrichers that there might be missing data and clues on how this data can be found. Knowledge Spaces in Learning Management Systems Lastly, we want to mention that modern AI-powered learning management systems (LMS) also use the term Knowledge Space. In the context of an LMS, knowledge space is a set of concepts that must be mastered to achieve proficiency in a field. Each student has a Knowledge State that shows where they are in learning a topic. AI-powered LMS systems use recommendation engines to recommend learning content associated with the edges of known concepts in each student's Knowledge Space. I will be discussing the topic of AI in education and Knowledge Spaces in a future blog post. Summary In summary, The Knowledge Triangle is one of the most useful metaphors in our graph architecture toolkit. Along with The Neighborhood Walk, the Open World, and the Jenga Tower, it forms the basis for our introductory chapter on Knowledge Graph concepts. I want to thank my friend Arun Batchu for introducing me to the Knowledge Triangle his willingness to transfer his wisdom to me. References The Knowledge Triangle Blog Post Published Aug. 31st, 2019 https://en.wikipedia.org/wiki/Level_of_analysis#Computational Bloom's Taxonomy AI futures - venture beat","title":"Ch 5 Knowledge Triangle"},{"location":"concepts/knowledge-triangle/#the-knowledge-triangle","text":"Knowledge does not emerge from data. - Judea Pearl","title":"The Knowledge Triangle"},{"location":"concepts/knowledge-triangle/#information-layer","text":"","title":"Information Layer"},{"location":"concepts/knowledge-triangle/#knowledge-layer","text":"","title":"Knowledge Layer"},{"location":"concepts/knowledge-triangle/#other-models","text":"","title":"Other Models"},{"location":"concepts/knowledge-triangle/#the-dikw-pyramid","text":"Although we use the term \u201cknowledge\u201d broadly in normal conversation, it has a specific meaning in the AI and graph database community. Even within computer science, it has many different meanings based on the context of a discussion. This article gives a suggested definition of the term \u201cknowledge\u201d and uses the Knowledge Triangle metaphor to explain our definition. We will then show some variations of the Knowledge Triangle and see how the word knowledge is used in information management and learning management systems. I have found that having a clear image of the knowledge triangle in your mind is essential to understanding the processes around modern database architectures. Here is our definition of Knowledge in the context of AI and graph databases: Knowledge is connected-information that is query ready. This definition is a much shorter than the Wikipedia Knowledge page which is: \u2026a familiarity, awareness, or understanding of someone or something, such as facts, information, descriptions, or skills, which is acquired through experience or education by perceiving, discovering, or learning. The Wikipedia definition is longer, more general and applicable to many domains like philosophy, learning, and cognitive science. Our definition is shorter and only intended for the context of computing. Our definition is also dependant on how we define \u201cinformation\u201d, \u201cconnected\u201d, and \u201cquery ready\u201d. To understand these terms, let\u2019s reference the Knowledge Triangle figure above.","title":"The DIKW Pyramid"},{"location":"concepts/knowledge-triangle/#the-data-layer","text":"In the knowledge triangle diagram, let\u2019s start at the bottom Data Layer. The data layer contains unprocessed raw information in the forms of binary codes, numeric codes, dates, strings, and full-text descriptions that we find in documents. The data layer can also include images (just as a jpeg file), speech (in the form of a sound file), and video data. You can imagine raw data as a stream of ones and zeros. It is a raw dump of data from your hard drive. Some types of raw data, such as an image \u2014 can be directly understood by a person just by viewing it. Usually, raw data is not typically useful without additional processing. We call this processing of raw data enrichment.","title":"The Data Layer"},{"location":"concepts/knowledge-triangle/#enrichment","text":"Enrichment takes raw data and extracts the things we care about and converts data into Information. This Information consists of items we call business entities: people, places, events, and concepts.","title":"Enrichment"},{"location":"concepts/knowledge-triangle/#the-information-layer","text":"Information is the second layer of the Knowledge Triangle. Information is more useful than raw data, but Information itself consists of islands of disconnected items. When we start to link information together, it becomes part of the Knowledge layer.","title":"The Information Layer"},{"location":"concepts/knowledge-triangle/#the-knowledge-layer","text":"Knowledge is the top layer of the Knowledge Triangle. The knowledge layer puts information into context with the rest of the information in our system. It is this context that gives information structure. Structure gives us hints about how relevant information is for a given task.","title":"The Knowledge Layer"},{"location":"concepts/knowledge-triangle/#structure-informs-relevancy","text":"How does structure inform relevance? Let\u2019s take a search example. Let\u2019s say we have a book on the topic of NoSQL. The word \u201cNoSQL\u201d should appear in the title of that book. There also might be other books on related topics, but they only mention NoSQL in a footnote of the book. If the counts of the term NoSQL are the same in both books then the book on NoSQL might be buried far down in the search results. A search engine that uses structural search knows that titles are essential in findability. Structural search engines boost hits of a keyword within a title of a document by a factor of 10 or 100. Many search engines used for intranet search (notability the default Microsoft Sharepoint) ignore the structure of a document when doing document retrieval, so they have a reputation for their inability to find documents. The structured search example above is an excellent example of where query readiness is enhanced in the knowledge layer. The fact that a keyword appears somewhere in a document reflects very little structure. The fact that a keyword appears in a title has much more value. The fact that the keyword appeared in a chapter title gives us some knowledge that that entire chapter is about that keyword.","title":"Structure Informs Relevancy"},{"location":"concepts/knowledge-triangle/#enrichment-and-machine-learning","text":"Today most enrichment is done by using simple rule-based systems. The most basic rules are called data ingestion rules where data transformation maps are created and executed when new data is loaded into our system. A typical map rule says take data from the fourth column of the CSV file and assign this to the field PersonFamilyName. These rules are manually created and maintained. About 70% of the cost of building enterprise knowledge graphs are related to building and maintaining these mapping processes. These mapping steps are often the most tedious parts of building AI systems since they require attention to detail and validation. Source systems frequently change, and there the meaning of codes may drift over time. Continuous testing and data quality staff are essential for these processes to be robust. The phrase garbage-in, garbage-out (GIGO) applies. What is revolutionary about the mapping process is we are just starting to see machine learning play a role in this process. These processes are often called automated schema mapping or algorithm assisted mapping. To be automated, these processes involve keeping a careful log of prior mappings as a training set. New maps can then be predicted with up to 95% accuracy for new data sources. These algorithms leverage lexical names, field definitions, data profiles, and semantic links for predicting matching. Automatic schema mapping is an active field of research for many organizations building knowledge graphs. Automated mapping will lower the cost of building enterprise knowledge graphs dramatically. Graph algorithms such as cosine similarity can be ideal for finding the right matches.","title":"Enrichment and Machine Learning"},{"location":"concepts/knowledge-triangle/#structure-and-abstraction","text":"We should also note that many things in the real world also reflect the Knowledge Triangle architecture of raw data at the bottom and connected concepts at the top. One of my favorite examples is the multi-level architecture of the neural networks in animal brains, as depicted below. Brains have multiple layers of neural networks. Data arrives at the bottom layers and travels upward with each layer representing more abstract concepts. The human neocortex has up to six layers of processing. This figure is derived from Jeff Hawkin\u2019s book On Intelligence. Just like the Knowledge Triangle, raw data arrives at the bottom layer and travels upwards. But unlike our three-layer model, brains have up to six layers of complex non-linear data transformations. At the top of the stack, concepts such as recognition of an object in a file, the detection of a specific object in an image or the detection a person\u2019s face are turned to the \u201con\u201d state. There are also feedback layers downward so that if the output of one layer has quality problems, new signals are sent back down to gain more insights at what objects are recognized. Many people like to use brain metaphors when they explain knowledge graphs. Although some of these metaphors are useful, I urge you to use them cautiously. Brains typically have 10,000 connections per-vertex, and each connection does complex signal processing. So the architectures are very different in practice. The last term we need to define is query readiness.","title":"Structure and Abstraction"},{"location":"concepts/knowledge-triangle/#query-readiness","text":"Of the many ways we can store data, which forms are the most useful for general analysis? Which forms need the minimum of processing before we can look for insights? What are the queries, searches, and algorithms we can use plug it to quickly find meaning in the data? The larger the number of these things you can use without modification, the more query ready your data is. What the industry is finding is that the number of algorithms available to graph developers today is large and growing. The rise of distributed native labeled property graphs is making these algorithms available even for tens of billions of vertices. In summary, graphs are wining the algorithms race. The performance and scale-out abilities of modern graph database are pushing them to the forefront of innovation.","title":"Query Readiness"},{"location":"concepts/knowledge-triangle/#variations-on-the-knowledge-triangle","text":"There are also many variations on the basic knowledge triangle metaphor that are useful in some situations. One of the most common is to add a Wisdom layer on top of the Knowledge layer. This four-layer triangle is known as the DIKW pyramid and is used frequently in information architecture discussions. I tend to downplay the role of wisdom in my early knowledge graph courses since the wisdom layer seems to be associated with touchy-feely topics or stories about visiting the guru on the mountain top for advice. That being said, there are some useful things to consider about the term wisdom. For example, when you go to an experienced person for advice, you share with them your problem and the context of that problem. You expect them to use their knowledge to give you advice about your problem. You are expecting them to transfer a bit of their knowledge to you. We imagine the wisdom layer as a feedback layer to the structure of the knowledge layer. Wisdom can inform us how we can structure our knowledge in a way that it can be transferred from one context to another and still be valuable. Stated in another way, can we take a small sub-graph out of an enterprise knowledge graph and port it to another graph and still be useful? For example, let\u2019s suppose we have a sub-graph that stores information about geography. It might have postal codes, cities, counties, states, regions, countries, islands and continents in the graphs. Can we lift the geospatial subgraph out of one graph and drop into another graph? In neural networks and deep learning, taking a few layers of one neural network and dropping it into another network is called transfer learning. Transfer learning is frequently used in image and language models where training times are extensive. How you reconnect these networks into a new setting is a non-trivial problem. These are the questions about the knowledge layer that you should be asking when you design your enterprise knowledge graph. If calling reuse issues the \u201cWisdom\u201d layer helps you in your discussions, we encourage you to adopt this layer.","title":"Variations on the Knowledge Triangle"},{"location":"concepts/knowledge-triangle/#data-science-and-knowledge-science","text":"In some of my prior articles, I discussed the trends of moving from data science to knowledge science. We can also use the Knowledge Triangle metaphor to explain this process. This process is fundamentally about allowing staff direct access to a connected graph of your enterprise knowledge, thus saving them all the hassles of making meaning out of your raw data in the data lake. Data science staff can get faster time to insights using direct access to a knowledge graph. To wrap up the post, I also want to suggest one other version of the knowledge triangle that has been mapped to an actual set of tools in a production knowledge graph. Instead of the abstract concept of raw data, we replace it with a diagram of a Data Lake or an object store such as Amazon S3. At the Information layer, we list the concepts we are looking for in the Data Lake, the definitions of these concepts, and the rules to validate each of these atomic data elements to make them valid. We also allow users to associate each business entity with a URI so they can be linked together in the next higher step. At the Knowledge Graph layer, we talk about making connections between the entities found in the information layer and the tools we use to connect data and find missing relationships automatically. These processes include entity resolution, master data management, deduplication and shape validation. Image for post From Data Lakes to transfer learning. The Knowledge Triangle in practice. This diagram also mentions that there is often a feedback layer that automatically sends alerts to the data enrichers that there might be missing data and clues on how this data can be found. Knowledge Spaces in Learning Management Systems Lastly, we want to mention that modern AI-powered learning management systems (LMS) also use the term Knowledge Space. In the context of an LMS, knowledge space is a set of concepts that must be mastered to achieve proficiency in a field. Each student has a Knowledge State that shows where they are in learning a topic. AI-powered LMS systems use recommendation engines to recommend learning content associated with the edges of known concepts in each student's Knowledge Space. I will be discussing the topic of AI in education and Knowledge Spaces in a future blog post.","title":"Data Science and Knowledge Science"},{"location":"concepts/knowledge-triangle/#summary","text":"In summary, The Knowledge Triangle is one of the most useful metaphors in our graph architecture toolkit. Along with The Neighborhood Walk, the Open World, and the Jenga Tower, it forms the basis for our introductory chapter on Knowledge Graph concepts. I want to thank my friend Arun Batchu for introducing me to the Knowledge Triangle his willingness to transfer his wisdom to me.","title":"Summary"},{"location":"concepts/knowledge-triangle/#references","text":"The Knowledge Triangle Blog Post Published Aug. 31st, 2019 https://en.wikipedia.org/wiki/Level_of_analysis#Computational Bloom's Taxonomy AI futures - venture beat","title":"References"},{"location":"concepts/load-and-stress-testing/","text":"","title":"Load and Stress Testing"},{"location":"concepts/natural-language-processing/","text":"Natural Language Processing What is Natural Language Processing NLP Use Cases Storing NLP Extracts in and EKG When to Use Search Engines What is Natural Language Processing Two Worlds - Text and Codes NLP Use Cases Extracting Facts from Documents Document Classification Classifying documents based on concepts, not keywords. Semantic Search Using taxonomies and ontologies for keyword expansions Chatbots Generating Graph Queries Adding Documents to an EKG Metadata","title":"Natural Language Processing"},{"location":"concepts/natural-language-processing/#natural-language-processing","text":"What is Natural Language Processing NLP Use Cases Storing NLP Extracts in and EKG When to Use Search Engines","title":"Natural Language Processing"},{"location":"concepts/natural-language-processing/#what-is-natural-language-processing","text":"","title":"What is Natural Language Processing"},{"location":"concepts/natural-language-processing/#two-worlds-text-and-codes","text":"","title":"Two Worlds - Text and Codes"},{"location":"concepts/natural-language-processing/#nlp-use-cases","text":"","title":"NLP Use Cases"},{"location":"concepts/natural-language-processing/#extracting-facts-from-documents","text":"","title":"Extracting Facts from Documents"},{"location":"concepts/natural-language-processing/#document-classification","text":"Classifying documents based on concepts, not keywords.","title":"Document Classification"},{"location":"concepts/natural-language-processing/#semantic-search","text":"Using taxonomies and ontologies for keyword expansions","title":"Semantic Search"},{"location":"concepts/natural-language-processing/#chatbots","text":"","title":"Chatbots"},{"location":"concepts/natural-language-processing/#generating-graph-queries","text":"","title":"Generating Graph Queries"},{"location":"concepts/natural-language-processing/#adding-documents-to-an-ekg","text":"Metadata","title":"Adding Documents to an EKG"},{"location":"concepts/scale-out/","text":"Scale-Out Graph Databases Distributing Query Loads Autosharding Partitioning High Availability Fault Tolerance Rolling Upgrades","title":"Scaling Out"},{"location":"concepts/scale-out/#scale-out-graph-databases","text":"","title":"Scale-Out Graph Databases"},{"location":"concepts/scale-out/#distributing-query-loads","text":"","title":"Distributing Query Loads"},{"location":"concepts/scale-out/#autosharding","text":"","title":"Autosharding"},{"location":"concepts/scale-out/#partitioning","text":"","title":"Partitioning"},{"location":"concepts/scale-out/#high-availability","text":"","title":"High Availability"},{"location":"concepts/scale-out/#fault-tolerance","text":"","title":"Fault Tolerance"},{"location":"concepts/scale-out/#rolling-upgrades","text":"","title":"Rolling Upgrades"},{"location":"intro/about-this-book/","text":"About the Enterprise Knowledge Graphs Book Intended Audience The book is intended for individuals and organizations that are considering or currently implementing Enterprise Knowledge Graphs (EKGs). In this book, we define EKGs as scalable graph databases that span two or more business units. The definitions of these terms will be defined in the first chapter of the book. Part 1: Introduction to EKGs Part 1 of this book introduces you to the high-level concepts in EKGs. We describe the what EKGs are, why companies build them and and how EKGs start and grow. Chapter P1.1: Introduction defines what an EKG is and how it is being used in large organizations. We address the question of scalability and how many graph database systems claim to be scalable but fall down when they reach specific architectural limitations. We also bring up the concept of role-based access control since we have found this feature key to widespread adoption of enterprise knowledge graphs. We also introduce other key terms and give their definitions. Chapter P1.2: Cost and Benefits describes why organizations are building EKGs. We briefly cover the cost-benefits of building EKGs and describe the key use-cases of enterprise-knowledge graphs. and economic models that will help you determine if they might be a good fit for your organization. We provide examples of how integration of data aids both customer experience an empowers data scientists to be more productive. Chapter 3: Lifecycles focuses on how EKGs are built in practice in the real world. We describe how pilot projects are created but also designed to scale to meet enterprise needs. We describe how EKG data models evolve an grow to absorbs new knowledge that provides deeper insights. We also discuss the importance of subgraphs and both corse an fine-grain role-based access control. Finally, we introduce the concept of \"Edge of Chaos\" and how it guides how EKGs grow. Part 2: EKG Concepts Part 2 is an in depth analysis of the core concepts involved in building and running EKGs. [Chapter P2.1: Introduction] introduces the concept of the knowledge triangle and the processes we used to turn raw data into entity-based information and then to connect this information into useful and queryable knowledge. [Chapter 2.2: Modeling] covers enterprise-scale graph data modeling issues and the key issue of sharable data models between multiple business groups. If data models can be shared they avoid duplication and lower analytical costs for the enterprise. [Chapter P2.3: Ingestion] covers the concept of scalable transactions and data ingestion into an EKG. We discuss the issues related to change-data-capture in source systems, schema matching and schema mapping, canonical data models, business events, streaming, publish-subscribe patterns and load and stress testing. [Chapter P2.4: Enrichment] introduces the concept if enrichment of data and connecting information. This includes connecting incomming data to taxonomies and ontologies as well as reference data. Part 3: Case Studies Part 3 is a deep dive into the most important case studies that help EKGs get launched. Getting your first project off the ground successfully is critical for EKG success.","title":"About this book"},{"location":"intro/about-this-book/#about-the-enterprise-knowledge-graphs-book","text":"","title":"About the Enterprise Knowledge Graphs Book"},{"location":"intro/about-this-book/#intended-audience","text":"The book is intended for individuals and organizations that are considering or currently implementing Enterprise Knowledge Graphs (EKGs). In this book, we define EKGs as scalable graph databases that span two or more business units. The definitions of these terms will be defined in the first chapter of the book.","title":"Intended Audience"},{"location":"intro/about-this-book/#part-1-introduction-to-ekgs","text":"Part 1 of this book introduces you to the high-level concepts in EKGs. We describe the what EKGs are, why companies build them and and how EKGs start and grow. Chapter P1.1: Introduction defines what an EKG is and how it is being used in large organizations. We address the question of scalability and how many graph database systems claim to be scalable but fall down when they reach specific architectural limitations. We also bring up the concept of role-based access control since we have found this feature key to widespread adoption of enterprise knowledge graphs. We also introduce other key terms and give their definitions. Chapter P1.2: Cost and Benefits describes why organizations are building EKGs. We briefly cover the cost-benefits of building EKGs and describe the key use-cases of enterprise-knowledge graphs. and economic models that will help you determine if they might be a good fit for your organization. We provide examples of how integration of data aids both customer experience an empowers data scientists to be more productive. Chapter 3: Lifecycles focuses on how EKGs are built in practice in the real world. We describe how pilot projects are created but also designed to scale to meet enterprise needs. We describe how EKG data models evolve an grow to absorbs new knowledge that provides deeper insights. We also discuss the importance of subgraphs and both corse an fine-grain role-based access control. Finally, we introduce the concept of \"Edge of Chaos\" and how it guides how EKGs grow.","title":"Part 1: Introduction to EKGs"},{"location":"intro/about-this-book/#part-2-ekg-concepts","text":"Part 2 is an in depth analysis of the core concepts involved in building and running EKGs. [Chapter P2.1: Introduction] introduces the concept of the knowledge triangle and the processes we used to turn raw data into entity-based information and then to connect this information into useful and queryable knowledge. [Chapter 2.2: Modeling] covers enterprise-scale graph data modeling issues and the key issue of sharable data models between multiple business groups. If data models can be shared they avoid duplication and lower analytical costs for the enterprise. [Chapter P2.3: Ingestion] covers the concept of scalable transactions and data ingestion into an EKG. We discuss the issues related to change-data-capture in source systems, schema matching and schema mapping, canonical data models, business events, streaming, publish-subscribe patterns and load and stress testing. [Chapter P2.4: Enrichment] introduces the concept if enrichment of data and connecting information. This includes connecting incomming data to taxonomies and ontologies as well as reference data.","title":"Part 2: EKG Concepts"},{"location":"intro/about-this-book/#part-3-case-studies","text":"Part 3 is a deep dive into the most important case studies that help EKGs get launched. Getting your first project off the ground successfully is critical for EKG success.","title":"Part 3: Case Studies"},{"location":"intro/acknowledgements/","text":"Enterprise Knowledge Graph Acknowledgements I would like to thank the following people: Arun Batchu Nikhil Deshpande Parker Erickson Hank Head Jonathan Herke Mark Megerian John Santelli Sujith Sasidharan Ed Sverdlin Sudeep Vishnumurthy I want to express my gratitude to everyone within the Optum Advanced Technology Collaborative for helping me refine our storytelling strategies to our business areas. Many other people have guided me in my journey in becoming a better storytelling driven solution architect. They have taught me that is critical for us to understand both how technologies work, and to be able to relate the benefits with stories that our stakeholders will remember.","title":"Acknowledgements"},{"location":"intro/acknowledgements/#enterprise-knowledge-graph-acknowledgements","text":"I would like to thank the following people: Arun Batchu Nikhil Deshpande Parker Erickson Hank Head Jonathan Herke Mark Megerian John Santelli Sujith Sasidharan Ed Sverdlin Sudeep Vishnumurthy I want to express my gratitude to everyone within the Optum Advanced Technology Collaborative for helping me refine our storytelling strategies to our business areas. Many other people have guided me in my journey in becoming a better storytelling driven solution architect. They have taught me that is critical for us to understand both how technologies work, and to be able to relate the benefits with stories that our stakeholders will remember.","title":"Enterprise Knowledge Graph Acknowledgements"},{"location":"intro/ekg-cost-benefit/","text":"Chapter 2: Why Build An Enterprise Knowledge Graph? EKG Use Case Taxonomy Cost-Benefit Analysis Metcalf's Law and Network Effects Business Value Measuring EKG costs Measuring EKG tangible benefits Measuring EKG intangible benefits Brains are prediction machines When we build any large resource for an organization we are going to need to ask for money to build it. If you work for a company like Apple and you are respected expert at building enterprise knowledge graphs, you many not need to go to a finance committee and justify your spending requests. Apple hires world experts in specific fields and then trusts their judgement. As long as you have a good track record they will keep giving you funding without asking a lot of questions. But most of us don't work at Apple and we are not world leaders with a long successful track record of building EKGs. We will have to fight for every penny we spend on our EKG pilot projects until the ROI is so clear to everyone that the finance people are begging you to expand the scope of the EKG. Until that happens, we are going to need to learn to speak the language of finance to get our EKGs off the ground. EKG Use Case Taxonomy A Taxonomy of Graph Use Cases Performance Fast Relationship Traversals (no JOINS) Easy to write complex queries ** Easy to perform deep link analysis Flexibility Rules Metcalf's Law and Network Effects Cost-Benefit Analysis [The Business Value of Computers] Wikipedia on Business Value Peter Drucker Measuring Costs Cost of extracting knowledge from operational source systems Change data-capture (CDC) Publishing business events The Easy to Measure Benefits Integrated views of anything Integrated views of your customers Centralized business Rules Recommendation systems Difficult to Measure Benefits The value of insights Tracking early insights Predicting future insights How are innovations and patents valued in your organization? Focus on datasets that generate shared value Brains are Prediction Machines Pick up a ball. Throw it in the air and catch it. A simple act. But to perform this simple act our brain has to do incredible things. Most importantly, your brain will predict where the ball is going so you can move your hands to catch it. And it does this task effortlessly in real time. You don't really have to think about it. This metaphor, of the brain as a prediction machine that easily can predict the future position of a ball in flight, is how we want you to think of your enterprise knowledge graph. The EKG is a machine that will help you predict what actions you can take to best serve your customers. That is one of the best reasons to build them. We use the metaphor of the EKG as your companies \"brain\" carefully. Because the current generation of graph databases really are very different than a human brain. Neurons in a human brain are very different from a CPU thread stepping through vertexes and edges of a graph. A typical enterprise graph has about six connections between a vertex and neighboring vertices. A human brain has around 10,000 connections for every neuron. An EKG can be inspired by the human brain just like plans are inspired by birds and submarines are inspired by fish. Systems Thinking Questions Should business units be charged for use of an EKG? How would change-backs be calculated? What is the role of innovation in your organization. Does your organization manage a list of ideas from all employees? Are the ideas sorted by cost-benefit analysis? How would you describe key events in technology and their relationship to cost-benefit analysis? What is the relationship between the Chief Executive Officer and the Chief Financial Officer in your organization? Are finance staff trained on valuing intangible benefits? How is overall knowledge-capture valued in your organization? Do employees get a bonus for sharing codifiable knowledge?","title":"Ch 2 Costs & Benefits"},{"location":"intro/ekg-cost-benefit/#chapter-2-why-build-an-enterprise-knowledge-graph","text":"EKG Use Case Taxonomy Cost-Benefit Analysis Metcalf's Law and Network Effects Business Value Measuring EKG costs Measuring EKG tangible benefits Measuring EKG intangible benefits Brains are prediction machines When we build any large resource for an organization we are going to need to ask for money to build it. If you work for a company like Apple and you are respected expert at building enterprise knowledge graphs, you many not need to go to a finance committee and justify your spending requests. Apple hires world experts in specific fields and then trusts their judgement. As long as you have a good track record they will keep giving you funding without asking a lot of questions. But most of us don't work at Apple and we are not world leaders with a long successful track record of building EKGs. We will have to fight for every penny we spend on our EKG pilot projects until the ROI is so clear to everyone that the finance people are begging you to expand the scope of the EKG. Until that happens, we are going to need to learn to speak the language of finance to get our EKGs off the ground.","title":"Chapter 2: Why Build An Enterprise Knowledge Graph?"},{"location":"intro/ekg-cost-benefit/#ekg-use-case-taxonomy","text":"A Taxonomy of Graph Use Cases Performance Fast Relationship Traversals (no JOINS) Easy to write complex queries ** Easy to perform deep link analysis Flexibility Rules","title":"EKG Use Case Taxonomy"},{"location":"intro/ekg-cost-benefit/#metcalfs-law-and-network-effects","text":"","title":"Metcalf's Law and Network Effects"},{"location":"intro/ekg-cost-benefit/#cost-benefit-analysis","text":"[The Business Value of Computers] Wikipedia on Business Value Peter Drucker","title":"Cost-Benefit Analysis"},{"location":"intro/ekg-cost-benefit/#measuring-costs","text":"Cost of extracting knowledge from operational source systems Change data-capture (CDC) Publishing business events","title":"Measuring Costs"},{"location":"intro/ekg-cost-benefit/#the-easy-to-measure-benefits","text":"Integrated views of anything Integrated views of your customers Centralized business Rules Recommendation systems","title":"The Easy to Measure Benefits"},{"location":"intro/ekg-cost-benefit/#difficult-to-measure-benefits","text":"The value of insights Tracking early insights Predicting future insights How are innovations and patents valued in your organization? Focus on datasets that generate shared value","title":"Difficult to Measure Benefits"},{"location":"intro/ekg-cost-benefit/#brains-are-prediction-machines","text":"Pick up a ball. Throw it in the air and catch it. A simple act. But to perform this simple act our brain has to do incredible things. Most importantly, your brain will predict where the ball is going so you can move your hands to catch it. And it does this task effortlessly in real time. You don't really have to think about it. This metaphor, of the brain as a prediction machine that easily can predict the future position of a ball in flight, is how we want you to think of your enterprise knowledge graph. The EKG is a machine that will help you predict what actions you can take to best serve your customers. That is one of the best reasons to build them. We use the metaphor of the EKG as your companies \"brain\" carefully. Because the current generation of graph databases really are very different than a human brain. Neurons in a human brain are very different from a CPU thread stepping through vertexes and edges of a graph. A typical enterprise graph has about six connections between a vertex and neighboring vertices. A human brain has around 10,000 connections for every neuron. An EKG can be inspired by the human brain just like plans are inspired by birds and submarines are inspired by fish.","title":"Brains are Prediction Machines"},{"location":"intro/ekg-cost-benefit/#systems-thinking-questions","text":"Should business units be charged for use of an EKG? How would change-backs be calculated? What is the role of innovation in your organization. Does your organization manage a list of ideas from all employees? Are the ideas sorted by cost-benefit analysis? How would you describe key events in technology and their relationship to cost-benefit analysis? What is the relationship between the Chief Executive Officer and the Chief Financial Officer in your organization? Are finance staff trained on valuing intangible benefits? How is overall knowledge-capture valued in your organization? Do employees get a bonus for sharing codifiable knowledge?","title":"Systems Thinking Questions"},{"location":"intro/ekg-timeline/","text":"Enterprise Knowledge Graph Timeline Putting graph representations of knowledge in historical context Here is a timeline of some of significant events that have led to the creation of the enterprise knowledge graph. We focus on how the representation of knowledge impacted the way that problems were solved. We focus on understanding the momentum of various design patterns and the innovations that allowed scale-out knowledge representations. First Forms of Knowledge Storage: Brain Evolution Lets take a step back and see how brains evolved to store knowledge. Centralized nervous systems may have first evolved around 521 million years ago. They probably evolved from simple Nerve Networks which consisted of specialized cells that worked together to process information. All networks that process information can be modeled by graph databases. What is interesting to note is that higher life forms never have evolved information processing systems that use tabular data storage. This is an artificial construct that evolved from writing systems. Wikipedia page on Brain Evolution Cuniform Tablets Around 3,000 BCE in the fertile crescent, a group of farmers started to record transactions as rows in clay tablets. Despite its limitations in storing relationships, for the next 5,000 years, tabular representations of data became the dominant way to store and transmit knowledge. This was the first known case of persistent data representation. Although the transactions were initially simple, they started in motion a long change of representing data in tabular structures. A process that still exists today. Hero of Alexandria Hero is known to have created the first devices that store information on mechanical devices that control the playback of effects such as sounds. This was one of the first forms of machine readable knowledge. Hero of Alexandria Book of Ingenious Devices A large illustrated work on mechanical devices, including automata, published in 850 by the three brothers of Persian descent, known as the Banu Musa. This book was a collection of best practices for storing and playing back information. Book of Ingenious Devices Looms Basile Bouchon developed the punch card as a control for looms in 1725. Leonhard Euler Invents Graph Theory In 1736 Euler invented graph theory while pondering the problem of walking around the city of Konisberg. By reducing the topology of the city, the rivers and islands to a graph he showed that you could not walk through the city and cross each of those bridges once and only once. ![] Seven Bridge of Konisberg Punched Cards In 1890 Herman Hollerith developed punched cards for use in commercial data processing. These cards became the backbone of machine readable information storage until core memory was invented. It is interesting to note that both programs and data were stored on punch cards. Early batch programmers would submit both data decks and program decks together and return the following day to see the results of their programs often printed in additional card decks. At the time, storing data on rectangular cards was an extremely practical decision. It made it easy to build hardware to store and read information. An unfortunate consequence was that for the next 125 years many people were taught that data within a computer's memory must also have this format. Wikipedia on Punched Cards Flat File Databases The earliest databases were usually modeled using the same data structure as punch cards: data loaded one row at a time. The layout of fields within the cards might be fixed with or variable width depending on the type of data being stored. The predominant language for information processing was COBOL for business and FORTRAN for scientific computing. Both these languages used functions that read and wrote flat file data. Wikipedia Flat File Database History Relations Added to Flat Files In 1970 the concept of adding relationships to flat files was codified into a new type of database appropriately called \"Relational\" databases. Although the name \"relational\" was used, fast relationship traversal were not a primary concern. The system required all rows of both tables to be fist analyzed before the relationships could be discovered when a query was executed. We should note that here were several early efforts to create databases that relied on fast in-memory pointer management. These were called Navigational Databases because to follow relationships you had to navigate a network of pointer references. Although many of the concepts in these databases continued to mature in path traversal languages such as XPath and XQuery , they did not catch on due to the difficulty of migrating away from the reliance on flat files as the predominant way of storing data. Wikipedia Navigational Database Wikipedia Relational Database History Resource Description Format (RDF) The need to store relationships between information was addressed by Ramanathan V. Guha and Tim Bray starting around 1995. A first public draft of RDF appeared in October 1997 as part of standards promoted by the World Wid Web consortium. The RDF data model was a brilliant development but was widely misunderstood due to poor understanding of its potential. Many thought it was only for storing metadata or for storing data within a database. In reality it's strength was a robust standard method for serializing complex knowledge in a portable format. The Semantic Web In May of 2001 Scientific American published an article on the Semantic Web. This article launched and entire industry of companies that wanted to use connected data to power AI systems. The standards did allow anyone on the web to publish knowledge that could be easily linked into an knowledge graph. But the stack of tools built on top of RDF had many issues with sustainability and have never gained widespread commercial adoption in most large corporations. Neo4j: The First Labeled Property Graph In May of 2007 Emil Eifrem committed the first labeled property graph data model as an open-source in-memory Java library. Version 1.0 of the Neo4j database was released in February 2010. Unlike the RDF model that did not allow relationships to have properties, the LPG model allowed every relationship to have any number of properties. This meant that adding a property to a relationship did not require you to refactor the graph structure (Reification) causing all queries to be rewritten. We finally had a sustainable data model that allowed the models to gracefully grow as complexity grew. Neo4j also kept their in-memory model simple which allowed simple pointer hopping operations to be used to traverse the graph. This is a key to performance and will enable future hardware optimizations. Neo4j First GitHub Commit May 2007 The Google Knowledge Graph On May 16, 2012, Google published the \"Things Not Strings\" blog post. Now the world knew that graphs were no longer an academic interest. Google's graph serviced millions of requests per minute and was available 24X7. Knowledge Graphs became cool. Introducing the Knowledge Graph: things, not strings TigerGraph Releases the First Distributed LPG Although the Neo4j system was innovative for it's time, it was built around a memory model that did not include queries that spanned multiple servers. In 2017 TigerGraph release a version of a native labeled property graph that was designed from scratch to take into account distributed queries. This allowed it to have robust scale-out performance to large clusters to meet the needs of large enterprise-scale applications.","title":"Timeline"},{"location":"intro/ekg-timeline/#enterprise-knowledge-graph-timeline","text":"Putting graph representations of knowledge in historical context Here is a timeline of some of significant events that have led to the creation of the enterprise knowledge graph. We focus on how the representation of knowledge impacted the way that problems were solved. We focus on understanding the momentum of various design patterns and the innovations that allowed scale-out knowledge representations.","title":"Enterprise Knowledge Graph Timeline"},{"location":"intro/ekg-timeline/#first-forms-of-knowledge-storage-brain-evolution","text":"Lets take a step back and see how brains evolved to store knowledge. Centralized nervous systems may have first evolved around 521 million years ago. They probably evolved from simple Nerve Networks which consisted of specialized cells that worked together to process information. All networks that process information can be modeled by graph databases. What is interesting to note is that higher life forms never have evolved information processing systems that use tabular data storage. This is an artificial construct that evolved from writing systems. Wikipedia page on Brain Evolution","title":"First Forms of Knowledge Storage: Brain Evolution"},{"location":"intro/ekg-timeline/#cuniform-tablets","text":"Around 3,000 BCE in the fertile crescent, a group of farmers started to record transactions as rows in clay tablets. Despite its limitations in storing relationships, for the next 5,000 years, tabular representations of data became the dominant way to store and transmit knowledge. This was the first known case of persistent data representation. Although the transactions were initially simple, they started in motion a long change of representing data in tabular structures. A process that still exists today.","title":"Cuniform Tablets"},{"location":"intro/ekg-timeline/#hero-of-alexandria","text":"Hero is known to have created the first devices that store information on mechanical devices that control the playback of effects such as sounds. This was one of the first forms of machine readable knowledge. Hero of Alexandria","title":"Hero of Alexandria"},{"location":"intro/ekg-timeline/#book-of-ingenious-devices","text":"A large illustrated work on mechanical devices, including automata, published in 850 by the three brothers of Persian descent, known as the Banu Musa. This book was a collection of best practices for storing and playing back information. Book of Ingenious Devices","title":"Book of Ingenious Devices"},{"location":"intro/ekg-timeline/#looms","text":"Basile Bouchon developed the punch card as a control for looms in 1725.","title":"Looms"},{"location":"intro/ekg-timeline/#leonhard-euler-invents-graph-theory","text":"In 1736 Euler invented graph theory while pondering the problem of walking around the city of Konisberg. By reducing the topology of the city, the rivers and islands to a graph he showed that you could not walk through the city and cross each of those bridges once and only once. ![] Seven Bridge of Konisberg","title":"Leonhard Euler Invents Graph Theory"},{"location":"intro/ekg-timeline/#punched-cards","text":"In 1890 Herman Hollerith developed punched cards for use in commercial data processing. These cards became the backbone of machine readable information storage until core memory was invented. It is interesting to note that both programs and data were stored on punch cards. Early batch programmers would submit both data decks and program decks together and return the following day to see the results of their programs often printed in additional card decks. At the time, storing data on rectangular cards was an extremely practical decision. It made it easy to build hardware to store and read information. An unfortunate consequence was that for the next 125 years many people were taught that data within a computer's memory must also have this format. Wikipedia on Punched Cards","title":"Punched Cards"},{"location":"intro/ekg-timeline/#flat-file-databases","text":"The earliest databases were usually modeled using the same data structure as punch cards: data loaded one row at a time. The layout of fields within the cards might be fixed with or variable width depending on the type of data being stored. The predominant language for information processing was COBOL for business and FORTRAN for scientific computing. Both these languages used functions that read and wrote flat file data. Wikipedia Flat File Database History","title":"Flat File Databases"},{"location":"intro/ekg-timeline/#relations-added-to-flat-files","text":"In 1970 the concept of adding relationships to flat files was codified into a new type of database appropriately called \"Relational\" databases. Although the name \"relational\" was used, fast relationship traversal were not a primary concern. The system required all rows of both tables to be fist analyzed before the relationships could be discovered when a query was executed. We should note that here were several early efforts to create databases that relied on fast in-memory pointer management. These were called Navigational Databases because to follow relationships you had to navigate a network of pointer references. Although many of the concepts in these databases continued to mature in path traversal languages such as XPath and XQuery , they did not catch on due to the difficulty of migrating away from the reliance on flat files as the predominant way of storing data. Wikipedia Navigational Database Wikipedia Relational Database History","title":"Relations Added to Flat Files"},{"location":"intro/ekg-timeline/#resource-description-format-rdf","text":"The need to store relationships between information was addressed by Ramanathan V. Guha and Tim Bray starting around 1995. A first public draft of RDF appeared in October 1997 as part of standards promoted by the World Wid Web consortium. The RDF data model was a brilliant development but was widely misunderstood due to poor understanding of its potential. Many thought it was only for storing metadata or for storing data within a database. In reality it's strength was a robust standard method for serializing complex knowledge in a portable format.","title":"Resource Description Format (RDF)"},{"location":"intro/ekg-timeline/#the-semantic-web","text":"In May of 2001 Scientific American published an article on the Semantic Web. This article launched and entire industry of companies that wanted to use connected data to power AI systems. The standards did allow anyone on the web to publish knowledge that could be easily linked into an knowledge graph. But the stack of tools built on top of RDF had many issues with sustainability and have never gained widespread commercial adoption in most large corporations.","title":"The Semantic Web"},{"location":"intro/ekg-timeline/#neo4j-the-first-labeled-property-graph","text":"In May of 2007 Emil Eifrem committed the first labeled property graph data model as an open-source in-memory Java library. Version 1.0 of the Neo4j database was released in February 2010. Unlike the RDF model that did not allow relationships to have properties, the LPG model allowed every relationship to have any number of properties. This meant that adding a property to a relationship did not require you to refactor the graph structure (Reification) causing all queries to be rewritten. We finally had a sustainable data model that allowed the models to gracefully grow as complexity grew. Neo4j also kept their in-memory model simple which allowed simple pointer hopping operations to be used to traverse the graph. This is a key to performance and will enable future hardware optimizations. Neo4j First GitHub Commit May 2007","title":"Neo4j: The First Labeled Property Graph"},{"location":"intro/ekg-timeline/#the-google-knowledge-graph","text":"On May 16, 2012, Google published the \"Things Not Strings\" blog post. Now the world knew that graphs were no longer an academic interest. Google's graph serviced millions of requests per minute and was available 24X7. Knowledge Graphs became cool. Introducing the Knowledge Graph: things, not strings","title":"The Google Knowledge Graph"},{"location":"intro/ekg-timeline/#tigergraph-releases-the-first-distributed-lpg","text":"Although the Neo4j system was innovative for it's time, it was built around a memory model that did not include queries that spanned multiple servers. In 2017 TigerGraph release a version of a native labeled property graph that was designed from scratch to take into account distributed queries. This allowed it to have robust scale-out performance to large clusters to meet the needs of large enterprise-scale applications.","title":"TigerGraph Releases the First Distributed LPG"},{"location":"intro/introduction/","text":"Chapter 1: What is an Enterprise Knowledge Graph Knowledge is Power - Imam Ali (AD 599-661) EKGs are becoming the central nervous system of an organization Graphs are a NoSQL architectural patterns Defining enterprise knowledge graphs The Role of architectural scalability Growth Rates of graph databases technology Scale out graph database hardware Scale out graph query languages EKGs are the central nervous system of an organization Knowledge is an invisible force that binds parts of an organization together. When managed correctly, enterprise knowledge can become the central nervous system that responds to customer needs and continually evolves to create new insights and new products. This is a vision of the Enterprise Knowledge Graph. A single entity that becomes the continually evolving brain of a dynamic organization. The technology that forms the foundation of EKGs, the scalable graph database, has only become cost effective in the last few years. As scalable graph technology matures it is being used with both existing sematic graph technologies and new emerging technologies like machine learning, natural language processing and advanced graph algorithms. How EKGs take root in an organization and mature is not an exact science. Creating sustainable EKGs usually takes takes large teams with executive support, strong leadership, open minded-staff and clearly communicated vision of the future. In this chapter we will provide you a high-level overview of what EKGs are and why scale-out graph databases are necessary for their sustainability. Graphs are a NoSQL Architectural Patterns Enterprise Knowledge Graphs (EKGs) are a type of graph database that are designed to scale to meet the demanding requirements of large organizations. Graph databases are just one type of NoSQL database architectural patten that we use to solve business problems. EKGs are also frequently used in combination with other NoSQL databases since each architecture has its strengths and weaknesses. If you are a solution architect your job is to work with business units to find how to correctly match business problems to the right NoSQL database architecture pattern. This book will help you determine if EKGs might be a good fit for one or more of your business areas. Although they have become popular, it is critical that we don't begin to think EKGs are the only solution to business problems. Solution architects must have a strong understanding of all NoSQL architecture patterns to keeping recommendations objective. They must know the pros and cons of each of these architectures. Although it is out of the scope of this book to do a deep dive into each of the six architectural pattern, we present each of them here briefly so you can explore more on your own. The six types of NoSQL database architectural patterns are described in Figure 1.1. The six key architectures are: Relational - where data is stored as fixed format rows in tables and new data is added one row at a time. Relationships between tables are calculated at query time using JOIN statements that require central indexes to be used to traverse the relationships. Ironically, relational databases put relationship traversal as a secondary concern and don't optimize the design for fast traversal of billions of relationships. Because relationships evolved as an \"add on feature\" to COBOL flat files running on mainframes, the focus of a relational database is fast atomic transactions on row-oriented data. Also known as row-stores. Analytical - where data is stored in centralized fact tables with simple relationships to dimensional tables. Dimensions each represent a way you classify the facts in the fact table. Analytical databases severely restrict the number of JOIN operations to optimise performance but force everyone to agree on the dimensions used to classify data. Analytical databases tend to be the most difficult to use between departments since the denormalization process can be very specific to a single departments view of the enterprise. Key-value Store - a very simple type of data storage system that is deliberately designed to be simple so it can easily scale. Key-value stores have a simple API (store, get and delete) and the salient fact is that you cannot efficiently query the content of a value. It is considered a back box or \"binary blob\" of information. Because if the simple design of key-value stores they are easy to distribute over a large cluster of computers and are cost effective when measured annual cost per terabyte per year. They are an ideal complement to EKGs since EKGs focus on minimizing RAM usage. EKGs frequently store just the key portion of items such as images or document references. Column-family Store - these databases are similar to key-value stores but they partition the key into multiple components such as a row and column portion of the key. A spreadsheet is a good example where a key is a cell at to get to the cell you must have the row and column identifiers of the cell. Graph Database - a database composed of vertices and edges, both with attributes, where relationship traversal is a primary concern, not an afterthought. The graph database architecture pattern is the foundation for all enterprise knowledge graphs. Document Store - a database that is composed of recursive tree-like structures where the atomic unit of storage is branches and leaves of this tree. Document stores proved path-like query languages to reach any points in a tree using simple path expressions that may contain wildcards. Document stores are ideal when you have hierarchial tree-like data such as documents or serialized objects that contain other objects. Both XML and JSON are considered data models to serialize document structures. Document stores are ideal for document search and retrieval since they can use document structure to aid in relevancy ranking. These six database architecture patterns are frequently used with the Architecture Tradeoff Analysis Method (ATAM) to help organizations have a transparent discussion on the pros and cons of each alternative. A high-level overview of the ATAM process is shown in Figure 1.2: Defining Enterprise Knowledge Graphs In this book, we define Enterprise Knowledge Graphs as the following: An Enterprise Knowledge Graph is a scalable graph database that stores information from two or more departments of an organization. You will note that this definition is both somewhat general and very specific. It is general because it includes many graph projects using a wide variety of technologies. It is very specific in that it requires whatever graph database being used to have scalable technology under the hood. We have very specific rules about what we mean by enterprise scalable. Unfortunately, this eliminates most departmental graph projects in use today that don't use true scale-out technologies. Why do we define EKGs with scalability as a prerequisite? Because to truly meet the future needs of a large enterprise we must build our graph pilot projects on an infrastructure that will not fall over and die as it grows beyond the initial pilot phase. I have seen many well intentioned software architects claim to be building enterprise-scalable graph databases only to have the projects fail due to performance problems as they grow beyond their early stages. We also exclude knowledge graphs that are only attempting to solve problems for a specific department or specific business unit. These can still be valuable projects for an enterprise since they can help individuals and departments learn the capabilities of graph databases. But we would still classify them as departmental graph project, not enterprise-scale graph projects. Now let's take a closer look at what we mean by enterprise scalable. Enterprise Scalable Knowledge Graphs Scalability is an inherent characteristic of any database architecture. It implies that as the size and complexity of the databases grows, the architecture must accommodate this growth without rewriting the core applications. Although unexpected performance problems with specific queries might occur as the size of your database grows, they should be fixable by doing simple query optimization. To be a true EKG pilot you should never have to redo your architecture from scratch. You should be able to know that the exact same queries should run on ten thousand vertex graph and a trillion vertex graph. The key aspect of scalable graph databases is the ability to distribute a graph over a large number of independent but closely connected servers in a data center. As demands grows, the number of servers in the cluster must easily scale without users or operations being impacted. As you add new servers data must automatically be redistributed over the new servers in the cluster. Scalability Means the Four Vs Volume, velocity, variability and veracity are considered the four Vs that define scalable systems. Volume refers to the total amount of data in our knowledge graph. For example enterprise-scale graphs may easily contain over 10 billion vertices and 50 billion edges. Velocity means that new inserts, updates and deletes might be coming in fast via streaming events and these events must be ACID compliant and still never slow down read access times. Service levels agreements (SLAs) must focus not on total average times, but the averages of the slowest 5% of the transactions. Variability means that data is not uniform and can be easily stuffed into a single fact table of an OLAP cube. Anyone should be able to add new assertions to the graph without ever needing to rewrite queries. This property is called agility and sometime sustainability Veracity means we need to be able to validate the quality of incoming data in real-time and quickly raise warning flags if corrupt data is being transmitted into our EKG. Data quality may not be important for small departmental graph projects but becomes critical as you merge data from hundreds of different data sources. Note: We avoid using the term \"[Big Data](../glossary.md#big-data\" in this book. It is an ambiguous statement of a problem and adds no insight into a solution. Scalability Means Automatic Sharding and Rebalancing The first criteria is that when the size of the enterprise graph grows to take on a new department or a new project, new hardware can be installed and the database is smart enough to use the new hardware without extensive pain and suffering by the database staff. This is illustrated in Figure 1.2. So to be brutally honest, if your graph database does not automatically rebalance data as new nodes are added or removed from a distributed graph cluster I don't consider it a true enterprise graph solution. It may still solve important business problems, but it does not fit our definition for this book. Scalability Means Scalable Access Control Many graph products claim to have performance scale out capabilities, but their software falls down when we give them a a detailed list of what roles can access what data. This means they might be able to scale their data to 100 nodes, but only a single role of \"admin\" can be defined for the graph. They provide an all-or nothing approach to security. This model works in small applications where access to the application is controlled by security access rules. However it breaks down when I want 100 teams to only be able to run queries on their own team's data. We will provide more examples of vertex-level role-based access control in Chapter 3 Scalability Means High Availability Many graph products are perfect for a small team in a single city that work 9am to 5pm. Upgrades can be done at night and on weekends. But Enterprise class systems don't have users in a single city. Employees and customers are all over the world. That means when we upgrade our database with a new version of software we can't shut it down. We need distributed servers that can take a single node down, upgrade the software and get it back into the cluster without ever dropping a single transaction. Scalability Means Ease of Creating Distributed Queries If you are building enterprise-class knowledge graphs you may need to support 100+ concurrent developer all writing queries on a graph distributed over 100+ nodes in a cluster. These queries need to efficiently distribute their work over each node and bring back just the data that is relevant to the results. For example if you run a count of customers that have purchased a specific product, that query needs to distribute the query to each node and have each node return a simple count to the server where the query originated. Those counts are summed together and returned to the user. This type of query is sometimes called a map-reduce query since only the counts (reduced data) are returned to the origin server. The actual customer data never never needs to move around. Scalability Means Resource Quotas In a small project graph, you often have control and review processes of the code that each developer is working on. In an enterprise setting you can't control everyone's code. You need to assume that user will write run-away queries or just run queries that are not optimized for performance during the development and testing phases of query development. You enterprise graph needs to be able to monitor rouge queries and shut them down when vertex counts, edges counts, CPU counts or memory resources reach reasonable limits. Without these resource quotas a single query can take down an entire enterprise graph database. So we need fine-grain controls in query resources, even when the queries may be running on 100 different nodes in a cluster. Scalability Means Scalable Metadata Management For small project graphs, you many only have a few hundred vertices, edges and attributes. For enterprise-scale graphs you may have thousands of these items and staff that are new to the enterprise knowledge graph need to be able to quickly search for the structures they are interested in. Once they find the items of interest they need to understand their meaning and what the various code within value sets mean (reference data). New users also many need to ask the data steward for each attribute questions such as where did the data come from, what assumptions were made about the data if if the dataset contains any sensitive information that might restrict who can view the data. Scalability Needs to Be Cost Effective and Sustainable I have worked on several enterprise data warehouse projects that use relational database to perform complex queries that required many JOIN operations. Both Inman and Kimball data warehouse methodologies use relational databases and JOIN operations as their main vehicle for deployment. Most of the initial projects work fine, but as databases grow in complexity, the number of JOIN operations increase and the performance of the complex queries slowed down. The number of people who can confidently write 25-level JOIN queries is also much smaller then those that can write 10-level JOINs. Even the relational database that are built around priopirateary vendor technology that do many tricks with custom FPGA hardware to minimize the impact of JOIN operations become slower over time. But in the end, the cost most of these enterprise data warehouse systems exceeded the value of the insight gained from the systems. Dispute spending hundreds of millions of dollars on attempting to build enterprise scale analytics systems, they get decommissioned. The costs exceeded the benefits of the system. Our goal when building EKGs is to keep a sharp focus on both the costs of these systems and continuing to provide undisputable evidence of value to our business stakeholders. We will explore this topic in the chapter on EKG Cost Benefit Analysis . EKGs and Knowledge Architecture Although we know that EKGs are designed to scale to encompass many parts of an organization, we need to be humble and realize that not all knowledge in your organization can be easily transferred into machine readable components of your EKG. This is because most knowledge is also highly contextual and for the near term future, will mostly reside in the brains of our employees. Tacit and Codifiable Knowledge Knowledge can be divided into two forms: Tacit Knowledge is the type of knowledge that only resides in our employees brains. It can not be easily converted into structures within our knowledge graph such as taxonomies, ontologies, decision trees and inference rules. Codifiable Knowledge ../glossary.md#codifiable-knowledge) is knowledge that can be converted into some for of machine readable representation. These ultimately become coded as a set of vertices, edge and attributes within our EKG or other system. Knowledge Management The overall Knowledge Architecture of any organization is driven by a firms approach to Knowledge Management. We define Knowledge Management as the combination of three disciplines: Human Resource Management - How to we create policies to encourage employees to share their knowledge? Do we make it easy for them to create and share knowledge? Do we give bonuses for employees that create and maintain content in a wiki or a EKG rules system? If employees contribute many highly rated answers to questions on web sites like StackOverflow should they get a bonus? Should authors of blogs that get a high number of views and show sustained though leadership in your industry get higher pay? Library Science - How do we organize our knowledge? So we have official company approved taxonomies and ontologies? Do we have product taxonomies? Do we manually tag content based on the preferred labels in our taxonomies? Do we analyze public taxonomies and our competitive taxonomies to see if their organizational structures give them a competitive advantage. Search - how does our organization create search systems for our knowledge? Do we use keyword only search or do we use our company taxonomies and ontologies to understand how alternate terms are used to connect to related concepts. Do we use NLP and automatic document classification to associated documents with the right concepts, products and product managers? Can product managers get automatic notification if a document, job posting or competitive product is added to the knowledge repository? You can see that the EKG we design must take our companies overall enterprise knowledge management strategies into account. EKGs can reinforce classification and search strategies and by connecting content to our employees we can create metrics for creating bonuses for employees that contribute valuable knowledge. Systems Thinking What other disciplines might impact how you design and grow your EKG? Is a narrow range of computer science skills enough to create a successful knowledge graph? How can you use your knowledge of the human brain to promote your EKG? What are the limitations of using brain metaphors in this process? How do industry standards impact the way you represent knowledge in your organization?","title":"Ch 1 Introduction"},{"location":"intro/introduction/#chapter-1-what-is-an-enterprise-knowledge-graph","text":"Knowledge is Power - Imam Ali (AD 599-661) EKGs are becoming the central nervous system of an organization Graphs are a NoSQL architectural patterns Defining enterprise knowledge graphs The Role of architectural scalability Growth Rates of graph databases technology Scale out graph database hardware Scale out graph query languages","title":"Chapter 1: What is an Enterprise Knowledge Graph"},{"location":"intro/introduction/#ekgs-are-the-central-nervous-system-of-an-organization","text":"Knowledge is an invisible force that binds parts of an organization together. When managed correctly, enterprise knowledge can become the central nervous system that responds to customer needs and continually evolves to create new insights and new products. This is a vision of the Enterprise Knowledge Graph. A single entity that becomes the continually evolving brain of a dynamic organization. The technology that forms the foundation of EKGs, the scalable graph database, has only become cost effective in the last few years. As scalable graph technology matures it is being used with both existing sematic graph technologies and new emerging technologies like machine learning, natural language processing and advanced graph algorithms. How EKGs take root in an organization and mature is not an exact science. Creating sustainable EKGs usually takes takes large teams with executive support, strong leadership, open minded-staff and clearly communicated vision of the future. In this chapter we will provide you a high-level overview of what EKGs are and why scale-out graph databases are necessary for their sustainability.","title":"EKGs are the central nervous system of an organization"},{"location":"intro/introduction/#graphs-are-a-nosql-architectural-patterns","text":"Enterprise Knowledge Graphs (EKGs) are a type of graph database that are designed to scale to meet the demanding requirements of large organizations. Graph databases are just one type of NoSQL database architectural patten that we use to solve business problems. EKGs are also frequently used in combination with other NoSQL databases since each architecture has its strengths and weaknesses. If you are a solution architect your job is to work with business units to find how to correctly match business problems to the right NoSQL database architecture pattern. This book will help you determine if EKGs might be a good fit for one or more of your business areas. Although they have become popular, it is critical that we don't begin to think EKGs are the only solution to business problems. Solution architects must have a strong understanding of all NoSQL architecture patterns to keeping recommendations objective. They must know the pros and cons of each of these architectures. Although it is out of the scope of this book to do a deep dive into each of the six architectural pattern, we present each of them here briefly so you can explore more on your own. The six types of NoSQL database architectural patterns are described in Figure 1.1. The six key architectures are: Relational - where data is stored as fixed format rows in tables and new data is added one row at a time. Relationships between tables are calculated at query time using JOIN statements that require central indexes to be used to traverse the relationships. Ironically, relational databases put relationship traversal as a secondary concern and don't optimize the design for fast traversal of billions of relationships. Because relationships evolved as an \"add on feature\" to COBOL flat files running on mainframes, the focus of a relational database is fast atomic transactions on row-oriented data. Also known as row-stores. Analytical - where data is stored in centralized fact tables with simple relationships to dimensional tables. Dimensions each represent a way you classify the facts in the fact table. Analytical databases severely restrict the number of JOIN operations to optimise performance but force everyone to agree on the dimensions used to classify data. Analytical databases tend to be the most difficult to use between departments since the denormalization process can be very specific to a single departments view of the enterprise. Key-value Store - a very simple type of data storage system that is deliberately designed to be simple so it can easily scale. Key-value stores have a simple API (store, get and delete) and the salient fact is that you cannot efficiently query the content of a value. It is considered a back box or \"binary blob\" of information. Because if the simple design of key-value stores they are easy to distribute over a large cluster of computers and are cost effective when measured annual cost per terabyte per year. They are an ideal complement to EKGs since EKGs focus on minimizing RAM usage. EKGs frequently store just the key portion of items such as images or document references. Column-family Store - these databases are similar to key-value stores but they partition the key into multiple components such as a row and column portion of the key. A spreadsheet is a good example where a key is a cell at to get to the cell you must have the row and column identifiers of the cell. Graph Database - a database composed of vertices and edges, both with attributes, where relationship traversal is a primary concern, not an afterthought. The graph database architecture pattern is the foundation for all enterprise knowledge graphs. Document Store - a database that is composed of recursive tree-like structures where the atomic unit of storage is branches and leaves of this tree. Document stores proved path-like query languages to reach any points in a tree using simple path expressions that may contain wildcards. Document stores are ideal when you have hierarchial tree-like data such as documents or serialized objects that contain other objects. Both XML and JSON are considered data models to serialize document structures. Document stores are ideal for document search and retrieval since they can use document structure to aid in relevancy ranking. These six database architecture patterns are frequently used with the Architecture Tradeoff Analysis Method (ATAM) to help organizations have a transparent discussion on the pros and cons of each alternative. A high-level overview of the ATAM process is shown in Figure 1.2:","title":"Graphs are a NoSQL Architectural Patterns"},{"location":"intro/introduction/#defining-enterprise-knowledge-graphs","text":"In this book, we define Enterprise Knowledge Graphs as the following: An Enterprise Knowledge Graph is a scalable graph database that stores information from two or more departments of an organization. You will note that this definition is both somewhat general and very specific. It is general because it includes many graph projects using a wide variety of technologies. It is very specific in that it requires whatever graph database being used to have scalable technology under the hood. We have very specific rules about what we mean by enterprise scalable. Unfortunately, this eliminates most departmental graph projects in use today that don't use true scale-out technologies. Why do we define EKGs with scalability as a prerequisite? Because to truly meet the future needs of a large enterprise we must build our graph pilot projects on an infrastructure that will not fall over and die as it grows beyond the initial pilot phase. I have seen many well intentioned software architects claim to be building enterprise-scalable graph databases only to have the projects fail due to performance problems as they grow beyond their early stages. We also exclude knowledge graphs that are only attempting to solve problems for a specific department or specific business unit. These can still be valuable projects for an enterprise since they can help individuals and departments learn the capabilities of graph databases. But we would still classify them as departmental graph project, not enterprise-scale graph projects. Now let's take a closer look at what we mean by enterprise scalable.","title":"Defining Enterprise Knowledge Graphs"},{"location":"intro/introduction/#enterprise-scalable-knowledge-graphs","text":"Scalability is an inherent characteristic of any database architecture. It implies that as the size and complexity of the databases grows, the architecture must accommodate this growth without rewriting the core applications. Although unexpected performance problems with specific queries might occur as the size of your database grows, they should be fixable by doing simple query optimization. To be a true EKG pilot you should never have to redo your architecture from scratch. You should be able to know that the exact same queries should run on ten thousand vertex graph and a trillion vertex graph. The key aspect of scalable graph databases is the ability to distribute a graph over a large number of independent but closely connected servers in a data center. As demands grows, the number of servers in the cluster must easily scale without users or operations being impacted. As you add new servers data must automatically be redistributed over the new servers in the cluster.","title":"Enterprise Scalable Knowledge Graphs"},{"location":"intro/introduction/#scalability-means-the-four-vs","text":"Volume, velocity, variability and veracity are considered the four Vs that define scalable systems. Volume refers to the total amount of data in our knowledge graph. For example enterprise-scale graphs may easily contain over 10 billion vertices and 50 billion edges. Velocity means that new inserts, updates and deletes might be coming in fast via streaming events and these events must be ACID compliant and still never slow down read access times. Service levels agreements (SLAs) must focus not on total average times, but the averages of the slowest 5% of the transactions. Variability means that data is not uniform and can be easily stuffed into a single fact table of an OLAP cube. Anyone should be able to add new assertions to the graph without ever needing to rewrite queries. This property is called agility and sometime sustainability Veracity means we need to be able to validate the quality of incoming data in real-time and quickly raise warning flags if corrupt data is being transmitted into our EKG. Data quality may not be important for small departmental graph projects but becomes critical as you merge data from hundreds of different data sources. Note: We avoid using the term \"[Big Data](../glossary.md#big-data\" in this book. It is an ambiguous statement of a problem and adds no insight into a solution.","title":"Scalability Means the Four Vs"},{"location":"intro/introduction/#scalability-means-automatic-sharding-and-rebalancing","text":"The first criteria is that when the size of the enterprise graph grows to take on a new department or a new project, new hardware can be installed and the database is smart enough to use the new hardware without extensive pain and suffering by the database staff. This is illustrated in Figure 1.2. So to be brutally honest, if your graph database does not automatically rebalance data as new nodes are added or removed from a distributed graph cluster I don't consider it a true enterprise graph solution. It may still solve important business problems, but it does not fit our definition for this book.","title":"Scalability Means Automatic Sharding and Rebalancing"},{"location":"intro/introduction/#scalability-means-scalable-access-control","text":"Many graph products claim to have performance scale out capabilities, but their software falls down when we give them a a detailed list of what roles can access what data. This means they might be able to scale their data to 100 nodes, but only a single role of \"admin\" can be defined for the graph. They provide an all-or nothing approach to security. This model works in small applications where access to the application is controlled by security access rules. However it breaks down when I want 100 teams to only be able to run queries on their own team's data. We will provide more examples of vertex-level role-based access control in Chapter 3","title":"Scalability Means Scalable Access Control"},{"location":"intro/introduction/#scalability-means-high-availability","text":"Many graph products are perfect for a small team in a single city that work 9am to 5pm. Upgrades can be done at night and on weekends. But Enterprise class systems don't have users in a single city. Employees and customers are all over the world. That means when we upgrade our database with a new version of software we can't shut it down. We need distributed servers that can take a single node down, upgrade the software and get it back into the cluster without ever dropping a single transaction.","title":"Scalability Means High Availability"},{"location":"intro/introduction/#scalability-means-ease-of-creating-distributed-queries","text":"If you are building enterprise-class knowledge graphs you may need to support 100+ concurrent developer all writing queries on a graph distributed over 100+ nodes in a cluster. These queries need to efficiently distribute their work over each node and bring back just the data that is relevant to the results. For example if you run a count of customers that have purchased a specific product, that query needs to distribute the query to each node and have each node return a simple count to the server where the query originated. Those counts are summed together and returned to the user. This type of query is sometimes called a map-reduce query since only the counts (reduced data) are returned to the origin server. The actual customer data never never needs to move around.","title":"Scalability Means Ease of Creating Distributed Queries"},{"location":"intro/introduction/#scalability-means-resource-quotas","text":"In a small project graph, you often have control and review processes of the code that each developer is working on. In an enterprise setting you can't control everyone's code. You need to assume that user will write run-away queries or just run queries that are not optimized for performance during the development and testing phases of query development. You enterprise graph needs to be able to monitor rouge queries and shut them down when vertex counts, edges counts, CPU counts or memory resources reach reasonable limits. Without these resource quotas a single query can take down an entire enterprise graph database. So we need fine-grain controls in query resources, even when the queries may be running on 100 different nodes in a cluster.","title":"Scalability Means Resource Quotas"},{"location":"intro/introduction/#scalability-means-scalable-metadata-management","text":"For small project graphs, you many only have a few hundred vertices, edges and attributes. For enterprise-scale graphs you may have thousands of these items and staff that are new to the enterprise knowledge graph need to be able to quickly search for the structures they are interested in. Once they find the items of interest they need to understand their meaning and what the various code within value sets mean (reference data). New users also many need to ask the data steward for each attribute questions such as where did the data come from, what assumptions were made about the data if if the dataset contains any sensitive information that might restrict who can view the data.","title":"Scalability Means Scalable Metadata Management"},{"location":"intro/introduction/#scalability-needs-to-be-cost-effective-and-sustainable","text":"I have worked on several enterprise data warehouse projects that use relational database to perform complex queries that required many JOIN operations. Both Inman and Kimball data warehouse methodologies use relational databases and JOIN operations as their main vehicle for deployment. Most of the initial projects work fine, but as databases grow in complexity, the number of JOIN operations increase and the performance of the complex queries slowed down. The number of people who can confidently write 25-level JOIN queries is also much smaller then those that can write 10-level JOINs. Even the relational database that are built around priopirateary vendor technology that do many tricks with custom FPGA hardware to minimize the impact of JOIN operations become slower over time. But in the end, the cost most of these enterprise data warehouse systems exceeded the value of the insight gained from the systems. Dispute spending hundreds of millions of dollars on attempting to build enterprise scale analytics systems, they get decommissioned. The costs exceeded the benefits of the system. Our goal when building EKGs is to keep a sharp focus on both the costs of these systems and continuing to provide undisputable evidence of value to our business stakeholders. We will explore this topic in the chapter on EKG Cost Benefit Analysis .","title":"Scalability Needs to Be Cost Effective and Sustainable"},{"location":"intro/introduction/#ekgs-and-knowledge-architecture","text":"Although we know that EKGs are designed to scale to encompass many parts of an organization, we need to be humble and realize that not all knowledge in your organization can be easily transferred into machine readable components of your EKG. This is because most knowledge is also highly contextual and for the near term future, will mostly reside in the brains of our employees.","title":"EKGs and Knowledge Architecture"},{"location":"intro/introduction/#tacit-and-codifiable-knowledge","text":"Knowledge can be divided into two forms: Tacit Knowledge is the type of knowledge that only resides in our employees brains. It can not be easily converted into structures within our knowledge graph such as taxonomies, ontologies, decision trees and inference rules. Codifiable Knowledge ../glossary.md#codifiable-knowledge) is knowledge that can be converted into some for of machine readable representation. These ultimately become coded as a set of vertices, edge and attributes within our EKG or other system.","title":"Tacit and Codifiable Knowledge"},{"location":"intro/introduction/#knowledge-management","text":"The overall Knowledge Architecture of any organization is driven by a firms approach to Knowledge Management. We define Knowledge Management as the combination of three disciplines: Human Resource Management - How to we create policies to encourage employees to share their knowledge? Do we make it easy for them to create and share knowledge? Do we give bonuses for employees that create and maintain content in a wiki or a EKG rules system? If employees contribute many highly rated answers to questions on web sites like StackOverflow should they get a bonus? Should authors of blogs that get a high number of views and show sustained though leadership in your industry get higher pay? Library Science - How do we organize our knowledge? So we have official company approved taxonomies and ontologies? Do we have product taxonomies? Do we manually tag content based on the preferred labels in our taxonomies? Do we analyze public taxonomies and our competitive taxonomies to see if their organizational structures give them a competitive advantage. Search - how does our organization create search systems for our knowledge? Do we use keyword only search or do we use our company taxonomies and ontologies to understand how alternate terms are used to connect to related concepts. Do we use NLP and automatic document classification to associated documents with the right concepts, products and product managers? Can product managers get automatic notification if a document, job posting or competitive product is added to the knowledge repository? You can see that the EKG we design must take our companies overall enterprise knowledge management strategies into account. EKGs can reinforce classification and search strategies and by connecting content to our employees we can create metrics for creating bonuses for employees that contribute valuable knowledge.","title":"Knowledge Management"},{"location":"intro/introduction/#systems-thinking","text":"What other disciplines might impact how you design and grow your EKG? Is a narrow range of computer science skills enough to create a successful knowledge graph? How can you use your knowledge of the human brain to promote your EKG? What are the limitations of using brain metaphors in this process? How do industry standards impact the way you represent knowledge in your organization? <!-- Non-published notes: Other definitions of enterprise knowledge graph: Note the definition on this page does not address the word \"enterprise\": https://help.poolparty.biz/pp/white-papers-release-notes/poolparty-technical-white-paper/an-enterprise-knowledge-graph-life-cycle-a-summary/the-enterprise-knowledge-graph-a-definition Note that Pool Party draw from employees that have a strong semantic-web-centric knowledge base. https://enterprise-knowledge.com/what-is-an-enterprise-knowledge-graph-and-why-do-i-want-one/#:~:text=An%20enterprise%20knowledge%20graph%20is,by%20both%20humans%20and%20machines. An enterprise knowledge graph is a representation of an organization\u2019s knowledge domain and artifacts that is understood by both humans and machines. It is a collection of references to your organization\u2019s knowledge assets, content, and data that leverages a data model to describe the people, places, and things and how they are related. ->","title":"Systems Thinking"},{"location":"intro/lifecycle/","text":"Enterprise Knowledge Graph Life cycles How EKGs work in practice Pilot projects Going into production Finding customers Charge backs The value of insight The First Customer The Pilot Going Into Production The Second Customer Finding Customers Charge Backs The Value of Insight","title":"Ch 3 Lifecycle"},{"location":"intro/lifecycle/#enterprise-knowledge-graph-life-cycles","text":"How EKGs work in practice Pilot projects Going into production Finding customers Charge backs The value of insight","title":"Enterprise Knowledge Graph Life cycles"},{"location":"intro/lifecycle/#the-first-customer","text":"","title":"The First Customer"},{"location":"intro/lifecycle/#the-pilot","text":"","title":"The Pilot"},{"location":"intro/lifecycle/#going-into-production","text":"","title":"Going Into Production"},{"location":"intro/lifecycle/#the-second-customer","text":"","title":"The Second Customer"},{"location":"intro/lifecycle/#finding-customers","text":"","title":"Finding Customers"},{"location":"intro/lifecycle/#charge-backs","text":"","title":"Charge Backs"},{"location":"intro/lifecycle/#the-value-of-insight","text":"","title":"The Value of Insight"},{"location":"intro/preface/","text":"Preface This book is my personal attempt to help people understand one of the most important developments in information technology: the rise of the Enterprise Knowledge Graph (EKG). I have spent most of my career helping organizations understand the strategic impact of various emerging technologies. In 2011, working with Tony Shaw at Dataversity, we created one of the first international conferences in matching business problems to the emerging market of NoSQL databases. In 2014, working with my wife, Ann Kelly, we published our book \"Making Sense of NoSQL\" which became one of the highest-rated books on the topic of NoSQL databases. Our NoSQL book was the first to propose a taxonomy of database architectures that are used to guide the solution matching process. Graph databases were one of the six solutions we found that provided a unique set of value propositions. However, at the time, graph databases didn't scale well to meet the demanding requirements of the enterprise. My research into graph databases was restricted to what could be done on small projects that ran on a single computer. In rare cases, with large R&D budgets a Cray Graph Engine. But there were so many limitations I could not see the technology as being widespread. The introduction of TigerGraph in 2017 started to change everything. Now we had a graph database that truly could gracefully scale from one to hundreds of servers to meet the demanding needs of the enterprise. TigerGraph also runs on commodity hardware and had the required security controls that would us to allow sensitive data to be loaded but not visible by everyone. This made the prospect of commercial adoption of Enterprise Knowledge Graphs viable for the first time. If you are a specialist looking for tips about a single aspect of Enterprise Knowledge Graphs, this may not be the book for you. If you are a generalist, you have come to the right place. My goal is to integrate knowledge from many fields: solution architecture, NoSQL, data warehousing, enterprise analytics, data discovery, visualization, database design, database modeling, graph algorithms, distributed systems, high-performance computing, hardware architecture, integrated circuit chip design, rules engines, schema matching, integration, data science, design patterns, machine learning, prediction, semantics, search, natural language processing, business glossaries, taxonomies, ontology management, systems thinking, complex adaptive systems, storytelling, biology, brain science, psychology, cognitive bias, philosophy, empiricism, sales, marketing, strategic planning, forecasting and finally, a bit of science fiction and futurism. I value a generalist approach and wholistic Systems Thinking to help us put EKGs into the context any organizations strategic direction. What brings all these topics together? The creation and promotion of Enterprise Knowledge Graphs and how they relate to your firms overall Enterprise Knowledge Architecture strategy. If you are interested in building an Enterprise Knowledge Graph this book is for you. At the end of many of the chapters you will see exercises on Systems Thinking that will encourage you to consider how the topics in that chapter might relate to other fields of study and other strategic directives in your organization. I know that many of the readers many not be familiar many of these topics and each is filled with complex terms that may mean things differently to you from your worldview. So I have provided an extensive Glossary of Terms with both general definitions as well as how the concepts related to Enterprise Knowledge Graphs. After reading this book, you will see that Enterprise Knowledge Graphs can provide not just a strong return on investments today, but they will become the foundation that companies will build their organization's \"enterprise brains\" in the future. Imagine opening a chatbot that knows everything about your company. All the customers, all the products, all the competitive products, all the production flows, all the employees, all the recent promotions and reorgs, all the training programs, all the documents, all the individual systems, all the reports, all the key concepts, and all the sales trends. The Enterprise Knowledge Graph is enabling all of these things today.","title":"Preface"},{"location":"intro/preface/#preface","text":"This book is my personal attempt to help people understand one of the most important developments in information technology: the rise of the Enterprise Knowledge Graph (EKG). I have spent most of my career helping organizations understand the strategic impact of various emerging technologies. In 2011, working with Tony Shaw at Dataversity, we created one of the first international conferences in matching business problems to the emerging market of NoSQL databases. In 2014, working with my wife, Ann Kelly, we published our book \"Making Sense of NoSQL\" which became one of the highest-rated books on the topic of NoSQL databases. Our NoSQL book was the first to propose a taxonomy of database architectures that are used to guide the solution matching process. Graph databases were one of the six solutions we found that provided a unique set of value propositions. However, at the time, graph databases didn't scale well to meet the demanding requirements of the enterprise. My research into graph databases was restricted to what could be done on small projects that ran on a single computer. In rare cases, with large R&D budgets a Cray Graph Engine. But there were so many limitations I could not see the technology as being widespread. The introduction of TigerGraph in 2017 started to change everything. Now we had a graph database that truly could gracefully scale from one to hundreds of servers to meet the demanding needs of the enterprise. TigerGraph also runs on commodity hardware and had the required security controls that would us to allow sensitive data to be loaded but not visible by everyone. This made the prospect of commercial adoption of Enterprise Knowledge Graphs viable for the first time. If you are a specialist looking for tips about a single aspect of Enterprise Knowledge Graphs, this may not be the book for you. If you are a generalist, you have come to the right place. My goal is to integrate knowledge from many fields: solution architecture, NoSQL, data warehousing, enterprise analytics, data discovery, visualization, database design, database modeling, graph algorithms, distributed systems, high-performance computing, hardware architecture, integrated circuit chip design, rules engines, schema matching, integration, data science, design patterns, machine learning, prediction, semantics, search, natural language processing, business glossaries, taxonomies, ontology management, systems thinking, complex adaptive systems, storytelling, biology, brain science, psychology, cognitive bias, philosophy, empiricism, sales, marketing, strategic planning, forecasting and finally, a bit of science fiction and futurism. I value a generalist approach and wholistic Systems Thinking to help us put EKGs into the context any organizations strategic direction. What brings all these topics together? The creation and promotion of Enterprise Knowledge Graphs and how they relate to your firms overall Enterprise Knowledge Architecture strategy. If you are interested in building an Enterprise Knowledge Graph this book is for you. At the end of many of the chapters you will see exercises on Systems Thinking that will encourage you to consider how the topics in that chapter might relate to other fields of study and other strategic directives in your organization. I know that many of the readers many not be familiar many of these topics and each is filled with complex terms that may mean things differently to you from your worldview. So I have provided an extensive Glossary of Terms with both general definitions as well as how the concepts related to Enterprise Knowledge Graphs. After reading this book, you will see that Enterprise Knowledge Graphs can provide not just a strong return on investments today, but they will become the foundation that companies will build their organization's \"enterprise brains\" in the future. Imagine opening a chatbot that knows everything about your company. All the customers, all the products, all the competitive products, all the production flows, all the employees, all the recent promotions and reorgs, all the training programs, all the documents, all the individual systems, all the reports, all the key concepts, and all the sales trends. The Enterprise Knowledge Graph is enabling all of these things today.","title":"Preface"},{"location":"intro/stories/","text":"Enterprise Knowledge Graph Stories These are a few stories related to enterprise knowledge graphs. Glossary to Taxonomy to Ontology to Graph Tracy had a background in library science. She was asked to help a manufacturing company organize their datasets. Stage 1: The Glossary On Tracy's first day on the job, she heard dozens of terms she had never heard before. Many of them were acronyms of internal systems and projects. She started buy writing down the terms that she didn't understand in the first column of a spreadsheet. She put the definitions of the term in the second column. She was building a business glossary. Stage 2: The Taxonomy After a while, Tracy saw some recurring patterns in her terms. Some were computer application names, some were product names and some were \"other\". She started grouping the related terms together and added another column for the category of each term. She had a concept taxonomy. Stage 3: The Ontology After a while, Tracy's categories started to grow and become more complex. Categories had sub-categories and now she started to see relationships between terms. Terms had broader terms and narrower terms. Some teams used different names (or labels) for the same concept. Tracy now had a graph of concepts. She had an ontology. Tracy could no longer maintain the system using a simple spreadsheet. She worked with her peers to create a web front end to a graph database so it was easy for anyone to add and update concepts. Stage 4: Reference Data Her graph database continued to grow. For many concepts she was asked to store a code-set that described the valid values that data element could contain. She was building a reference data set. Here reference data started simply - a list of country codes, a list of state codes and a list of regions that included states. Then they asked her to list all the cities that they sold products. Soon she was tracking all the cities in the world and their long-lat coordinates. But this was OK, because the graph database that Tracy selected scaled well as the data complexity and size grew. State 5: The Project Knowledge Graph The company like the fact that when Tracy was asked to add a new feature it was always done quickly. The data model scaled well and Tracy was not forced to rewrite queries as the data grew. As the request for more data continued Tracy added detailed product information and the customers that used these products. She then got regular updates of customer lists and their customer satisfaction surveys. Now multiple departments wanted access to Tracy's database. She became an enterprise resource. State 6: The Enterprise Knowledge Graph Now that Tracy had more and more customer data and their purchase history, Tracy had to use multiple servers to manage the data. The company also needed to access customer records 24X7 so she put in tools to automatically replicate data and automatically migrated data to new servers as they were added to the cluster. Tracy had built a highly available enterprise knowledge graph. Customer 360 For many years the company had grown by acquiring new companies. Unfortunately, the computer systems used by each company was incompatible so the customer call centers would have to put customers on hold to long into each of the 10 different computers that stored the customer information.","title":"Stories"},{"location":"intro/stories/#enterprise-knowledge-graph-stories","text":"These are a few stories related to enterprise knowledge graphs.","title":"Enterprise Knowledge Graph Stories"},{"location":"intro/stories/#glossary-to-taxonomy-to-ontology-to-graph","text":"Tracy had a background in library science. She was asked to help a manufacturing company organize their datasets.","title":"Glossary to Taxonomy to Ontology to Graph"},{"location":"intro/stories/#stage-1-the-glossary","text":"On Tracy's first day on the job, she heard dozens of terms she had never heard before. Many of them were acronyms of internal systems and projects. She started buy writing down the terms that she didn't understand in the first column of a spreadsheet. She put the definitions of the term in the second column. She was building a business glossary.","title":"Stage 1: The Glossary"},{"location":"intro/stories/#stage-2-the-taxonomy","text":"After a while, Tracy saw some recurring patterns in her terms. Some were computer application names, some were product names and some were \"other\". She started grouping the related terms together and added another column for the category of each term. She had a concept taxonomy.","title":"Stage 2: The Taxonomy"},{"location":"intro/stories/#stage-3-the-ontology","text":"After a while, Tracy's categories started to grow and become more complex. Categories had sub-categories and now she started to see relationships between terms. Terms had broader terms and narrower terms. Some teams used different names (or labels) for the same concept. Tracy now had a graph of concepts. She had an ontology. Tracy could no longer maintain the system using a simple spreadsheet. She worked with her peers to create a web front end to a graph database so it was easy for anyone to add and update concepts.","title":"Stage 3: The Ontology"},{"location":"intro/stories/#stage-4-reference-data","text":"Her graph database continued to grow. For many concepts she was asked to store a code-set that described the valid values that data element could contain. She was building a reference data set. Here reference data started simply - a list of country codes, a list of state codes and a list of regions that included states. Then they asked her to list all the cities that they sold products. Soon she was tracking all the cities in the world and their long-lat coordinates. But this was OK, because the graph database that Tracy selected scaled well as the data complexity and size grew.","title":"Stage 4: Reference Data"},{"location":"intro/stories/#state-5-the-project-knowledge-graph","text":"The company like the fact that when Tracy was asked to add a new feature it was always done quickly. The data model scaled well and Tracy was not forced to rewrite queries as the data grew. As the request for more data continued Tracy added detailed product information and the customers that used these products. She then got regular updates of customer lists and their customer satisfaction surveys. Now multiple departments wanted access to Tracy's database. She became an enterprise resource.","title":"State 5: The Project Knowledge Graph"},{"location":"intro/stories/#state-6-the-enterprise-knowledge-graph","text":"Now that Tracy had more and more customer data and their purchase history, Tracy had to use multiple servers to manage the data. The company also needed to access customer records 24X7 so she put in tools to automatically replicate data and automatically migrated data to new servers as they were added to the cluster. Tracy had built a highly available enterprise knowledge graph.","title":"State 6: The Enterprise Knowledge Graph"},{"location":"intro/stories/#customer-360","text":"For many years the company had grown by acquiring new companies. Unfortunately, the computer systems used by each company was incompatible so the customer call centers would have to put customers on hold to long into each of the 10 different computers that stored the customer information.","title":"Customer 360"},{"location":"intro/trends/","text":"Enterprise Knowledge Graph Database Trends In this chapter: DB Engines Trends Hardware Trends Enterprise Graph Vendor Trends Graph Query Language Standardization Trends Standard Graph Algorithm Trends Graph Machine Learning Trends Third Party Trends Graph Visualization Library Trends Graph Experts Salary Trends DB Engines Trends Hardware Trends Enterprise Graph Vendor Trends Graph Query Language Standardization Trends Standard Graph Algorithm Trends Graph Machine Learning Trends Third Party Trends Graph Visualization Library Trends Graph Experts Salary Trends References https://db-engines.com/en/ranking_trend/graph+dbms https://www.eweek.com/database/why-experts-see-graph-databases-headed-to-mainstream-use https://www.eweek.com/database/first-industrywide-graph-db-conference-set-for-sept.-28-30 https://www.eweek.com/innovation/top-10-technology-analysts-of-2020","title":"Ch 4 Graph Adoption Trends"},{"location":"intro/trends/#enterprise-knowledge-graph-database-trends","text":"In this chapter: DB Engines Trends Hardware Trends Enterprise Graph Vendor Trends Graph Query Language Standardization Trends Standard Graph Algorithm Trends Graph Machine Learning Trends Third Party Trends Graph Visualization Library Trends Graph Experts Salary Trends","title":"Enterprise Knowledge Graph Database Trends"},{"location":"intro/trends/#db-engines-trends","text":"","title":"DB Engines Trends"},{"location":"intro/trends/#hardware-trends","text":"","title":"Hardware Trends"},{"location":"intro/trends/#enterprise-graph-vendor-trends","text":"","title":"Enterprise Graph Vendor Trends"},{"location":"intro/trends/#graph-query-language-standardization-trends","text":"","title":"Graph Query Language Standardization Trends"},{"location":"intro/trends/#standard-graph-algorithm-trends","text":"","title":"Standard Graph Algorithm Trends"},{"location":"intro/trends/#graph-machine-learning-trends","text":"","title":"Graph Machine Learning Trends"},{"location":"intro/trends/#third-party-trends","text":"","title":"Third Party Trends"},{"location":"intro/trends/#graph-visualization-library-trends","text":"","title":"Graph Visualization Library Trends"},{"location":"intro/trends/#graph-experts-salary-trends","text":"","title":"Graph Experts Salary Trends"},{"location":"intro/trends/#references","text":"https://db-engines.com/en/ranking_trend/graph+dbms https://www.eweek.com/database/why-experts-see-graph-databases-headed-to-mainstream-use https://www.eweek.com/database/first-industrywide-graph-db-conference-set-for-sept.-28-30 https://www.eweek.com/innovation/top-10-technology-analysts-of-2020","title":"References"},{"location":"promoting/cognitive-bias/","text":"Cognitive Bias in Enterprise Knowledge Selection In this chapter we will look into the reason that organizations are not adopting enterprise knowledge graphs due to logical errors in judgement. We call these persistent patterns of error Cognitive Bias . When we combine this knowledge with our understanding of the Technology adoption life cycle and Windows of Opportunity we can start to make create a predictive model of when and organization might be ready to adopt enterprise knowledge graph technology. Being able to predict when an organization is ready to make the transition from a relational dominated world to a graph dominated world does not imply that everyone who is ready to change will successfully make the transition. Sometimes random events occur that block our evolutionary progression. When I first started to focus on solution architecture consulting, I was frequently hired by organizations that wanted to bring in an objective external consultant to help them evaluate options for a specific business project. Although our book, Making Sense of NoSQL had ample information on how to do this objective analysis, many companies still wanted an experienced outside person to oversee this processes. Although many of these projects went well, I was often disheartened when organizations didn't make the appropriate choices. I reasoned that they were making political choices, not rational choices based on evidence, and tried to wash my hands of their choices and I moved on to the next project. Then in 2013, I attended a conference sponsored by the people that developed and used the ATAM process originally developed at CMU's Software Engineering Institute. This was the CMU Saturn Conference that focused on researchers trying to understand the architecture analysis process. One of the speakers was the incredibly insightful Mary Poppendieck . Mary's presentation was all about how cognitive bias has a strong influence on how organizations select any given technology. I was thrilled to finally have a precise taxonomy of the reasons that politics drove organizational decision making. You can see the slides of my presentation on the application of ATAM to database selection here . You can see that many of the concepts in this book are present in this presentation. Since Mary's talk in 2013 I have carefully documented many of the bias I have seen in organization decision making. Here are some of them: Anchoring bias Availability bias a.k.a. memory bias, familiarity heuristic Bandwagon effect Confirmation bias a.k.a. Fiter bubble Halo effect Hindsight bias Illusory superiority bias Framing effect Narrative-bias Representativeness heuristic Silver bullet Status_quo_bias Sunk cost a.k.a. Gamblers fallacy Anchoring Bias Availability Bias Bandwagon Effect Confirmation Bias Halo Effect Hindsight Bias Illusionary Superiority Bias Framing Effect Narrative Bias Representativeness Heuristic Silver Bullet Sunk Cost","title":"Cognitive Bias"},{"location":"promoting/cognitive-bias/#cognitive-bias-in-enterprise-knowledge-selection","text":"In this chapter we will look into the reason that organizations are not adopting enterprise knowledge graphs due to logical errors in judgement. We call these persistent patterns of error Cognitive Bias . When we combine this knowledge with our understanding of the Technology adoption life cycle and Windows of Opportunity we can start to make create a predictive model of when and organization might be ready to adopt enterprise knowledge graph technology. Being able to predict when an organization is ready to make the transition from a relational dominated world to a graph dominated world does not imply that everyone who is ready to change will successfully make the transition. Sometimes random events occur that block our evolutionary progression. When I first started to focus on solution architecture consulting, I was frequently hired by organizations that wanted to bring in an objective external consultant to help them evaluate options for a specific business project. Although our book, Making Sense of NoSQL had ample information on how to do this objective analysis, many companies still wanted an experienced outside person to oversee this processes. Although many of these projects went well, I was often disheartened when organizations didn't make the appropriate choices. I reasoned that they were making political choices, not rational choices based on evidence, and tried to wash my hands of their choices and I moved on to the next project. Then in 2013, I attended a conference sponsored by the people that developed and used the ATAM process originally developed at CMU's Software Engineering Institute. This was the CMU Saturn Conference that focused on researchers trying to understand the architecture analysis process. One of the speakers was the incredibly insightful Mary Poppendieck . Mary's presentation was all about how cognitive bias has a strong influence on how organizations select any given technology. I was thrilled to finally have a precise taxonomy of the reasons that politics drove organizational decision making. You can see the slides of my presentation on the application of ATAM to database selection here . You can see that many of the concepts in this book are present in this presentation. Since Mary's talk in 2013 I have carefully documented many of the bias I have seen in organization decision making. Here are some of them: Anchoring bias Availability bias a.k.a. memory bias, familiarity heuristic Bandwagon effect Confirmation bias a.k.a. Fiter bubble Halo effect Hindsight bias Illusory superiority bias Framing effect Narrative-bias Representativeness heuristic Silver bullet Status_quo_bias Sunk cost a.k.a. Gamblers fallacy","title":"Cognitive Bias in Enterprise Knowledge Selection"},{"location":"promoting/cognitive-bias/#anchoring-bias","text":"","title":"Anchoring Bias"},{"location":"promoting/cognitive-bias/#availability-bias","text":"","title":"Availability Bias"},{"location":"promoting/cognitive-bias/#bandwagon-effect","text":"","title":"Bandwagon Effect"},{"location":"promoting/cognitive-bias/#confirmation-bias","text":"","title":"Confirmation Bias"},{"location":"promoting/cognitive-bias/#halo-effect","text":"","title":"Halo Effect"},{"location":"promoting/cognitive-bias/#hindsight-bias","text":"","title":"Hindsight Bias"},{"location":"promoting/cognitive-bias/#illusionary-superiority-bias","text":"","title":"Illusionary Superiority Bias"},{"location":"promoting/cognitive-bias/#framing-effect","text":"","title":"Framing Effect"},{"location":"promoting/cognitive-bias/#narrative-bias","text":"","title":"Narrative Bias"},{"location":"promoting/cognitive-bias/#representativeness-heuristic","text":"","title":"Representativeness Heuristic"},{"location":"promoting/cognitive-bias/#silver-bullet","text":"","title":"Silver Bullet"},{"location":"promoting/cognitive-bias/#sunk-cost","text":"","title":"Sunk Cost"},{"location":"promoting/conclusion/","text":"Conclusion Never mistake a clear view for a short distance.\u201d \u2014 Paul Saffo We now come to an end of our journey through our tour of our knowledge about enterprise knowledge graphs. We have touched on a wide number of topics and integrated concepts that I suspect you will not find in any single course in a typical universities course catalog. Which is too bad. Curriculum design is often driven by historical artifacts, not what is most useful to students. Artificial barriers between college and university departments promotes specialized hedgehog thinking , not systems thinking. I hope that you will agree, that to successfully implement sustainable enterprise knowledge graph you need to be a generalist and draw knowledge from many disciplines. I hope this book will help you in your journey. I would like to conclude with a few thought experiments about what enterprise knowledge graphs can become in the near term future. Being aware of the Paul Saffo quote above, I am going to hold off on giving you a predicted timeline of when these events will occur. Prediction is hard, and there are many uncertain events that will radically change the the course of enterprise knowledge graphs. Specialized Enterprise Graph Hardware We know that as graph databases increase in popularity, that they will gain the attention of people that are trying to optimize hardware to match the unique workload of enterprise knowledge graphs. Companies like Graphcore and Intel with their PIUMA project have already begun this process. The challenge is that not all graph databases will leverage this new hardware. Doing so requires graph databases to take advantage of specialized hardware to create fast graph traversal. It will take time for both software and hardware to align their systems to work together. Natural Language Query Software like GPT-3 has already shown it can take English language description of queries and convert them to SQL. Look for similar tools that will allow non-technical staff to literally ask complex questions about the content of a knowledge graph. Embeddings Everywhere Graph embedding is just in it's infancy. Right now it takes a huge amount of effort from dedicated data scientists that also understand graph data models to build embeddings. In the future, graph databases will automatically build embedding for you by default. Embedding will be associated with vertices, edges and paths and intelligent query generators will know how to use them in queries. We will finally get to the state where our data science and our query developers will work together in a seamless way. Subgraphs and Micro Reasoners Enterprise knowledge graphs may have standardized subgraphs that can be quickly loaded with a few clicks of an administration tool. Need all the geolocations in the US including every geocoded address, city, state, county and zip code? We have a subgraph for you and all the inference queries over that subgraph.","title":"Conclusion"},{"location":"promoting/conclusion/#conclusion","text":"Never mistake a clear view for a short distance.\u201d \u2014 Paul Saffo We now come to an end of our journey through our tour of our knowledge about enterprise knowledge graphs. We have touched on a wide number of topics and integrated concepts that I suspect you will not find in any single course in a typical universities course catalog. Which is too bad. Curriculum design is often driven by historical artifacts, not what is most useful to students. Artificial barriers between college and university departments promotes specialized hedgehog thinking , not systems thinking. I hope that you will agree, that to successfully implement sustainable enterprise knowledge graph you need to be a generalist and draw knowledge from many disciplines. I hope this book will help you in your journey. I would like to conclude with a few thought experiments about what enterprise knowledge graphs can become in the near term future. Being aware of the Paul Saffo quote above, I am going to hold off on giving you a predicted timeline of when these events will occur. Prediction is hard, and there are many uncertain events that will radically change the the course of enterprise knowledge graphs.","title":"Conclusion"},{"location":"promoting/conclusion/#specialized-enterprise-graph-hardware","text":"We know that as graph databases increase in popularity, that they will gain the attention of people that are trying to optimize hardware to match the unique workload of enterprise knowledge graphs. Companies like Graphcore and Intel with their PIUMA project have already begun this process. The challenge is that not all graph databases will leverage this new hardware. Doing so requires graph databases to take advantage of specialized hardware to create fast graph traversal. It will take time for both software and hardware to align their systems to work together.","title":"Specialized Enterprise Graph Hardware"},{"location":"promoting/conclusion/#natural-language-query","text":"Software like GPT-3 has already shown it can take English language description of queries and convert them to SQL. Look for similar tools that will allow non-technical staff to literally ask complex questions about the content of a knowledge graph.","title":"Natural Language Query"},{"location":"promoting/conclusion/#embeddings-everywhere","text":"Graph embedding is just in it's infancy. Right now it takes a huge amount of effort from dedicated data scientists that also understand graph data models to build embeddings. In the future, graph databases will automatically build embedding for you by default. Embedding will be associated with vertices, edges and paths and intelligent query generators will know how to use them in queries. We will finally get to the state where our data science and our query developers will work together in a seamless way.","title":"Embeddings Everywhere"},{"location":"promoting/conclusion/#subgraphs-and-micro-reasoners","text":"Enterprise knowledge graphs may have standardized subgraphs that can be quickly loaded with a few clicks of an administration tool. Need all the geolocations in the US including every geocoded address, city, state, county and zip code? We have a subgraph for you and all the inference queries over that subgraph.","title":"Subgraphs and Micro Reasoners"},{"location":"promoting/strategic-serendipity/","text":"Strategic Serendipity Luck favors the prepared. - Louis Pasteur When we thing of enterprise strategies, we often think of large groups of executives at multi-day off-site retreats. They will be brainstorming, classifying ideas, looking for synergies, prioritizing, and then putting details into what they hope will be thoughtful planning for the next year and beyond. Their goal is to prioritize concrete tasks that can be acted upon, assigned to teams, measured and then deliver results that can be measured with key performance indicators . Their job is to remove uncertainty and randomness from their plan. But there is ample evidence that innovation and insight are not easy to predict. Many stories about great discoveries such as the discovery of Penicillin by Alexander Fleming amount to random events that literally blew in through an open window. Serendipity is the occurrence of random events that trigger positive outcomes. Strategy and serendipity are oppositional thoughts. Not everyone is good at holding oppositional ideas in their mind to come up with new innovations. It takes practice. We define Strategic Serendipity as the process of thoughtfully creating an environment that promotes insights, serendipity and tangible savings from newly connected information. This means creating an environment around your EKG that encourages discovery. Discovery and the EKG Discovery is different from creating regular operational reports on your EKG. Operational reports show things like counts of vertices of various types with special conditions. The key is operational report on an EKG are run regularly and consistently and often feed the monitoring dashboards that are customized for each user. But other than an occasional new trend line, they don't play a role in the EKG Discovery process. In the context of the EKG, data discovery (also called data mining) is a process of discovering new patterns in large data sets using a combination of query tools, machine learning and often visualization. A Story of Connected Data, Serendipity and Fraud Detection Let me start out with a true story about what I mean by this idea of strategic serendipity environment. When we first building graph databases we started to connect new data sources that had not been connected before. One of our bright young engineers started seeing an unusual pattern of very small healthcare claims in dataset. Being naturally curious, the engineer started to \"connect some dots\". Because the team had not just loaded the flat claims items but also loaded and linked the providers that submitted the claims, the engineer could write simple queries to see that many of the small claims came from just a few providers. Although the individual claims were small, when taken in aggregate, they amounted to over $10M in claims. This pattern just did not make sense to the engineer. So the engineer started to dig deeper. The providers submitted these tiny claims were well outside the normal ranges of what other providers charged for similar procedures. The engineer came up with charge distributions for other providers and then showed the average claims for \"unusual\" providers were much lower than normal. Once they had the unusual providers identified they also saw that these claims were for procedures that their specialty codes didn't indicated they should be doing. The engineer had found that some fraudsters had found a \"loophole\" in our audit rules. Things below a specific threshold just went undetected by our audit rules, regardless of volume. The fraud ring used stolen physician IDs to route claim reimbursements to their own personal bank accounts. What is important about this story is that the engineer could never have found the patterns without connecting claims with physicians with their specialty classifications. The values was not just in the flat claims files. We needed to connect the data together using a graph database. Remember that no one ever sat down with the team and said \"go find fraudulent claims\". We know that fraud in healthcare claims cost US taxpayers what is estimated to be $50 billion dollars a year. But fraudsters have grown every more sophisticated in their tactics. And not all engineers are encouraged to follow their curious instincts. So this is the job of a enterprise strategist. We need to create environments that encourage insight and discovery. Understand that it is impossible to predict exactly what the discoveries will be. We can't predict when and where the discoveries will be. But let's be very clear. If we don't create an integrated set of connected data the insights may never be found.","title":"Strategic Serendipity"},{"location":"promoting/strategic-serendipity/#strategic-serendipity","text":"Luck favors the prepared. - Louis Pasteur When we thing of enterprise strategies, we often think of large groups of executives at multi-day off-site retreats. They will be brainstorming, classifying ideas, looking for synergies, prioritizing, and then putting details into what they hope will be thoughtful planning for the next year and beyond. Their goal is to prioritize concrete tasks that can be acted upon, assigned to teams, measured and then deliver results that can be measured with key performance indicators . Their job is to remove uncertainty and randomness from their plan. But there is ample evidence that innovation and insight are not easy to predict. Many stories about great discoveries such as the discovery of Penicillin by Alexander Fleming amount to random events that literally blew in through an open window. Serendipity is the occurrence of random events that trigger positive outcomes. Strategy and serendipity are oppositional thoughts. Not everyone is good at holding oppositional ideas in their mind to come up with new innovations. It takes practice. We define Strategic Serendipity as the process of thoughtfully creating an environment that promotes insights, serendipity and tangible savings from newly connected information. This means creating an environment around your EKG that encourages discovery.","title":"Strategic Serendipity"},{"location":"promoting/strategic-serendipity/#discovery-and-the-ekg","text":"Discovery is different from creating regular operational reports on your EKG. Operational reports show things like counts of vertices of various types with special conditions. The key is operational report on an EKG are run regularly and consistently and often feed the monitoring dashboards that are customized for each user. But other than an occasional new trend line, they don't play a role in the EKG Discovery process. In the context of the EKG, data discovery (also called data mining) is a process of discovering new patterns in large data sets using a combination of query tools, machine learning and often visualization.","title":"Discovery and the EKG"},{"location":"promoting/strategic-serendipity/#a-story-of-connected-data-serendipity-and-fraud-detection","text":"Let me start out with a true story about what I mean by this idea of strategic serendipity environment. When we first building graph databases we started to connect new data sources that had not been connected before. One of our bright young engineers started seeing an unusual pattern of very small healthcare claims in dataset. Being naturally curious, the engineer started to \"connect some dots\". Because the team had not just loaded the flat claims items but also loaded and linked the providers that submitted the claims, the engineer could write simple queries to see that many of the small claims came from just a few providers. Although the individual claims were small, when taken in aggregate, they amounted to over $10M in claims. This pattern just did not make sense to the engineer. So the engineer started to dig deeper. The providers submitted these tiny claims were well outside the normal ranges of what other providers charged for similar procedures. The engineer came up with charge distributions for other providers and then showed the average claims for \"unusual\" providers were much lower than normal. Once they had the unusual providers identified they also saw that these claims were for procedures that their specialty codes didn't indicated they should be doing. The engineer had found that some fraudsters had found a \"loophole\" in our audit rules. Things below a specific threshold just went undetected by our audit rules, regardless of volume. The fraud ring used stolen physician IDs to route claim reimbursements to their own personal bank accounts. What is important about this story is that the engineer could never have found the patterns without connecting claims with physicians with their specialty classifications. The values was not just in the flat claims files. We needed to connect the data together using a graph database. Remember that no one ever sat down with the team and said \"go find fraudulent claims\". We know that fraud in healthcare claims cost US taxpayers what is estimated to be $50 billion dollars a year. But fraudsters have grown every more sophisticated in their tactics. And not all engineers are encouraged to follow their curious instincts. So this is the job of a enterprise strategist. We need to create environments that encourage insight and discovery. Understand that it is impossible to predict exactly what the discoveries will be. We can't predict when and where the discoveries will be. But let's be very clear. If we don't create an integrated set of connected data the insights may never be found.","title":"A Story of Connected Data, Serendipity and Fraud Detection"},{"location":"promoting/technology-adoption/","text":"Technology Adoption There is nothing permanent except change. - Heraclitus In 1991 Geoffrey Moore wrote his groundbreaking book, Crossing the Chasm . In this book Moore discussed the idea the Technology Adoption Lifecycle based on prior work in the diffusion of innovation . Moore's key insight was the key difference between why organization adopt new technologies. Innovators and early adaptors focused on giving their organizations a competitive advantage using technology. In contrast, early majority buyers are interested in lowering operational costs and need solid evidence that many similar organizations to their own have already reached uncontested cost savings.","title":"Technology Adoption"},{"location":"promoting/technology-adoption/#technology-adoption","text":"There is nothing permanent except change. - Heraclitus In 1991 Geoffrey Moore wrote his groundbreaking book, Crossing the Chasm . In this book Moore discussed the idea the Technology Adoption Lifecycle based on prior work in the diffusion of innovation . Moore's key insight was the key difference between why organization adopt new technologies. Innovators and early adaptors focused on giving their organizations a competitive advantage using technology. In contrast, early majority buyers are interested in lowering operational costs and need solid evidence that many similar organizations to their own have already reached uncontested cost savings.","title":"Technology Adoption"},{"location":"promoting/windows-of-opportunity/","text":"Windows of Opportunity in Knowledge Graph Adoption Defining the EKG window of opportunity Necessary preconditions The EKG pitch deck The cost-benefit analysis The pilot project Testing scalability Taking advantage of a crisis Most large organization goes through annual planning cycles. Executives ponder how they can cut costs, raise sales and create new systems that might be more responsive to customer needs. But it is rare that executives all sit around and say \"we really need an enterprise knowledge graph\". From what I can tell, it has never happened. Companies that successfully adopt enterprise knowledge graphs go through a series of stages before they embark on a pilot EKG project. Once they do start the pilot project, the clocks usually begin ticking. If the right preconditions are not met, the pilot may fail and the organization will quickly determine that the EKG is not a good fit for their organization. This narrow time slot is called the Window of Opportunity for EKGs. Knowing how to recognize these windows and take advantage of them is a key skill we attempt to explore in this book. Necessary Preconditions Prepping the Pitch Deck When I worked with Steve Jobs at NeXT computer one of my jobs was to prep events for Steve's presentations. I learned important lessons on the value of preparation before you give a presentation. Everything has to do with attention to detail and finding a message that will resonate with your audience. People will respect you if you do a detailed analysis of their situation before you propose a solution. Speak the language of the audience - start with their strategic objectives Doing your homeworks - knowing their pain points Knowing who they respect - what books do their leaders quote from? Using insider knowledge - what pain points will the developers share References - who else in the organization do they trust Managing emotions: fear and greed - fear they will be left out - greed for the bonus of cost reduction and better insights Knowing how much detail is enough The Cost Benefit Analysis","title":"Windows of Opportunity in Knowledge Graph Adoption"},{"location":"promoting/windows-of-opportunity/#windows-of-opportunity-in-knowledge-graph-adoption","text":"Defining the EKG window of opportunity Necessary preconditions The EKG pitch deck The cost-benefit analysis The pilot project Testing scalability Taking advantage of a crisis Most large organization goes through annual planning cycles. Executives ponder how they can cut costs, raise sales and create new systems that might be more responsive to customer needs. But it is rare that executives all sit around and say \"we really need an enterprise knowledge graph\". From what I can tell, it has never happened. Companies that successfully adopt enterprise knowledge graphs go through a series of stages before they embark on a pilot EKG project. Once they do start the pilot project, the clocks usually begin ticking. If the right preconditions are not met, the pilot may fail and the organization will quickly determine that the EKG is not a good fit for their organization. This narrow time slot is called the Window of Opportunity for EKGs. Knowing how to recognize these windows and take advantage of them is a key skill we attempt to explore in this book.","title":"Windows of Opportunity in Knowledge Graph Adoption"},{"location":"promoting/windows-of-opportunity/#necessary-preconditions","text":"","title":"Necessary Preconditions"},{"location":"promoting/windows-of-opportunity/#prepping-the-pitch-deck","text":"When I worked with Steve Jobs at NeXT computer one of my jobs was to prep events for Steve's presentations. I learned important lessons on the value of preparation before you give a presentation. Everything has to do with attention to detail and finding a message that will resonate with your audience. People will respect you if you do a detailed analysis of their situation before you propose a solution. Speak the language of the audience - start with their strategic objectives Doing your homeworks - knowing their pain points Knowing who they respect - what books do their leaders quote from? Using insider knowledge - what pain points will the developers share References - who else in the organization do they trust Managing emotions: fear and greed - fear they will be left out - greed for the bonus of cost reduction and better insights Knowing how much detail is enough","title":"Prepping the Pitch Deck"},{"location":"promoting/windows-of-opportunity/#the-cost-benefit-analysis","text":"","title":"The Cost Benefit Analysis"}]}